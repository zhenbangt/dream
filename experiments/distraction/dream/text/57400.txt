Env ID: [374]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.10582885891199112
Distance: 9.591608047485352
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1366773545742035
Distance: 9.597436904907227
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.6210569143295288
Distance: 9.634114265441895
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.12746581435203552
Distance: 8.913057327270508
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.09526214748620987
Distance: 8.940523147583008
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.3275352418422699
Distance: 8.935785293579102
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.7331337928771973
Distance: 9.163320541381836
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.05372323840856552
Distance: 6.33018684387207
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.6451674699783325
Distance: 6.176463603973389
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.05115947872400284
Distance: 6.721631050109863
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.1818246841430664
Distance: 6.67279052734375
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.47125348448753357
Distance: 4.390965938568115
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.18418589234352112
Distance: 4.762219429016113
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.3632756173610687
Distance: 4.478033542633057
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

