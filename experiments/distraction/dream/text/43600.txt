Env ID: [528]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.1420961320400238
Distance: 8.407365798950195
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.1106029525399208
Distance: 8.449461936950684
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.18474635481834412
Distance: 8.460064888000488
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.12546882033348083
Distance: 8.544811248779297
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -1.255487084388733
Distance: 8.570280075073242
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.35686835646629333
Distance: 9.725767135620117
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -1.0107609033584595
Distance: 9.982635498046875
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.4752841889858246
Distance: 10.893396377563477
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.6382690668106079
Distance: 11.268680572509766
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.9692217111587524
Distance: 11.806949615478516
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.9222179651260376
Distance: 12.67617130279541
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.5501724481582642
Distance: 13.49838924407959
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.40798911452293396
Distance: 13.948561668395996
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.18910083174705505
Distance: 14.256550788879395
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.07304153591394424
Distance: 14.345651626586914
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: 0.023346327245235443
Distance: 14.318693161010742
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: 0.13243523240089417
Distance: 14.19534683227539
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: 0.0700763687491417
Distance: 13.962911605834961
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.0017686858773231506
Distance: 13.792835235595703
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.046541787683963776
Distance: 13.69460391998291
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

