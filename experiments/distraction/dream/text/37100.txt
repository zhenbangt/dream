Env ID: [66]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.10780248790979385
Distance: 7.091213226318359
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.2655263841152191
Distance: 7.099015712738037
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.025079824030399323
Distance: 7.264542102813721
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.2335735261440277
Distance: 7.189621925354004
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 5, 1], dtype=torch.int32)
Action: up
Reward: -0.0032173171639442444
Distance: 7.323195457458496
Next state: tensor([5, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 6, 0], dtype=torch.int32)
Action: up
Reward: 0.16578999161720276
Distance: 7.226412773132324
Next state: tensor([5, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 7, 0], dtype=torch.int32)
Action: up
Reward: -0.012141801416873932
Distance: 6.960622787475586
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 8, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.1239830031991005
Distance: 6.872764587402344
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

