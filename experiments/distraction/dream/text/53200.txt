Env ID: [198]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.1823793351650238
Distance: 9.466442108154297
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.06690368801355362
Distance: 9.548821449279785
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.0411046743392944
Distance: 9.515725135803223
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.0010105147957801819
Distance: 8.37462043762207
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.3161178529262543
Distance: 8.275630950927734
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.19262561202049255
Distance: 8.491748809814453
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.07697639614343643
Distance: 8.58437442779541
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.18616238236427307
Distance: 8.56135082244873
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.2463909089565277
Distance: 8.275188446044922
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.15245667099952698
Distance: 8.421579360961914
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.6092725992202759
Distance: 8.169122695922852
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.14223995804786682
Distance: 8.67839527130127
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.22882995009422302
Distance: 8.436155319213867
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.258963018655777
Distance: 8.564985275268555
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.45818671584129333
Distance: 8.206022262573242
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.17418059706687927
Distance: 8.564208984375
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.16601905226707458
Distance: 8.638389587402344
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.24249973893165588
Distance: 8.704408645629883
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.5272241830825806
Distance: 8.361908912658691
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.1086803451180458
Distance: 8.789133071899414
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

