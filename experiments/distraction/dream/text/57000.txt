Env ID: [187]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.06640873104333878
Distance: 9.22356128692627
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09793243557214737
Distance: 9.189970016479492
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.5062297582626343
Distance: 9.187902450561523
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.5905405282974243
Distance: 7.581672668457031
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.22027263045310974
Distance: 8.072213172912598
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.4650817811489105
Distance: 8.192485809326172
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.2539001405239105
Distance: 8.557567596435547
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.10766277462244034
Distance: 8.711467742919922
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.2158418595790863
Distance: 8.719130516052246
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.10973968356847763
Distance: 8.834972381591797
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.7010531425476074
Distance: 8.625232696533203
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.41087207198143005
Distance: 5.824179649353027
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

