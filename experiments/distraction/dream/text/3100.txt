Env ID: [275]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.2308717668056488
Distance: 9.776286125183105
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.04768504947423935
Distance: 9.907157897949219
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.0951448455452919
Distance: 9.759472846984863
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.37763747572898865
Distance: 9.754617691040039
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.11301574856042862
Distance: 10.032255172729492
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.0156492218375206
Distance: 10.045270919799805
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

