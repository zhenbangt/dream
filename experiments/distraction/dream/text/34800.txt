Env ID: [528]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.13011321425437927
Distance: 7.6794939041137695
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.030352972447872162
Distance: 7.709607124328613
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.07756529003381729
Distance: 7.579254150390625
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.17844685912132263
Distance: 7.556819438934326
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.06615934520959854
Distance: 7.635266304016113
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.14479169249534607
Distance: 7.601425647735596
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([2, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.33334454894065857
Distance: 7.646217346191406
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([1, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10990533977746964
Distance: 7.879561901092529
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([1, 3, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.17184916138648987
Distance: 7.889467239379883
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([1, 3, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.0733121857047081
Distance: 7.617618083953857
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([1, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.05454263836145401
Distance: 7.444305896759033
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([1, 3, 0], dtype=torch.int32)
Action: right
Reward: -0.05310497432947159
Distance: 7.398848533630371
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([2, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.30703219771385193
Distance: 7.351953506469727
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([2, 3, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1985226571559906
Distance: 7.558985710144043
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([2, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.296909898519516
Distance: 7.657508373260498
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([2, 3, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.10578451305627823
Distance: 7.8544182777404785
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([2, 3, 0], dtype=torch.int32)
Action: pickup
Reward: 0.04255523532629013
Distance: 7.860202789306641
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([2, 3, 0], dtype=torch.int32)
Action: left
Reward: 0.2083210051059723
Distance: 7.717647552490234
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([1, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.002365492284297943
Distance: 7.409326553344727
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

