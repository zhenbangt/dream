Env ID: [253]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.142277330160141
Distance: 5.726900100708008
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.053583718836307526
Distance: 5.769177436828613
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.2951112687587738
Distance: 5.722761154174805
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: 0.10330524295568466
Distance: 5.917872428894043
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.11807499080896378
Distance: 5.714567184448242
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.14724883437156677
Distance: 5.73264217376709
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 2, 0], dtype=torch.int32)
Action: left
Reward: -0.47685346007347107
Distance: 5.779891014099121
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 2, 0], dtype=torch.int32)
Action: noop
Reward: -0.239750474691391
Distance: 6.156744480133057
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 2, 0], dtype=torch.int32)
Action: drop
Reward: -0.08214101940393448
Distance: 6.296494960784912
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.541547417640686
Distance: 6.2786359786987305
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2604922354221344
Distance: 6.720183372497559
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 8, 1], dtype=torch.int32)
Action: right
Reward: 0.12170305103063583
Distance: 6.359691143035889
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 8, 0], dtype=torch.int32)
Action: noop
Reward: -0.3247128427028656
Distance: 6.137988090515137
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 8, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.03790483623743057
Distance: 6.362700939178467
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 8, 0], dtype=torch.int32)
Action: noop
Reward: -0.20475491881370544
Distance: 6.300605773925781
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 8, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.13122645020484924
Distance: 6.405360698699951
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 8, 0], dtype=torch.int32)
Action: pickup
Reward: 0.09587516635656357
Distance: 6.174134254455566
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 8, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.12230215221643448
Distance: 5.978259086608887
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 8, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.04868040233850479
Distance: 6.000561237335205
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 8, 0], dtype=torch.int32)
Action: pickup
Reward: 0.012684725224971771
Distance: 5.949241638183594
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

