Env ID: [429]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.1896730363368988
Distance: 9.749471664428711
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.0475963354110718
Distance: 9.839144706726074
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3421434462070465
Distance: 8.691548347473145
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 5, 1], dtype=torch.int32)
Action: drop
Reward: -0.9239031076431274
Distance: 8.249404907226562
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.38103732466697693
Distance: 9.073307991027832
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.6751610040664673
Distance: 9.354345321655273
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.8140941858291626
Distance: 9.929506301879883
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.6838117837905884
Distance: 10.643600463867188
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 5, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.2529119551181793
Distance: 11.227412223815918
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

