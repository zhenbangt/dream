Env ID: [121]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.14028319716453552
Distance: 9.804569244384766
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.014254190027713776
Distance: 9.844852447509766
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.4100814759731293
Distance: 9.759106636047363
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.3941541612148285
Distance: 10.069188117980957
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.5909305810928345
Distance: 10.36334228515625
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 5, 1], dtype=torch.int32)
Action: up
Reward: -1.158094048500061
Distance: 10.854272842407227
Next state: tensor([3, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 6, 0], dtype=torch.int32)
Action: left
Reward: -0.4410644471645355
Distance: 11.91236686706543
Next state: tensor([2, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([2, 6, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1731334626674652
Distance: 12.25343132019043
Next state: tensor([2, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([2, 6, 0], dtype=torch.int32)
Action: noop
Reward: -0.0559297576546669
Distance: 12.32656478881836
Next state: tensor([2, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([2, 6, 0], dtype=torch.int32)
Action: right
Reward: 0.043283842504024506
Distance: 12.28249454498291
Next state: tensor([3, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 6, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.08450756222009659
Distance: 12.13921070098877
Next state: tensor([3, 6, 0], dtype=torch.int32)
================================================================================

