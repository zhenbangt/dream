Env ID: [165]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.05256042629480362
Distance: 9.528884887695312
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.2210506200790405
Distance: 9.4814453125
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -1.1365429162979126
Distance: 8.160394668579102
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.17906227707862854
Distance: 9.196937561035156
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.0941062942147255
Distance: 8.917875289916992
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.047856904566287994
Distance: 8.911981582641602
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.063652515411377
Distance: 8.859838485717773
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.06387033313512802
Distance: 6.696186065673828
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.11605749279260635
Distance: 6.532315731048584
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

