Env ID: [88]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.11915741115808487
Distance: 8.380912780761719
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.04599819332361221
Distance: 8.400070190429688
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: 0.022730253636837006
Distance: 8.346068382263184
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.1884962022304535
Distance: 8.22333812713623
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.11163101345300674
Distance: 8.311834335327148
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 5, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.11414871364831924
Distance: 8.323465347290039
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

