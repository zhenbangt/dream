Env ID: [99]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.1046043410897255
Distance: 8.861747741699219
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10242996364831924
Distance: 8.866352081298828
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.873287558555603
Distance: 8.868782043457031
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.0976482406258583
Distance: 7.89549446105957
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.1394706666469574
Distance: 7.8931427001953125
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.13488349318504333
Distance: 7.932613372802734
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.026986002922058
Distance: 7.967496871948242
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.022089578211307526
Distance: 6.840510845184326
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.12010965496301651
Distance: 6.762600421905518
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.10478410869836807
Distance: 6.782710075378418
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.2637594938278198
Distance: 6.78749418258667
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.06551017612218857
Distance: 5.423734664916992
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.1014019027352333
Distance: 5.2582244873046875
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.2226921021938324
Distance: 5.259626388549805
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 5.212832927703857
Distance: 5.382318496704102
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.06597539782524109
Distance: 0.06948548555374146
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.08868712931871414
Distance: 0.035460881888866425
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10766541957855225
Distance: 0.024148009717464447
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 5, 1], dtype=torch.int32)
Action: drop
Reward: -0.11493244767189026
Distance: 0.031813424080610275
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 5, 1], dtype=torch.int32)
Action: left
Reward: -0.1030188649892807
Distance: 0.046745866537094116
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

