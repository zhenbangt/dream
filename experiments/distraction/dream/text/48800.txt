Env ID: [264]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.10492382198572159
Distance: 8.40483283996582
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.04959163814783096
Distance: 8.409756660461426
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.24777564406394958
Distance: 8.35934829711914
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.21188488602638245
Distance: 8.507123947143555
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.2365032136440277
Distance: 8.195239067077637
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.3576265275478363
Distance: 8.331742286682129
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 5, 1], dtype=torch.int32)
Action: drop
Reward: -0.36445197463035583
Distance: 8.58936882019043
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: 0.020870588719844818
Distance: 8.85382080078125
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: 0.15524712204933167
Distance: 8.732950210571289
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: 0.004153631627559662
Distance: 8.477703094482422
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.11407718807458878
Distance: 8.373549461364746
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.17132529616355896
Distance: 8.387626647949219
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.19581469893455505
Distance: 8.458951950073242
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.1712251603603363
Distance: 8.554766654968262
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([3, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.14669188857078552
Distance: 8.625991821289062
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.24546489119529724
Distance: 8.672683715820312
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.23597773909568787
Distance: 8.818148612976074
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: 0.041142843663692474
Distance: 8.954126358032227
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.3531728684902191
Distance: 8.812983512878418
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.10824527591466904
Distance: 9.066156387329102
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

