Env ID: [506]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.12820109724998474
Distance: 8.701313018798828
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.07351741939783096
Distance: 8.729514122009277
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.018600843846797943
Distance: 8.703031539916992
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.17227420210838318
Distance: 8.584430694580078
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.12792643904685974
Distance: 8.656704902648926
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.215550035238266
Distance: 8.68463134765625
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.3066469132900238
Distance: 8.80018138885498
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.0794445052742958
Distance: 9.006828308105469
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.36299172043800354
Distance: 8.986272811889648
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.20869293808937073
Distance: 8.52328109741211
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.209538072347641
Distance: 8.214588165283203
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.3175605833530426
Distance: 8.324126243591309
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.6658419370651245
Distance: 7.9065656661987305
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.746658205986023
Distance: 7.140723705291748
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.025499723851680756
Distance: 6.294065475463867
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.2145337164402008
Distance: 6.16856575012207
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.10563001781702042
Distance: 5.854032039642334
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.3525661528110504
Distance: 5.859662055969238
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.600350022315979
Distance: 5.407095909118652
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.13793888688087463
Distance: 5.907445907592773
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

