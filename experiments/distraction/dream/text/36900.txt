Env ID: [44]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.09895095974206924
Distance: 9.835460662841797
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.11273441463708878
Distance: 9.83441162109375
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.14485225081443787
Distance: 9.847146034240723
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.08362159878015518
Distance: 9.891998291015625
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.01158294826745987
Distance: 9.875619888305664
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: 0.07061424106359482
Distance: 9.787202835083008
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.180027574300766
Distance: 9.616588592529297
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.23814257979393005
Distance: 9.696616172790527
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.0998760238289833
Distance: 9.834758758544922
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.11024055629968643
Distance: 9.834634780883789
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 4, 1], dtype=torch.int32)
Action: noop
Reward: 0.05079307407140732
Distance: 9.84487533569336
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.05867061764001846
Distance: 9.694082260131836
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.19329604506492615
Distance: 9.652752876281738
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.044011689722537994
Distance: 9.746048927307129
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

