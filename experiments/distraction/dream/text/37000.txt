Env ID: [396]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.02805481106042862
Distance: 8.68692684173584
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.22283610701560974
Distance: 8.614981651306152
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.11738166958093643
Distance: 8.737817764282227
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.19103965163230896
Distance: 8.755199432373047
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.03698406368494034
Distance: 8.84623908996582
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.27997836470603943
Distance: 8.783223152160645
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.10281028598546982
Distance: 8.963201522827148
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.22854384779930115
Distance: 8.760391235351562
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([7, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.12851294875144958
Distance: 8.888935089111328
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.43210068345069885
Distance: 8.917448043823242
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

