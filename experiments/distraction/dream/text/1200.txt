Env ID: [209]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.22656306624412537
Distance: 8.502575874328613
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.0292268767952919
Distance: 8.629138946533203
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.023223496973514557
Distance: 8.558365821838379
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.2271791398525238
Distance: 8.481589317321777
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.15397223830223083
Distance: 8.608768463134766
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.3467164933681488
Distance: 8.662740707397461
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.174503892660141
Distance: 8.909457206726074
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.12281761318445206
Distance: 8.98396110534668
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.1457410752773285
Distance: 9.006778717041016
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10447845607995987
Distance: 9.052519798278809
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.3693166673183441
Distance: 9.056998252868652
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.24916419386863708
Distance: 9.326314926147461
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([1, 1, 0], dtype=torch.int32)
Action: pickup
Reward: -0.452152818441391
Distance: 9.475479125976562
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([1, 1, 0], dtype=torch.int32)
Action: right
Reward: -0.2098899781703949
Distance: 9.827631950378418
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([2, 1, 0], dtype=torch.int32)
Action: noop
Reward: -0.28334102034568787
Distance: 9.937521934509277
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([2, 1, 0], dtype=torch.int32)
Action: pickup
Reward: -0.26529940962791443
Distance: 10.12086296081543
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([2, 1, 0], dtype=torch.int32)
Action: right
Reward: -0.07542381435632706
Distance: 10.286162376403809
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([3, 1, 0], dtype=torch.int32)
Action: drop
Reward: -0.14883288741111755
Distance: 10.26158618927002
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 1, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.02505359798669815
Distance: 10.310419082641602
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

