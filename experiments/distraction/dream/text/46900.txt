Env ID: [352]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.091954804956913
Distance: 9.474462509155273
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.07907352596521378
Distance: 9.46641731262207
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.10997448116540909
Distance: 9.445490837097168
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09847602993249893
Distance: 9.455465316772461
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.1599327027797699
Distance: 9.453941345214844
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.01344452053308487
Distance: 9.513874053955078
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.13848742842674255
Distance: 9.427318572998047
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.053698159754276276
Distance: 9.465806007385254
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.07796058803796768
Distance: 9.419504165649414
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 7, 1], dtype=torch.int32)
Action: drop
Reward: -0.3946243226528168
Distance: 9.397464752197266
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 7, 1], dtype=torch.int32)
Action: left
Reward: -0.15696868300437927
Distance: 9.692089080810547
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.14598235487937927
Distance: 9.74905776977539
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([1, 7, 0], dtype=torch.int32)
Action: pickup
Reward: -0.08402786403894424
Distance: 9.795040130615234
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([1, 7, 0], dtype=torch.int32)
Action: pickup
Reward: 0.11447181552648544
Distance: 9.779067993164062
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([1, 7, 0], dtype=torch.int32)
Action: down
Reward: 0.06005897372961044
Distance: 9.564596176147461
Next state: tensor([1, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([1, 6, 0], dtype=torch.int32)
Action: down
Reward: 0.09488334506750107
Distance: 9.404537200927734
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([1, 5, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12456665188074112
Distance: 9.209653854370117
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([1, 5, 0], dtype=torch.int32)
Action: down
Reward: 0.041341207921504974
Distance: 9.234220504760742
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([1, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.028041265904903412
Distance: 9.092879295349121
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.19132289290428162
Distance: 8.964838027954102
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

