Env ID: [154]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.09436283260583878
Distance: 8.301989555358887
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.15973815321922302
Distance: 8.29635238647461
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.11870727688074112
Distance: 8.356090545654297
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.016853906214237213
Distance: 8.374797821044922
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.10211620479822159
Distance: 8.291651725769043
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.20652827620506287
Distance: 8.293767929077148
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10236702114343643
Distance: 8.400296211242676
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.14602336287498474
Distance: 8.402663230895996
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.13674679398536682
Distance: 8.448686599731445
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.43540725111961365
Distance: 8.211939811706543
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.025524519383907318
Distance: 8.547347068786621
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.4169698655605316
Distance: 8.421822547912598
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.15290793776512146
Distance: 8.738792419433594
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.3507266938686371
Distance: 8.79170036315918
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.1381431519985199
Distance: 9.042427062988281
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.3401695191860199
Distance: 9.080570220947266
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.11417999118566513
Distance: 9.32073974609375
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.3292156159877777
Distance: 9.106559753417969
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.019803620874881744
Distance: 9.335775375366211
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 8, 1], dtype=torch.int32)
Action: up
Reward: -0.21356162428855896
Distance: 9.255578994750977
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

