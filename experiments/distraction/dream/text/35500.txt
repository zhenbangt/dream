Env ID: [363]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.013059042394161224
Distance: 9.859395980834961
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.4218479096889496
Distance: 9.746336936950684
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.8872884511947632
Distance: 10.068184852600098
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.300532728433609
Distance: 9.080896377563477
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.5534292459487915
Distance: 8.680363655090332
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.3478599488735199
Distance: 9.133792877197266
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.1561809480190277
Distance: 9.38165283203125
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.1231321319937706
Distance: 9.437833786010742
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.3915124833583832
Distance: 9.214701652526855
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([2, 3, 0], dtype=torch.int32)
Action: down
Reward: 0.08859195560216904
Distance: 9.506214141845703
Next state: tensor([2, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([2, 2, 0], dtype=torch.int32)
Action: left
Reward: -0.4241090714931488
Distance: 9.317622184753418
Next state: tensor([1, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([1, 2, 0], dtype=torch.int32)
Action: left
Reward: -0.23882636427879333
Distance: 9.641731262207031
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 2, 0], dtype=torch.int32)
Action: left
Reward: 0.0008176788687705994
Distance: 9.780557632446289
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 2, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.16107997298240662
Distance: 9.679739952087402
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

