Env ID: [517]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.06671056896448135
Distance: 7.257709503173828
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.20989695191383362
Distance: 7.224420070648193
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: noop
Reward: 0.06598319858312607
Distance: 6.914523124694824
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.009711362421512604
Distance: 6.748539924621582
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.34232911467552185
Distance: 6.6582512855529785
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([1, 1, 0], dtype=torch.int32)
Action: right
Reward: -0.42294368147850037
Distance: 6.900580406188965
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([2, 1, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.26223525404930115
Distance: 7.22352409362793
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([2, 1, 0], dtype=torch.int32)
Action: right
Reward: -0.46954402327537537
Distance: 7.385759353637695
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 1, 0], dtype=torch.int32)
Action: drop
Reward: -0.5889898538589478
Distance: 7.755303382873535
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 1, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.31532058119773865
Distance: 8.244293212890625
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

