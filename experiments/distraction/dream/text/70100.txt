Env ID: [242]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.04810772091150284
Distance: 9.841673851013184
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.29263535141944885
Distance: 9.78978157043457
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.17762717604637146
Distance: 9.397146224975586
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.18211039900779724
Distance: 9.474773406982422
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.07306060940027237
Distance: 9.556883811950684
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.1389379501342773
Distance: 9.52994441986084
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.08467569202184677
Distance: 6.291006565093994
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.41704902052879333
Distance: 6.106330871582031
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.13092860579490662
Distance: 6.423379898071289
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.462026596069336
Distance: 6.45430850982666
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.23796042799949646
Distance: 3.892282009124756
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.22808179259300232
Distance: 4.030242443084717
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.497360348701477
Distance: 4.158324241638184
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 0, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.2765706479549408
Distance: 2.5609638690948486
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

