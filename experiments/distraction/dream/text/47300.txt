Env ID: [242]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.07848415523767471
Distance: 8.85617733001709
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08660278469324112
Distance: 8.834661483764648
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.24237194657325745
Distance: 8.821264266967773
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.4429393708705902
Distance: 8.47889232635498
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.15274199843406677
Distance: 8.821831703186035
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.08674106746912003
Distance: 8.874573707580566
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.18583831191062927
Distance: 8.86131477355957
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.03011169284582138
Distance: 8.947153091430664
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.1723228394985199
Distance: 8.817041397094727
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.05510082095861435
Distance: 8.889364242553711
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: 0.20856419205665588
Distance: 8.73426342010498
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3017524778842926
Distance: 8.425699234008789
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.2829490602016449
Distance: 8.023946762084961
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.24097386002540588
Distance: 8.20689582824707
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.31427374482154846
Distance: 7.865921974182129
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 1, 1], dtype=torch.int32)
Action: noop
Reward: -0.09841833263635635
Distance: 7.451648235321045
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 1, 1], dtype=torch.int32)
Action: noop
Reward: -0.24709615111351013
Distance: 7.450066566467285
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 1, 1], dtype=torch.int32)
Action: noop
Reward: -0.17891892790794373
Distance: 7.59716272354126
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 1, 1], dtype=torch.int32)
Action: noop
Reward: -0.03834686428308487
Distance: 7.676081657409668
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 1, 1], dtype=torch.int32)
Action: noop
Reward: 0.015987776219844818
Distance: 7.614428520202637
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

