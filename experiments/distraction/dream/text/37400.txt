Env ID: [429]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.0751873031258583
Distance: 8.969907760620117
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12086639553308487
Distance: 8.94509506225586
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.10881481319665909
Distance: 8.965961456298828
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.1281343400478363
Distance: 8.974776268005371
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.004977799952030182
Distance: 9.002910614013672
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.1254030168056488
Distance: 8.907888412475586
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.0698753371834755
Distance: 8.9332914352417
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: 0.03923168033361435
Distance: 8.903166770935059
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.014979936182498932
Distance: 8.763935089111328
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.08657989650964737
Distance: 8.678915023803711
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.094915010035038
Distance: 8.665494918823242
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.2209220826625824
Distance: 8.660409927368164
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.19136294722557068
Distance: 8.781332015991211
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.2038770616054535
Distance: 8.872694969177246
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.20745810866355896
Distance: 8.976572036743164
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.2610679566860199
Distance: 9.084030151367188
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

