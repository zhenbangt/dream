Env ID: [539]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08489570766687393
Distance: 7.458164215087891
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1040249839425087
Distance: 7.443059921264648
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.00868091732263565
Distance: 7.447084903717041
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.06654319912195206
Distance: 7.3557658195495605
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.0860196128487587
Distance: 7.3223090171813965
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.02466592937707901
Distance: 7.308328628540039
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9151057004928589
Distance: 7.232994556427002
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10002050548791885
Distance: 6.217888832092285
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.18786773085594177
Distance: 6.217909336090088
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.015017606317996979
Distance: 6.305777072906494
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.82282292842865
Distance: 6.220794677734375
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1064053550362587
Distance: 4.297971725463867
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.08171234279870987
Distance: 4.30437707901001
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.139408111572266
Distance: 4.2860894203186035
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 8, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.10161824524402618
Distance: 0.04668157547712326
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

