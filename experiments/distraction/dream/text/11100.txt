Env ID: [396]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.14449509978294373
Distance: 7.227231502532959
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.06382665783166885
Distance: 7.271726608276367
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: up
Reward: 0.00724019855260849
Distance: 7.23555326461792
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2912183701992035
Distance: 7.128313064575195
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.012294389307498932
Distance: 7.319531440734863
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([1, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.27407607436180115
Distance: 7.231825828552246
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([1, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.42643269896507263
Distance: 7.405901908874512
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([1, 3, 0], dtype=torch.int32)
Action: drop
Reward: -0.5831195116043091
Distance: 7.732334613800049
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([1, 3, 0], dtype=torch.int32)
Action: noop
Reward: -1.1857010126113892
Distance: 8.2154541015625
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([1, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.21925410628318787
Distance: 9.301155090332031
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([1, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.09155025333166122
Distance: 9.420409202575684
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

