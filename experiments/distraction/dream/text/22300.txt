Env ID: [88]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.15623292326927185
Distance: 7.494992256164551
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.4965387284755707
Distance: 7.551225185394287
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3932599127292633
Distance: 7.947763919830322
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.0714484229683876
Distance: 7.454504013061523
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.25632962584495544
Distance: 7.425952434539795
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.1912895143032074
Distance: 7.582282066345215
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([2, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.2699342668056488
Distance: 7.673571586608887
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([2, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.06118927150964737
Distance: 7.843505859375
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([2, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.021072961390018463
Distance: 7.804695129394531
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([1, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.2816258370876312
Distance: 7.725768089294434
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([1, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.23823842406272888
Distance: 7.907393932342529
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([1, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.12882289290428162
Distance: 8.045632362365723
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([1, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.3689933717250824
Distance: 8.074455261230469
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([1, 5, 0], dtype=torch.int32)
Action: up
Reward: -0.0546480193734169
Distance: 8.343448638916016
Next state: tensor([1, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([1, 6, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.08397998660802841
Distance: 8.298096656799316
Next state: tensor([1, 6, 0], dtype=torch.int32)
================================================================================

