Env ID: [143]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.19514235854148865
Distance: 9.70796012878418
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.09541664272546768
Distance: 9.803102493286133
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.09877452999353409
Distance: 9.798519134521484
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.13839396834373474
Distance: 9.797293663024902
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.1312480866909027
Distance: 9.835687637329102
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.27926692366600037
Distance: 9.866935729980469
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.13553866744041443
Distance: 10.046202659606934
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2586750090122223
Distance: 10.081741333007812
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.5325390100479126
Distance: 9.723066329956055
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.3712402284145355
Distance: 10.15560531616211
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.5211483240127563
Distance: 10.42684555053711
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: 0.44938698410987854
Distance: 10.847993850708008
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.3870931565761566
Distance: 10.298606872558594
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.40342673659324646
Distance: 10.585700035095215
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.5286070108413696
Distance: 10.889126777648926
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.18487891554832458
Distance: 11.317733764648438
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.08367977291345596
Distance: 11.402612686157227
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.2171834707260132
Distance: 11.386292457580566
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 0, 1], dtype=torch.int32)
Action: right
Reward: 0.2973007261753082
Distance: 10.069108963012695
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 0, 0], dtype=torch.int32)
Action: left
Reward: 0.7169479131698608
Distance: 9.671808242797852
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

