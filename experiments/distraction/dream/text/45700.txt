Env ID: [66]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.07417068630456924
Distance: 8.519201278686523
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.1861186921596527
Distance: 8.493371963500977
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.013808824121952057
Distance: 8.579490661621094
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.06957969814538956
Distance: 8.49329948425293
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.256083369255066
Distance: 8.462879180908203
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.22061976790428162
Distance: 7.106795787811279
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.19415560364723206
Distance: 7.227415561676025
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1223507896065712
Distance: 6.933259963989258
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.12142381817102432
Distance: 6.955610752105713
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.038080595433712006
Distance: 6.977034568786621
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.3014846742153168
Distance: 6.838953971862793
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.026985742151737213
Distance: 7.040438652038574
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.35221919417381287
Distance: 6.967424392700195
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.09720096737146378
Distance: 7.219643592834473
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.2541280686855316
Distance: 7.21684455871582
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.17831620573997498
Distance: 7.370972633361816
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.14825066924095154
Distance: 7.449288845062256
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.2555771768093109
Distance: 7.497539520263672
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.0889950767159462
Distance: 7.653116703033447
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.2651449143886566
Distance: 7.642111778259277
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

