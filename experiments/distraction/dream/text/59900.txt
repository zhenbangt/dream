Env ID: [506]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.15469130873680115
Distance: 8.991437911987305
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.11430130153894424
Distance: 9.04612922668457
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.08275947719812393
Distance: 9.060430526733398
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.0811208486557007
Distance: 9.043190002441406
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.0799797996878624
Distance: 7.862069129943848
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.0009251609444618225
Distance: 7.682089328765869
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.12803134322166443
Distance: 7.583014488220215
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.1149572134017944
Distance: 7.611045837402344
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.44299545884132385
Distance: 6.396088600158691
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.19594964385032654
Distance: 5.853093147277832
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.4233179986476898
Distance: 5.949042797088623
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7480567693710327
Distance: 6.272360801696777
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.03871355205774307
Distance: 5.424304008483887
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.304471880197525
Distance: 5.363017559051514
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.11577930301427841
Distance: 4.958545684814453
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.16152915358543396
Distance: 4.742766380310059
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.13974609971046448
Distance: 4.804295539855957
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 5, 1], dtype=torch.int32)
Action: down
Reward: 0.029561422765254974
Distance: 4.564549446105957
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.18084296584129333
Distance: 4.434988021850586
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2761469781398773
Distance: 4.515830993652344
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

