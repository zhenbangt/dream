Env ID: [143]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.17836055159568787
Distance: 9.364219665527344
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12837085127830505
Distance: 9.442580223083496
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.11800537258386612
Distance: 9.470951080322266
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.05567321926355362
Distance: 9.488956451416016
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.1039958968758583
Distance: 9.444629669189453
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 3, 0], dtype=torch.int32)
Action: down
Reward: -0.3155418336391449
Distance: 9.448625564575195
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 2, 0], dtype=torch.int32)
Action: up
Reward: 0.11442317813634872
Distance: 9.664167404174805
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.0799327865242958
Distance: 9.44974422454834
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

