Env ID: [462]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.14744147658348083
Distance: 8.97203254699707
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.07424698024988174
Distance: 9.019474029541016
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.13700446486473083
Distance: 8.993721008300781
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.35658493638038635
Distance: 9.030725479125977
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.33259639143943787
Distance: 8.574140548706055
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.18418177962303162
Distance: 8.806736946105957
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.2717605531215668
Distance: 8.890918731689453
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.5253740549087524
Distance: 9.062679290771484
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.24108371138572693
Distance: 9.488053321838379
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.056645967066287994
Distance: 9.62913703918457
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.09615478664636612
Distance: 9.585783004760742
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.09008636325597763
Distance: 9.581937789916992
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.5305389165878296
Distance: 9.391851425170898
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.22558251023292542
Distance: 8.761312484741211
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.12527331709861755
Distance: 8.43572998046875
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.3008943498134613
Distance: 8.461003303527832
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1382499635219574
Distance: 8.661897659301758
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.12331733852624893
Distance: 8.70014762878418
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.22521552443504333
Distance: 8.723464965820312
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([6, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.0477357879281044
Distance: 8.84868049621582
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

