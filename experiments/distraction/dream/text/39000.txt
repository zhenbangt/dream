Env ID: [528]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12258873134851456
Distance: 7.753538131713867
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08103952556848526
Distance: 7.776126861572266
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.12057409435510635
Distance: 7.757166385650635
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.23532447218894958
Distance: 7.777740478515625
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.0428377166390419
Distance: 7.913064956665039
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.067906953394413
Distance: 7.855902671813965
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.20267638564109802
Distance: 7.823809623718262
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.07788381725549698
Distance: 7.926486015319824
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: 0.017311476171016693
Distance: 7.904369831085205
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.2371560037136078
Distance: 7.787058353424072
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.14257344603538513
Distance: 7.9242143630981445
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.16126546263694763
Distance: 7.966787815093994
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1443534791469574
Distance: 8.028053283691406
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.14092883467674255
Distance: 8.072406768798828
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.15230807662010193
Distance: 8.113335609436035
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.16219863295555115
Distance: 8.165643692016602
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.1295371949672699
Distance: 8.227842330932617
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.09244785457849503
Distance: 8.257379531860352
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10564479976892471
Distance: 8.24982738494873
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 7, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.00860748440027237
Distance: 8.255472183227539
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

