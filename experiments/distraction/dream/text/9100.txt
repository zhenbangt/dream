Env ID: [33]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.07898864895105362
Distance: 9.868757247924805
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.029003523290157318
Distance: 9.847745895385742
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: down
Reward: -0.21370086073875427
Distance: 9.718742370605469
Next state: tensor([0, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 0, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.19542083144187927
Distance: 9.832443237304688
Next state: tensor([0, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 0, 0], dtype=torch.int32)
Action: noop
Reward: -0.025495149195194244
Distance: 9.927864074707031
Next state: tensor([0, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 0, 0], dtype=torch.int32)
Action: noop
Reward: -0.3161798417568207
Distance: 9.85335922241211
Next state: tensor([0, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 0, 0], dtype=torch.int32)
Action: right
Reward: -0.3668142259120941
Distance: 10.069539070129395
Next state: tensor([1, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([1, 0, 0], dtype=torch.int32)
Action: pickup
Reward: -0.25773200392723083
Distance: 10.336353302001953
Next state: tensor([1, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([1, 0, 0], dtype=torch.int32)
Action: right
Reward: -0.004659272730350494
Distance: 10.494085311889648
Next state: tensor([2, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([2, 0, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.15152797102928162
Distance: 10.398744583129883
Next state: tensor([2, 0, 0], dtype=torch.int32)
================================================================================

