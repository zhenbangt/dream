Env ID: [484]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.15441474318504333
Distance: 9.468696594238281
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.01486358791589737
Distance: 9.523111343383789
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.2905009984970093
Distance: 9.43797492980957
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.12086953967809677
Distance: 8.047473907470703
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.11250648647546768
Distance: 7.82660436630249
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.024274446070194244
Distance: 7.839110851287842
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.20104989409446716
Distance: 7.76338529586792
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.09213504940271378
Distance: 7.864435195922852
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.15007677674293518
Distance: 7.856570243835449
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.30955275893211365
Distance: 7.6064934730529785
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.1077670082449913
Distance: 7.816046237945557
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.3640385568141937
Distance: 7.608279228210449
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.11996593326330185
Distance: 7.872317790985107
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.3804989755153656
Distance: 7.6523518562316895
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.1192707046866417
Distance: 7.9328508377075195
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.10656604915857315
Distance: 7.713580131530762
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.4927963316440582
Distance: 7.720146179199219
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.28498706221580505
Distance: 7.127349853515625
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: 0.02042093127965927
Distance: 7.3123369216918945
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.41061267256736755
Distance: 7.191915988922119
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

