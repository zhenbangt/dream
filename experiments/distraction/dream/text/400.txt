Env ID: [308]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.18308505415916443
Distance: 15.793867111206055
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.16848430037498474
Distance: 15.876952171325684
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.1216951385140419
Distance: 15.945436477661133
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.11238918453454971
Distance: 15.967131614685059
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 1, 1], dtype=torch.int32)
Action: up
Reward: -0.12232742458581924
Distance: 15.979520797729492
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([8, 2, 0], dtype=torch.int32)
Action: pickup
Reward: -0.05034599453210831
Distance: 16.001848220825195
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.09912262111902237
Distance: 15.952194213867188
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.08705482631921768
Distance: 15.951316833496094
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.097905732691288
Distance: 15.938371658325195
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.12504824995994568
Distance: 15.936277389526367
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.09607181698083878
Distance: 15.961325645446777
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.09728393703699112
Distance: 15.9573974609375
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 1, 1], dtype=torch.int32)
Action: up
Reward: -0.12862548232078552
Distance: 15.954681396484375
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.07771549373865128
Distance: 15.983306884765625
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.08560238033533096
Distance: 15.96102237701416
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.08706817775964737
Distance: 15.946624755859375
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.09555015712976456
Distance: 15.933692932128906
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.12915953993797302
Distance: 15.929243087768555
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.09349498897790909
Distance: 15.958402633666992
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.1035982146859169
Distance: 15.951897621154785
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

