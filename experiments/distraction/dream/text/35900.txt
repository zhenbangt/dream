Env ID: [121]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.17801913619041443
Distance: 9.71403694152832
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.0786357894539833
Distance: 9.7920560836792
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.24011477828025818
Distance: 9.770691871643066
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.003055192530155182
Distance: 9.910806655883789
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([6, 3, 0], dtype=torch.int32)
Action: down
Reward: -0.1077953353524208
Distance: 9.813861846923828
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.1964622437953949
Distance: 9.821657180786133
Next state: tensor([6, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 1, 0], dtype=torch.int32)
Action: down
Reward: 0.051271818578243256
Distance: 9.918119430541992
Next state: tensor([6, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 0, 0], dtype=torch.int32)
Action: noop
Reward: -0.06891212612390518
Distance: 9.766847610473633
Next state: tensor([6, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 0, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.07279548794031143
Distance: 9.735759735107422
Next state: tensor([6, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([6, 0, 0], dtype=torch.int32)
Action: down
Reward: 0.08278407901525497
Distance: 9.708555221557617
Next state: tensor([6, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([6, 0, 0], dtype=torch.int32)
Action: left
Reward: 0.07939662784337997
Distance: 9.525771141052246
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 0, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.029874421656131744
Distance: 9.34637451171875
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

