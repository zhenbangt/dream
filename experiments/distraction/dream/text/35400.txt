Env ID: [473]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.07151947170495987
Distance: 9.278604507446289
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9474862813949585
Distance: 9.250123977661133
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.26214274764060974
Distance: 8.202637672424316
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 1, 1], dtype=torch.int32)
Action: up
Reward: -0.03505764156579971
Distance: 8.36478042602539
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 2, 0], dtype=torch.int32)
Action: left
Reward: -0.08075962215662003
Distance: 8.299838066101074
Next state: tensor([7, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([7, 2, 0], dtype=torch.int32)
Action: left
Reward: -0.4094320237636566
Distance: 8.280597686767578
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 2, 0], dtype=torch.int32)
Action: drop
Reward: -0.27816447615623474
Distance: 8.5900297164917
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 2, 0], dtype=torch.int32)
Action: left
Reward: -0.5364929437637329
Distance: 8.768194198608398
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 2, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.19133147597312927
Distance: 9.204687118530273
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

