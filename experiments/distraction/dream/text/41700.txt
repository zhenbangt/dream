Env ID: [341]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.0899253860116005
Distance: 8.735301971435547
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.2287517488002777
Distance: 8.725227355957031
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.005218885838985443
Distance: 8.853979110717773
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.14937743544578552
Distance: 8.748760223388672
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.061917878687381744
Distance: 8.798137664794922
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.008097074925899506
Distance: 8.760055541992188
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.027019880712032318
Distance: 8.651958465576172
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.025722123682498932
Distance: 8.524938583374023
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.0008398070931434631
Distance: 8.450660705566406
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: 0.029235266149044037
Distance: 8.351500511169434
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: 0.03127994388341904
Distance: 8.222265243530273
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.02201233059167862
Distance: 8.090985298156738
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.24342402815818787
Distance: 8.0129976272583
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.1848808228969574
Distance: 8.156421661376953
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.2197767198085785
Distance: 8.241302490234375
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.27653560042381287
Distance: 8.361079216003418
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.30086764693260193
Distance: 8.537614822387695
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.22415676712989807
Distance: 8.738482475280762
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.47359713912010193
Distance: 8.414325714111328
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.964913010597229
Distance: 8.787922859191895
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

