Env ID: [66]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.05400810390710831
Distance: 8.003725051879883
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.07452879101037979
Distance: 7.957733154296875
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.42124947905540466
Distance: 7.932261943817139
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3207927644252777
Distance: 8.253511428833008
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: 0.31085625290870667
Distance: 8.47430419921875
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.15536078810691833
Distance: 8.063447952270508
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.5795150995254517
Distance: 8.11880874633789
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.5553661584854126
Distance: 8.598323822021484
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.06422386318445206
Distance: 9.053689956665039
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([1, 1, 0], dtype=torch.int32)
Action: right
Reward: -0.510633111000061
Distance: 9.017913818359375
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([2, 1, 0], dtype=torch.int32)
Action: right
Reward: 0.28305092453956604
Distance: 9.428546905517578
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 1, 0], dtype=torch.int32)
Action: drop
Reward: 0.15565720200538635
Distance: 9.045495986938477
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([3, 1, 0], dtype=torch.int32)
Action: down
Reward: 0.07510509341955185
Distance: 8.789838790893555
Next state: tensor([3, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([3, 0, 0], dtype=torch.int32)
Action: left
Reward: -0.06581554561853409
Distance: 8.614733695983887
Next state: tensor([2, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([2, 0, 0], dtype=torch.int32)
Action: down
Reward: -0.12088260799646378
Distance: 8.580549240112305
Next state: tensor([2, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([2, 0, 0], dtype=torch.int32)
Action: left
Reward: 0.2830299437046051
Distance: 8.601431846618652
Next state: tensor([1, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([1, 0, 0], dtype=torch.int32)
Action: up
Reward: -0.3824087083339691
Distance: 8.218401908874512
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([1, 1, 0], dtype=torch.int32)
Action: left
Reward: -0.588748574256897
Distance: 8.500810623168945
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.19593772292137146
Distance: 8.989559173583984
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7462447881698608
Distance: 9.08549690246582
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

