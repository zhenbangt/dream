Env ID: [506]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.081243135035038
Distance: 9.491363525390625
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.18223246932029724
Distance: 9.472606658935547
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.09043750911951065
Distance: 9.554839134216309
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.07737503200769424
Distance: 9.545276641845703
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.020205117762088776
Distance: 9.522651672363281
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.44639721512794495
Distance: 9.442856788635254
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.4041343629360199
Distance: 8.896459579467773
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.16642913222312927
Distance: 9.200593948364258
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.038805387914180756
Distance: 9.267023086547852
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.35262736678123474
Distance: 9.128217697143555
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2889379560947418
Distance: 9.380845069885254
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3209167420864105
Distance: 8.991907119750977
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.016201592981815338
Distance: 9.212823867797852
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.004822157323360443
Distance: 9.12902545928955
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.30740222334861755
Distance: 9.024203300476074
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.004647634923458099
Distance: 9.231605529785156
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.0435853973031044
Distance: 9.126957893371582
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.23061904311180115
Distance: 9.07054328918457
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.19034823775291443
Distance: 9.201162338256836
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.06008205562829971
Distance: 9.291510581970215
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

