Env ID: [99]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.07742748409509659
Distance: 9.803879737854004
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: 0.0999855026602745
Distance: 9.781307220458984
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.3289075791835785
Distance: 9.581321716308594
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.18850383162498474
Distance: 9.810229301452637
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.0011600479483604431
Distance: 9.898733139038086
Next state: tensor([6, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 5, 0], dtype=torch.int32)
Action: left
Reward: -0.017629243433475494
Distance: 9.79757308959961
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.19034156203269958
Distance: 9.715202331542969
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.15745124220848083
Distance: 9.805543899536133
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.039713285863399506
Distance: 9.862995147705078
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.013348199427127838
Distance: 9.723281860351562
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([1, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.19661179184913635
Distance: 9.636630058288574
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([1, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.4160486161708832
Distance: 9.340018272399902
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([2, 4, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.3900085389614105
Distance: 9.65606689453125
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

