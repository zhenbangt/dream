Env ID: [330]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.011894799768924713
Distance: 9.565422058105469
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.16485747694969177
Distance: 9.477316856384277
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1621910035610199
Distance: 9.542174339294434
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.25462016463279724
Distance: 9.604365348815918
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.34878501296043396
Distance: 9.75898551940918
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.19453296065330505
Distance: 10.007770538330078
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.08296165615320206
Distance: 10.102303504943848
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: 0.011382482945919037
Distance: 10.085265159606934
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.06134758144617081
Distance: 9.973882675170898
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.06734047085046768
Distance: 9.935230255126953
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.10053405910730362
Distance: 9.902570724487305
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.05809745937585831
Distance: 9.903104782104492
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.011805154383182526
Distance: 9.861202239990234
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: 0.024917028844356537
Distance: 9.7730073928833
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.012005425989627838
Distance: 9.648090362548828
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.049489594995975494
Distance: 9.56009578704834
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: left
Reward: -0.20443400740623474
Distance: 9.5095853805542
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.3647352159023285
Distance: 9.614019393920898
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.010989762842655182
Distance: 9.878754615783691
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.16101989150047302
Distance: 9.78974437713623
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

