Env ID: [264]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.0936347022652626
Distance: 7.12015962600708
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.19364938139915466
Distance: 7.113794326782227
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.28189095854759216
Distance: 7.207443714141846
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.31832465529441833
Distance: 7.389334678649902
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.43879184126853943
Distance: 7.607659339904785
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 7, 1], dtype=torch.int32)
Action: drop
Reward: 0.04460134357213974
Distance: 7.946451187133789
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 7, 1], dtype=torch.int32)
Action: down
Reward: -0.19363698363304138
Distance: 7.801849842071533
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 6, 0], dtype=torch.int32)
Action: drop
Reward: -0.34216079115867615
Distance: 7.895486831665039
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 6, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.1283210813999176
Distance: 8.13764762878418
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

