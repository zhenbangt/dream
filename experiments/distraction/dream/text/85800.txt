Env ID: [275]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.13404425978660583
Distance: 9.863643646240234
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.13322219252586365
Distance: 9.897687911987305
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.07362232357263565
Distance: 9.930910110473633
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.8196138143539429
Distance: 9.904532432556152
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.07916317135095596
Distance: 8.984918594360352
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.13343486189842224
Distance: 8.964081764221191
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.013046838343143463
Distance: 8.997516632080078
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.67503821849823
Distance: 8.910563468933105
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.17166271805763245
Distance: 7.135525226593018
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.06181631237268448
Distance: 6.86386251449585
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.10036716610193253
Distance: 6.825678825378418
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.533370852470398
Distance: 6.826045989990234
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.09163274616003036
Distance: 5.1926751136779785
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: 0.20021572709083557
Distance: 5.001042366027832
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.16843852400779724
Distance: 4.700826644897461
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.29434022307395935
Distance: 4.769265174865723
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.821955680847168
Distance: 4.9636054039001465
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.15485461056232452
Distance: 0.0416499488055706
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.060365933924913406
Distance: 0.09650455415248871
Next state: tensor([6, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([6, 5, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.14014720916748047
Distance: 0.056870486587285995
Next state: tensor([6, 5, 0], dtype=torch.int32)
================================================================================

