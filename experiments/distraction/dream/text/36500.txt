Env ID: [550]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.14786109328269958
Distance: 7.630322456359863
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.1354261338710785
Distance: 7.678183555603027
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.0089506134390831
Distance: 7.71360969543457
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.051755525171756744
Distance: 7.604659080505371
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.0968223586678505
Distance: 7.556414604187012
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 3, 0], dtype=torch.int32)
Action: down
Reward: 0.044437311589717865
Distance: 7.553236961364746
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.13800200819969177
Distance: 7.408799648284912
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 2, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.12366876751184464
Distance: 7.446801662445068
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

