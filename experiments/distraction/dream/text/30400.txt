Env ID: [253]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.04046592861413956
Distance: 4.45211124420166
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.011721991002559662
Distance: 4.392577171325684
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.16577300429344177
Distance: 4.280855178833008
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 5, 1], dtype=torch.int32)
Action: drop
Reward: -0.020263291895389557
Distance: 4.346628189086914
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 5, 1], dtype=torch.int32)
Action: up
Reward: -0.0022870078682899475
Distance: 4.2668914794921875
Next state: tensor([4, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.10647878795862198
Distance: 4.169178485870361
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.031670667231082916
Distance: 4.175657272338867
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.08321104198694229
Distance: 4.107327938079834
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.16353997588157654
Distance: 4.09053897857666
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

