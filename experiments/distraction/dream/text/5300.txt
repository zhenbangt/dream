Env ID: [473]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08481941372156143
Distance: 8.58886432647705
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.19968661665916443
Distance: 8.573683738708496
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.18839111924171448
Distance: 8.673370361328125
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([6, 3, 0], dtype=torch.int32)
Action: up
Reward: -0.31582698225975037
Distance: 8.384979248046875
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([6, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.008163072168827057
Distance: 8.60080623626709
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([7, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.17516669631004333
Distance: 8.5089693069458
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.3266015946865082
Distance: 8.584136009216309
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.13453063368797302
Distance: 8.810737609863281
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.25418147444725037
Distance: 8.845268249511719
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([6, 4, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.05454196780920029
Distance: 8.999449729919434
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

