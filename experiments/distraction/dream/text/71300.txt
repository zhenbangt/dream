Env ID: [209]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.053586579859256744
Distance: 9.022947311401367
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.093022920191288
Distance: 8.976533889770508
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.1153770461678505
Distance: 8.96955680847168
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.12349281460046768
Distance: 8.984933853149414
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.08938083797693253
Distance: 9.008426666259766
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3802780210971832
Distance: 8.997807502746582
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1153474822640419
Distance: 8.517529487609863
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.24241217970848083
Distance: 8.532876968383789
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.09207306057214737
Distance: 8.675289154052734
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.032710075378418
Distance: 8.667362213134766
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.28776559233665466
Distance: 6.534652233123779
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.228267103433609
Distance: 6.722417831420898
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.26440104842185974
Distance: 6.394150733947754
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.3691266775131226
Distance: 6.558551788330078
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3802371919155121
Distance: 5.089425086975098
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.1895643174648285
Distance: 5.369662284851074
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.6735901832580566
Distance: 5.459226608276367
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 0, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.07665958255529404
Distance: 2.685636520385742
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

