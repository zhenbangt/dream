Env ID: [165]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.05522117763757706
Distance: 9.519664764404297
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.107521414756775
Distance: 9.474885940551758
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.12153492122888565
Distance: 8.267364501953125
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.07972774654626846
Distance: 8.288899421691895
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.04694519191980362
Distance: 8.268627166748047
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.776272177696228
Distance: 8.215572357177734
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.30066433548927307
Distance: 6.339300155639648
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.519351601600647
Distance: 5.93863582611084
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.25565680861473083
Distance: 6.357987403869629
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2510165274143219
Distance: 6.513644218444824
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 7, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.050896741449832916
Distance: 6.162627696990967
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

