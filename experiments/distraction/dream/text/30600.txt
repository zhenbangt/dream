Env ID: [517]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.22857961058616638
Distance: 6.935596942901611
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7512925863265991
Distance: 7.064176559448242
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.42578181624412537
Distance: 6.212883949279785
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 5, 1], dtype=torch.int32)
Action: drop
Reward: -0.22687777876853943
Distance: 6.538665771484375
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.18346557021141052
Distance: 6.665543556213379
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.15143147110939026
Distance: 6.749009132385254
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 8, 1], dtype=torch.int32)
Action: noop
Reward: 0.2128686010837555
Distance: 6.497577667236328
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 8, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.009728528559207916
Distance: 6.184709072113037
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

