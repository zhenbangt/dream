Env ID: [121]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.13761672377586365
Distance: 9.749481201171875
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.009519197046756744
Distance: 9.787097930908203
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.09735450893640518
Distance: 9.696617126464844
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.4639907777309418
Distance: 9.693971633911133
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 0, 1], dtype=torch.int32)
Action: right
Reward: 0.21573010087013245
Distance: 10.057962417602539
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 0, 0], dtype=torch.int32)
Action: noop
Reward: 0.6806949377059937
Distance: 9.742232322692871
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 0, 0], dtype=torch.int32)
Action: up
Reward: -0.11260662227869034
Distance: 8.96153736114502
Next state: tensor([5, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 1, 0], dtype=torch.int32)
Action: down
Reward: -0.1410737931728363
Distance: 8.974143981933594
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 0, 0], dtype=torch.int32)
Action: up
Reward: -0.16741618514060974
Distance: 9.015217781066895
Next state: tensor([5, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 1, 0], dtype=torch.int32)
Action: down
Reward: 0.0007776245474815369
Distance: 9.082633972167969
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 0, 0], dtype=torch.int32)
Action: down
Reward: 0.19960156083106995
Distance: 8.981856346130371
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 0, 0], dtype=torch.int32)
Action: up
Reward: 0.025599859654903412
Distance: 8.682254791259766
Next state: tensor([5, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 1, 0], dtype=torch.int32)
Action: down
Reward: -0.007640458643436432
Distance: 8.556654930114746
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 0, 0], dtype=torch.int32)
Action: down
Reward: 0.15936985611915588
Distance: 8.464295387268066
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 0, 0], dtype=torch.int32)
Action: down
Reward: -0.006223298609256744
Distance: 8.204925537109375
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 0, 0], dtype=torch.int32)
Action: down
Reward: -0.33863505721092224
Distance: 8.111148834228516
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 0, 0], dtype=torch.int32)
Action: down
Reward: -0.910957932472229
Distance: 8.349783897399902
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 0, 0], dtype=torch.int32)
Action: down
Reward: -0.8754135370254517
Distance: 9.160741806030273
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 0, 0], dtype=torch.int32)
Action: down
Reward: -0.4580703675746918
Distance: 9.936155319213867
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 0, 0], dtype=torch.int32)
Action: down
Reward: -0.1718955934047699
Distance: 10.294225692749023
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

