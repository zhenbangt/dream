Env ID: [110]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.03306923061609268
Distance: 9.165891647338867
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.11556778103113174
Distance: 9.098960876464844
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.14454039931297302
Distance: 9.11452865600586
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([6, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.0666099563241005
Distance: 9.159069061279297
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([6, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.21829184889793396
Distance: 9.125679016113281
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1423412263393402
Distance: 9.24397087097168
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([7, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.23687037825584412
Distance: 9.286312103271484
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([7, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.26144084334373474
Distance: 9.423182487487793
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([7, 3, 0], dtype=torch.int32)
Action: right
Reward: 0.15620461106300354
Distance: 9.584623336791992
Next state: tensor([8, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 3, 0], dtype=torch.int32)
Action: left
Reward: 0.05872001498937607
Distance: 9.328418731689453
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([7, 3, 0], dtype=torch.int32)
Action: right
Reward: 0.20364704728126526
Distance: 9.169698715209961
Next state: tensor([8, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 3, 0], dtype=torch.int32)
Action: down
Reward: -0.0971284881234169
Distance: 8.86605167388916
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.07633266597986221
Distance: 8.863180160522461
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.003226853907108307
Distance: 8.839512825012207
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.07349071651697159
Distance: 8.7427396774292
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.1020885482430458
Distance: 8.716230392456055
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.1037517562508583
Distance: 8.718318939208984
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 2, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12153968960046768
Distance: 8.722070693969727
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 2, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.22886809706687927
Distance: 8.743610382080078
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

