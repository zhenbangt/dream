Env ID: [517]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1838112771511078
Distance: 7.107748031616211
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.15281352400779724
Distance: 7.191559314727783
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: 0.020063303411006927
Distance: 7.244372844696045
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.05147991329431534
Distance: 7.124309539794922
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.07695350795984268
Distance: 7.075789451599121
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.18395480513572693
Distance: 7.052742958068848
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.2088838517665863
Distance: 7.136697769165039
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1610794961452484
Distance: 7.24558162689209
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.24432620406150818
Distance: 7.306661128997803
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.0027371421456336975
Distance: 7.450987339019775
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.34293755888938904
Distance: 7.353724479675293
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.0615633949637413
Distance: 7.5966620445251465
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.11747302860021591
Distance: 7.435098648071289
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

