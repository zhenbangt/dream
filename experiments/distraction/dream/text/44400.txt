Env ID: [143]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.08924923092126846
Distance: 9.186921119689941
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.16846045851707458
Distance: 9.176170349121094
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.303593248128891
Distance: 9.244630813598633
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.23379096388816833
Distance: 9.448224067687988
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.23367270827293396
Distance: 9.582015037536621
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: 0.029006384313106537
Distance: 9.71568775177002
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.33955517411231995
Distance: 9.586681365966797
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -1.1211096048355103
Distance: 9.147126197814941
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.47167912125587463
Distance: 10.168235778808594
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.19010314345359802
Distance: 9.596556663513184
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.16586551070213318
Distance: 9.686659812927246
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: 0.16328755021095276
Distance: 9.752525329589844
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.12748393416404724
Distance: 9.489237785339355
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: 0.04371776431798935
Distance: 9.516721725463867
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.17982539534568787
Distance: 9.373003959655762
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.16747912764549255
Distance: 9.452829360961914
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.11678562313318253
Distance: 9.520308494567871
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.04171047359704971
Distance: 9.537094116210938
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -1.0329128503799438
Distance: 9.478804588317871
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.6190227270126343
Distance: 10.411717414855957
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

