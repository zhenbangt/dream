Env ID: [385]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.09326515346765518
Distance: 7.233382225036621
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09976444393396378
Distance: 7.22664737701416
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.5301040410995483
Distance: 7.226411819458008
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.15850552916526794
Distance: 5.596307754516602
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.09648952633142471
Distance: 5.654813289642334
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.26367196440696716
Distance: 5.651302814483643
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.16848143935203552
Distance: 5.814974784851074
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7399648427963257
Distance: 5.883456230163574
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.25159892439842224
Distance: 5.043491363525391
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.07244119793176651
Distance: 5.195090293884277
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.1534501016139984
Distance: 5.167531490325928
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.5447882413864136
Distance: 5.220981597900391
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.07490024715662003
Distance: 3.576193332672119
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 1, 1], dtype=torch.int32)
Action: up
Reward: -0.6706897020339966
Distance: 3.551093578338623
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.641396164894104
Distance: 4.121783256530762
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.07338438183069229
Distance: 4.663179397583008
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.132392019033432
Distance: 4.636563777923584
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.362356185913086
Distance: 4.6689558029174805
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.11886133998632431
Distance: 0.20659972727298737
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

