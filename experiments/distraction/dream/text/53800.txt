Env ID: [176]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.14983996748924255
Distance: 9.1412935256958
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.12781867384910583
Distance: 9.191133499145508
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.022223852574825287
Distance: 9.218952178955078
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.14689597487449646
Distance: 9.096728324890137
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.33436450362205505
Distance: 9.143624305725098
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([2, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1698070466518402
Distance: 9.377988815307617
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.19871768355369568
Distance: 9.447795867919922
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.18993529677391052
Distance: 9.546513557434082
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.15045222640037537
Distance: 9.636448860168457
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.15368136763572693
Distance: 9.686901092529297
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1587892472743988
Distance: 9.740582466125488
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.1305910050868988
Distance: 9.799371719360352
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.212986558675766
Distance: 9.829962730407715
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: 0.05281867831945419
Distance: 9.942949295043945
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.11536941677331924
Distance: 9.790130615234375
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: 0.04329242557287216
Distance: 9.805500030517578
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.009974859654903412
Distance: 9.66220760345459
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.15432509779930115
Distance: 9.55223274230957
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([1, 1, 0], dtype=torch.int32)
Action: right
Reward: -0.19327887892723083
Distance: 9.606557846069336
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([2, 1, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.06690540164709091
Distance: 9.699836730957031
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

