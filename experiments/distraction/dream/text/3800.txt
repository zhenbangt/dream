Env ID: [528]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.18735942244529724
Distance: 8.321677207946777
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.2930942475795746
Distance: 8.409036636352539
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([6, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.03828277438879013
Distance: 8.602130889892578
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([7, 4, 0], dtype=torch.int32)
Action: drop
Reward: 0.17837563157081604
Distance: 8.463848114013672
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([7, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.013954736292362213
Distance: 8.18547248840332
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([7, 4, 0], dtype=torch.int32)
Action: up
Reward: 0.09352817386388779
Distance: 8.099427223205566
Next state: tensor([7, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([7, 5, 0], dtype=torch.int32)
Action: down
Reward: 0.06666507571935654
Distance: 7.9058990478515625
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([7, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.009546853601932526
Distance: 7.73923397064209
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([7, 3, 0], dtype=torch.int32)
Action: right
Reward: -0.08989868313074112
Distance: 7.648780822753906
Next state: tensor([8, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 3, 0], dtype=torch.int32)
Action: left
Reward: -0.4402490556240082
Distance: 7.638679504394531
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([7, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.27151164412498474
Distance: 7.978928565979004
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

