Env ID: [165]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.02391185611486435
Distance: 8.01004695892334
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.5899721384048462
Distance: 7.886135101318359
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.5354887247085571
Distance: 8.376107215881348
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.7823997735977173
Distance: 8.811595916748047
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.2532821595668793
Distance: 9.493995666503906
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 2, 0], dtype=torch.int32)
Action: drop
Reward: -0.6903759241104126
Distance: 9.64727783203125
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.472970575094223
Distance: 10.237653732299805
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 2, 0], dtype=torch.int32)
Action: up
Reward: 0.05327644199132919
Distance: 10.610624313354492
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2943168580532074
Distance: 10.457347869873047
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.1267467439174652
Distance: 10.651664733886719
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.9248565435409546
Distance: 10.678411483764648
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

