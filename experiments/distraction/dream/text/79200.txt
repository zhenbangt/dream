Env ID: [33]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.1280733048915863
Distance: 9.1494722366333
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.0942150130867958
Distance: 9.177545547485352
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3057970106601715
Distance: 9.171760559082031
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10700664669275284
Distance: 8.765963554382324
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.03562507778406143
Distance: 8.772970199584961
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.13820132613182068
Distance: 8.708595275878906
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.844535231590271
Distance: 8.746796607971191
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.12757167220115662
Distance: 7.8022613525390625
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.0012496933341026306
Distance: 7.829833030700684
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.164707750082016
Distance: 7.728583335876465
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2645963728427887
Distance: 7.793291091918945
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.037273503839969635
Distance: 7.428694725036621
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.4109383523464203
Distance: 7.365968227386475
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 7.434053421020508
Distance: 7.676906585693359
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.0057104602456092834
Distance: 0.14285308122634888
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.0911579430103302
Distance: 0.03714262321591377
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

