Env ID: [231]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.1483655869960785
Distance: 8.508801460266113
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.10177192836999893
Distance: 8.557167053222656
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.030360795557498932
Distance: 8.558938980102539
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.6016658544540405
Distance: 8.489299774169922
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.024456597864627838
Distance: 7.787633895874023
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: 0.11009921878576279
Distance: 7.712090492248535
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.26800593733787537
Distance: 7.501991271972656
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.06622495502233505
Distance: 7.669997215270996
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 7, 1], dtype=torch.int32)
Action: down
Reward: -0.04029283672571182
Distance: 7.503772258758545
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.526736855506897
Distance: 7.444065093994141
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 5, 0], dtype=torch.int32)
Action: drop
Reward: -0.15747222304344177
Distance: 7.87080192565918
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 5, 0], dtype=torch.int32)
Action: noop
Reward: -0.20036372542381287
Distance: 7.928274154663086
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 5, 0], dtype=torch.int32)
Action: right
Reward: -0.3712678849697113
Distance: 8.028637886047363
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([1, 5, 0], dtype=torch.int32)
Action: right
Reward: -0.1380363404750824
Distance: 8.299905776977539
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([2, 5, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.113977812230587
Distance: 8.337942123413086
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

