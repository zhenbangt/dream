Env ID: [561]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.3074890077114105
Distance: 9.736374855041504
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.01059093326330185
Distance: 9.943863868713379
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.0018974319100379944
Distance: 9.833272933959961
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.0705476775765419
Distance: 9.735170364379883
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.030102156102657318
Distance: 9.705718040466309
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.3348928391933441
Distance: 9.575615882873535
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.1560417115688324
Distance: 9.810508728027344
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.2424064576625824
Distance: 9.86655044555664
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.23655280470848083
Distance: 10.008956909179688
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

