Env ID: [176]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.1307426393032074
Distance: 9.252325057983398
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1032329574227333
Distance: 9.28306770324707
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.04650459438562393
Distance: 9.286300659179688
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.006333924829959869
Distance: 9.232805252075195
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1619010865688324
Distance: 9.139139175415039
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1123729720711708
Distance: 9.201040267944336
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.24349746108055115
Distance: 9.21341323852539
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.29748114943504333
Distance: 9.356910705566406
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.25433310866355896
Distance: 9.554391860961914
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1697002351284027
Distance: 9.708724975585938
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.10550365597009659
Distance: 9.778425216674805
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.07154140621423721
Distance: 9.783928871154785
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.12569770216941833
Distance: 9.755470275878906
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1584840714931488
Distance: 9.781167984008789
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.16299495100975037
Distance: 9.839652061462402
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.17774733901023865
Distance: 9.902647018432617
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.19470176100730896
Distance: 9.98039436340332
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.20067939162254333
Distance: 10.075096130371094
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1691661775112152
Distance: 10.175775527954102
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.128178209066391
Distance: 10.244941711425781
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

