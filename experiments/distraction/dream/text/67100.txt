Env ID: [341]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.06503257900476456
Distance: 9.532573699951172
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.11337528377771378
Distance: 9.49760627746582
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09761963039636612
Distance: 9.510981559753418
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2091001570224762
Distance: 9.508601188659668
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1391386091709137
Distance: 9.199501037597656
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.21496924757957458
Distance: 8.960362434387207
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.09405479580163956
Distance: 9.075331687927246
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.044077493250370026
Distance: 9.06938648223877
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.438419908285141
Distance: 9.013463973999023
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.18100985884666443
Distance: 9.351883888244629
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.18555030226707458
Distance: 9.432893753051758
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.721876621246338
Distance: 9.518444061279297
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.48753318190574646
Distance: 4.696567535400391
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.025929830968379974
Distance: 5.084100723266602
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7928565740585327
Distance: 4.9581708908081055
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.32203444838523865
Distance: 4.065314292907715
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.1400218904018402
Distance: 4.287348747253418
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

