Env ID: [110]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.1547618806362152
Distance: 9.75131607055664
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: down
Reward: 0.03982963413000107
Distance: 9.80607795715332
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.16043242812156677
Distance: 9.666248321533203
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.15764561295509338
Distance: 9.726680755615234
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.6343809366226196
Distance: 9.469035148620605
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.23937949538230896
Distance: 10.003416061401367
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 3, 0], dtype=torch.int32)
Action: noop
Reward: -0.1074901595711708
Distance: 10.14279556274414
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 3, 0], dtype=torch.int32)
Action: noop
Reward: -0.324990838766098
Distance: 10.150285720825195
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 3, 0], dtype=torch.int32)
Action: noop
Reward: -0.0929943099617958
Distance: 10.375276565551758
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([6, 3, 0], dtype=torch.int32)
Action: noop
Reward: -0.21428069472312927
Distance: 10.368270874023438
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([6, 3, 0], dtype=torch.int32)
Action: right
Reward: -0.5636364221572876
Distance: 10.482551574707031
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([7, 3, 0], dtype=torch.int32)
Action: drop
Reward: -0.30390986800193787
Distance: 10.946187973022461
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([7, 3, 0], dtype=torch.int32)
Action: drop
Reward: -0.05404338985681534
Distance: 11.150097846984863
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([7, 3, 0], dtype=torch.int32)
Action: drop
Reward: -0.22169360518455505
Distance: 11.104141235351562
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([7, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.2312702238559723
Distance: 11.225834846496582
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

