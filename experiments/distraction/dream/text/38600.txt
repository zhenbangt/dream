Env ID: [495]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.0659252181649208
Distance: 5.037473678588867
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1891704499721527
Distance: 5.003398895263672
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.13320693373680115
Distance: 5.092569351196289
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.10610542446374893
Distance: 5.125776290893555
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.09640560299158096
Distance: 5.1318817138671875
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.05657253414392471
Distance: 5.128287315368652
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.09760961681604385
Distance: 5.084859848022461
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: 0.012526892125606537
Distance: 5.082469463348389
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.14167556166648865
Distance: 4.969942569732666
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.06405887752771378
Distance: 5.011618137359619
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.0907040610909462
Distance: 4.975677013397217
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.14311513304710388
Distance: 4.966381072998047
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.12768325209617615
Distance: 5.009496212005615
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.11620912700891495
Distance: 5.037179470062256
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.10668430477380753
Distance: 5.053388595581055
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.11824379116296768
Distance: 5.060072898864746
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.11639080196619034
Distance: 5.078316688537598
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.08361683040857315
Distance: 5.094707489013672
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.0705166831612587
Distance: 5.078324317932129
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.06663761287927628
Distance: 5.0488409996032715
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

