Env ID: [154]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.06843910366296768
Distance: 8.074905395507812
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.10211143642663956
Distance: 8.043344497680664
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.10068855434656143
Distance: 8.045455932617188
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.08380565792322159
Distance: 8.046144485473633
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.044484712183475494
Distance: 8.029950141906738
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.2627931535243988
Distance: 7.974434852600098
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.2111888825893402
Distance: 8.137228012084961
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.19796141982078552
Distance: 8.248416900634766
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.1736980378627777
Distance: 8.346378326416016
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.18194904923439026
Distance: 8.420076370239258
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 0, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.10636558383703232
Distance: 8.138127326965332
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

