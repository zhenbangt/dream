Env ID: [418]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.06478843837976456
Distance: 9.775568008422852
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7920783758163452
Distance: 9.7403564453125
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.10463085025548935
Distance: 8.848278045654297
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.05644092708826065
Distance: 8.643647193908691
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.15874633193016052
Distance: 8.600088119506836
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.5600699186325073
Distance: 8.658834457397461
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3648110330104828
Distance: 7.998764514923096
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.46389254927635193
Distance: 8.263575553894043
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.11722908169031143
Distance: 8.62746810913086
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.3333563804626465
Distance: 8.644697189331055
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7662571668624878
Distance: 6.21134090423584
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.08921919018030167
Distance: 5.345083713531494
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.282092571258545
Distance: 5.33430290222168
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3305400311946869
Distance: 2.9522104263305664
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.14238110184669495
Distance: 3.1827504634857178
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.517903208732605
Distance: 2.9403693675994873
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

