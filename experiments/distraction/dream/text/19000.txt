Env ID: [418]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.16632232069969177
Distance: 9.312747955322266
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.16101035475730896
Distance: 9.379070281982422
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.04040565341711044
Distance: 9.440080642700195
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.38052138686180115
Distance: 9.299674987792969
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.22265473008155823
Distance: 9.580196380615234
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: 0.381338506937027
Distance: 9.25754165649414
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([1, 1, 0], dtype=torch.int32)
Action: drop
Reward: 0.3363327085971832
Distance: 8.776203155517578
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([1, 1, 0], dtype=torch.int32)
Action: right
Reward: -0.199345201253891
Distance: 8.33987045288086
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([2, 1, 0], dtype=torch.int32)
Action: right
Reward: -0.12348423153162003
Distance: 8.439215660095215
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 1, 0], dtype=torch.int32)
Action: right
Reward: -0.15624961256980896
Distance: 8.462699890136719
Next state: tensor([4, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 1, 0], dtype=torch.int32)
Action: up
Reward: 0.044390104711055756
Distance: 8.518949508666992
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.15199622511863708
Distance: 8.37455940246582
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 2, 0], dtype=torch.int32)
Action: noop
Reward: -0.2560287415981293
Distance: 8.426555633544922
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.14894160628318787
Distance: 8.582584381103516
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 2, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.22993049025535583
Distance: 8.631525993347168
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

