Env ID: [198]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.003096960484981537
Distance: 9.217129707336426
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.006782151758670807
Distance: 9.114032745361328
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.3683229386806488
Distance: 9.020814895629883
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.08787956088781357
Distance: 9.289137840270996
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.20136681199073792
Distance: 9.101258277893066
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.05432186275720596
Distance: 8.799891471862793
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.554848313331604
Distance: 8.754213333129883
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.6007639169692993
Distance: 9.209061622619629
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.3015685975551605
Distance: 9.70982551574707
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

