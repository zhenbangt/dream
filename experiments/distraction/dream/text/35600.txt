Env ID: [33]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.1370912492275238
Distance: 8.81551456451416
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.056109048426151276
Distance: 8.852605819702148
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([2, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.20387306809425354
Distance: 8.808714866638184
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([2, 3, 0], dtype=torch.int32)
Action: right
Reward: 0.11608829349279404
Distance: 8.504841804504395
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.47132548689842224
Distance: 8.288753509521484
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 0, 1], dtype=torch.int32)
Action: drop
Reward: -0.22802028059959412
Distance: 8.660079002380371
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.8822027444839478
Distance: 8.78809928894043
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.8608995676040649
Distance: 9.57030200958252
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.5264259576797485
Distance: 10.331201553344727
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.01397857815027237
Distance: 10.757627487182617
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.11751136928796768
Distance: 10.671606063842773
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 2, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.015895463526248932
Distance: 10.689117431640625
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

