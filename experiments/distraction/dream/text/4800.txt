Env ID: [297]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.032892800867557526
Distance: 9.511199951171875
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.09096679836511612
Distance: 9.444092750549316
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.1475273072719574
Distance: 9.435059547424316
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.030864141881465912
Distance: 9.482586860656738
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.15063628554344177
Distance: 9.351722717285156
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.06946716457605362
Distance: 9.402359008789062
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: pickup
Reward: 0.07623615115880966
Distance: 9.371826171875
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.04304657131433487
Distance: 9.195590019226074
Next state: tensor([6, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 5, 0], dtype=torch.int32)
Action: up
Reward: -0.02624664455652237
Distance: 9.138636589050293
Next state: tensor([6, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([6, 6, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.03237781673669815
Distance: 9.0648832321167
Next state: tensor([6, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([6, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.12406215816736221
Distance: 8.997261047363281
Next state: tensor([6, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([6, 5, 0], dtype=torch.int32)
Action: right
Reward: -0.020121194422245026
Distance: 9.021323204040527
Next state: tensor([7, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([7, 5, 0], dtype=torch.int32)
Action: right
Reward: -0.028377152979373932
Distance: 8.941444396972656
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 5, 0], dtype=torch.int32)
Action: pickup
Reward: -0.16707763075828552
Distance: 8.869821548461914
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 5, 0], dtype=torch.int32)
Action: right
Reward: -0.13283595442771912
Distance: 8.936899185180664
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 5, 0], dtype=torch.int32)
Action: drop
Reward: -0.013369180262088776
Distance: 8.969735145568848
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 5, 0], dtype=torch.int32)
Action: left
Reward: -0.26126059889793396
Distance: 8.88310432434082
Next state: tensor([7, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([7, 5, 0], dtype=torch.int32)
Action: right
Reward: -0.04569397121667862
Distance: 9.044364929199219
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 5, 0], dtype=torch.int32)
Action: up
Reward: 0.018513105809688568
Distance: 8.990058898925781
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.11652527004480362
Distance: 8.871545791625977
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

