Env ID: [572]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.06080207973718643
Distance: 9.864974975585938
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.700029730796814
Distance: 9.825777053833008
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.15832862257957458
Distance: 9.025747299194336
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.34028300642967224
Distance: 9.084075927734375
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.07499561458826065
Distance: 9.324358940124512
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.051424406468868256
Distance: 9.299354553222656
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.333700567483902
Distance: 9.147930145263672
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2215162217617035
Distance: 8.714229583740234
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.5212815999984741
Distance: 8.835745811462402
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.3427843153476715
Distance: 8.21446418762207
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.8271032571792603
Distance: 7.771679878234863
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.074376679956913
Distance: 8.498783111572266
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([3, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.1976543366909027
Distance: 8.473159790039062
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.24705561995506287
Distance: 8.57081413269043
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.12620773911476135
Distance: 8.717869758605957
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2783292829990387
Distance: 8.49166202545166
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.353921502828598
Distance: 8.113332748413086
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.44888362288475037
Distance: 8.367254257202148
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -1.3695989847183228
Distance: 8.716137886047363
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.19663754105567932
Distance: 9.985736846923828
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

