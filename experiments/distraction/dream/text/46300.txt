Env ID: [55]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.11811599880456924
Distance: 9.16569709777832
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.16839656233787537
Distance: 9.183813095092773
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.07526455074548721
Distance: 9.252209663391113
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.04861316829919815
Distance: 9.227474212646484
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09872684627771378
Distance: 9.176087379455566
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.28641852736473083
Distance: 9.174814224243164
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.1747356355190277
Distance: 9.36123275756836
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: 0.14587727189064026
Distance: 9.435968399047852
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.007238768041133881
Distance: 9.190091133117676
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.22437533736228943
Distance: 9.082852363586426
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.026881791651248932
Distance: 9.20722770690918
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.891008734703064
Distance: 9.134109497070312
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 8, 1], dtype=torch.int32)
Action: right
Reward: -0.22991999983787537
Distance: 8.14310073852539
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 8, 0], dtype=torch.int32)
Action: up
Reward: -0.0241304412484169
Distance: 8.27302074432373
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 8, 0], dtype=torch.int32)
Action: left
Reward: 0.41972866654396057
Distance: 8.197151184082031
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 8, 1], dtype=torch.int32)
Action: up
Reward: 0.04990329593420029
Distance: 7.677422523498535
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 8, 1], dtype=torch.int32)
Action: up
Reward: -0.13840636610984802
Distance: 7.527519226074219
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 8, 1], dtype=torch.int32)
Action: up
Reward: -0.2650357186794281
Distance: 7.565925598144531
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 8, 1], dtype=torch.int32)
Action: right
Reward: -0.07864675670862198
Distance: 7.730961322784424
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 8, 0], dtype=torch.int32)
Action: left
Reward: 0.07243862003087997
Distance: 7.70960807800293
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

