Env ID: [264]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.1375328004360199
Distance: 9.631732940673828
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.0851503387093544
Distance: 9.669265747070312
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09047756344079971
Distance: 9.65441608428955
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7147945165634155
Distance: 9.644893646240234
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.04714832454919815
Distance: 8.830099105834961
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.12015590816736221
Distance: 8.777247428894043
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.13739928603172302
Distance: 8.797403335571289
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.8563827276229858
Distance: 8.834802627563477
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.17474183440208435
Distance: 7.878419876098633
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.03702344745397568
Distance: 7.953161716461182
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.07494793087244034
Distance: 7.81613826751709
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.8706244230270386
Distance: 7.791086196899414
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 1, 1], dtype=torch.int32)
Action: up
Reward: -0.06843862682580948
Distance: 5.820461750030518
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.3025232255458832
Distance: 5.788900375366211
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.14822855591773987
Distance: 5.991423606872559
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.08861150592565536
Distance: 5.743195056915283
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.8557671308517456
Distance: 5.554583549499512
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 0, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.1173638328909874
Distance: 4.598816394805908
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

