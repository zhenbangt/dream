Env ID: [198]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.12789449095726013
Distance: 7.607812881469727
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.06349935382604599
Distance: 7.635707378387451
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.5568016767501831
Distance: 7.472208023071289
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.19309577345848083
Distance: 6.815406322479248
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([1, 1, 0], dtype=torch.int32)
Action: up
Reward: -0.46226271986961365
Distance: 6.908502101898193
Next state: tensor([1, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([1, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.2676564157009125
Distance: 7.2707648277282715
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([1, 1, 0], dtype=torch.int32)
Action: drop
Reward: -0.2748490273952484
Distance: 7.438421249389648
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([1, 1, 0], dtype=torch.int32)
Action: up
Reward: -0.4823680818080902
Distance: 7.613270282745361
Next state: tensor([1, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([1, 2, 0], dtype=torch.int32)
Action: pickup
Reward: -0.3933773934841156
Distance: 7.995638370513916
Next state: tensor([1, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([1, 2, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.193566232919693
Distance: 8.289015769958496
Next state: tensor([1, 2, 0], dtype=torch.int32)
================================================================================

