Env ID: [407]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.10096301883459091
Distance: 8.97765064239502
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.14504298567771912
Distance: 8.776687622070312
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.056658171117305756
Distance: 8.821730613708496
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.17092856764793396
Distance: 8.665072441101074
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 5, 1], dtype=torch.int32)
Action: up
Reward: -0.01823291927576065
Distance: 8.736001014709473
Next state: tensor([4, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 6, 0], dtype=torch.int32)
Action: right
Reward: 0.0011009201407432556
Distance: 8.654233932495117
Next state: tensor([5, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 6, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.15478286147117615
Distance: 8.553133010864258
Next state: tensor([5, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 6, 0], dtype=torch.int32)
Action: up
Reward: -0.19080886244773865
Distance: 8.607915878295898
Next state: tensor([5, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 7, 0], dtype=torch.int32)
Action: left
Reward: -0.08321628719568253
Distance: 8.698724746704102
Next state: tensor([4, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 7, 0], dtype=torch.int32)
Action: right
Reward: 0.06193103641271591
Distance: 8.681941032409668
Next state: tensor([5, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 7, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.01599273830652237
Distance: 8.520009994506836
Next state: tensor([5, 7, 0], dtype=torch.int32)
================================================================================

