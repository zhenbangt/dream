Env ID: [550]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08935890346765518
Distance: 9.567729949951172
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.09703979641199112
Distance: 9.557088851928711
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.11684951931238174
Distance: 9.554128646850586
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.13171157240867615
Distance: 9.570978164672852
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.6802123785018921
Distance: 9.602689743041992
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 7, 1], dtype=torch.int32)
Action: down
Reward: 0.04719867557287216
Distance: 8.822477340698242
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.05933437496423721
Distance: 8.675278663635254
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 5, 0], dtype=torch.int32)
Action: down
Reward: -0.18173179030418396
Distance: 8.634613037109375
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.2683483064174652
Distance: 8.716344833374023
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.39172324538230896
Distance: 8.884693145751953
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 3, 0], dtype=torch.int32)
Action: drop
Reward: -0.4235254228115082
Distance: 9.176416397094727
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.5188623666763306
Distance: 9.4999418258667
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.24345168471336365
Distance: 9.918804168701172
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.06856689602136612
Distance: 10.062255859375
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.04631767421960831
Distance: 10.03082275390625
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.063878633081913
Distance: 9.977140426635742
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.07772407680749893
Distance: 9.941019058227539
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.07334861904382706
Distance: 9.918743133544922
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.0683055892586708
Distance: 9.892091751098633
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 3, 0], dtype=torch.int32)
Action: pickup
Reward: -0.04879341274499893
Distance: 9.860397338867188
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

