Env ID: [55]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.22104129195213318
Distance: 9.370570182800293
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.08876381069421768
Distance: 9.49161148071289
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.11968383938074112
Distance: 9.480375289916992
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([6, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.08860550075769424
Distance: 9.500059127807617
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([6, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.024459458887577057
Distance: 9.488664627075195
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.3065200746059418
Distance: 9.413124084472656
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: 0.031010054051876068
Distance: 9.619644165039062
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.003692246973514557
Distance: 9.48863410949707
Next state: tensor([6, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 5, 0], dtype=torch.int32)
Action: down
Reward: 0.009808920323848724
Distance: 9.392326354980469
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([6, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.15648707747459412
Distance: 9.282517433166504
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([6, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.12060127407312393
Distance: 9.339004516601562
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([6, 3, 0], dtype=torch.int32)
Action: noop
Reward: -0.10292015224695206
Distance: 9.35960578918457
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([6, 3, 0], dtype=torch.int32)
Action: noop
Reward: -0.12470493465662003
Distance: 9.362525939941406
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([6, 3, 0], dtype=torch.int32)
Action: right
Reward: -0.3274599015712738
Distance: 9.38723087310791
Next state: tensor([7, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([7, 3, 0], dtype=torch.int32)
Action: down
Reward: -0.2412925660610199
Distance: 9.614690780639648
Next state: tensor([7, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([7, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.16188010573387146
Distance: 9.755983352661133
Next state: tensor([7, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([7, 2, 0], dtype=torch.int32)
Action: noop
Reward: -0.0004383102059364319
Distance: 9.817863464355469
Next state: tensor([7, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([7, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.15847930312156677
Distance: 9.718301773071289
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.04507026821374893
Distance: 9.77678108215332
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 2, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.022286035120487213
Distance: 9.721851348876953
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

