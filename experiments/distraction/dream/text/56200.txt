Env ID: [374]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.13738307356834412
Distance: 9.524270057678223
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: 0.018829725682735443
Distance: 9.561653137207031
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.18490561842918396
Distance: 9.44282341003418
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09035549312829971
Distance: 9.527729034423828
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1064380630850792
Distance: 9.518084526062012
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.13566836714744568
Distance: 9.311646461486816
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.24080905318260193
Distance: 9.347314834594727
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.596396803855896
Distance: 9.488123893737793
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.9208812713623047
Distance: 8.791727066040039
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 7, 1], dtype=torch.int32)
Action: left
Reward: 0.18695154786109924
Distance: 5.770845890045166
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 7, 1], dtype=torch.int32)
Action: up
Reward: -0.8195239305496216
Distance: 5.483894348144531
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 8, 0], dtype=torch.int32)
Action: left
Reward: 0.08569183200597763
Distance: 6.203418254852295
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 8, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.2448645532131195
Distance: 6.017726421356201
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

