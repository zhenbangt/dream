Env ID: [187]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.0775056853890419
Distance: 8.983667373657227
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.07684097439050674
Distance: 8.961173057556152
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.0461252927780151
Distance: 8.938014030456543
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10230741649866104
Distance: 7.79188871383667
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.254126638174057
Distance: 7.794196128845215
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.05422792583703995
Distance: 7.948322772979736
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.0363574028015137
Distance: 7.90255069732666
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.14286193251609802
Distance: 4.766193389892578
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: 0.013123415410518646
Distance: 4.809055328369141
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.01919708400964737
Distance: 4.695931911468506
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.07131204754114151
Distance: 4.615128993988037
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.2657703161239624
Distance: 4.5864410400390625
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.08618555217981339
Distance: 3.220670700073242
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.1304207742214203
Distance: 3.2068562507629395
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.10427580028772354
Distance: 3.237277030944824
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.0988687053322792
Distance: 3.2415528297424316
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.03664384037256241
Distance: 3.2404215335845947
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.21244081854820251
Distance: 3.177065372467041
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9580575227737427
Distance: 3.289506196975708
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 0, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.038010694086551666
Distance: 2.2314486503601074
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

