Env ID: [484]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1290641725063324
Distance: 25.154804229736328
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09757385402917862
Distance: 25.183868408203125
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.09452972561120987
Distance: 25.181442260742188
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 2, 0], dtype=torch.int32)
Action: noop
Reward: -0.09736404567956924
Distance: 25.17597198486328
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 2, 0], dtype=torch.int32)
Action: drop
Reward: -0.07864151149988174
Distance: 25.173336029052734
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.078771211206913
Distance: 25.1519775390625
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.13556823134422302
Distance: 25.130748748779297
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.06492386013269424
Distance: 25.166316986083984
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.09906921535730362
Distance: 25.131240844726562
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1204601302742958
Distance: 25.13031005859375
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.11979828029870987
Distance: 25.15077018737793
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.11970100551843643
Distance: 25.170568466186523
Next state: tensor([7, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([7, 1, 0], dtype=torch.int32)
Action: drop
Reward: -0.07543335109949112
Distance: 25.190269470214844
Next state: tensor([7, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([7, 1, 0], dtype=torch.int32)
Action: down
Reward: -0.09617004543542862
Distance: 25.16570281982422
Next state: tensor([7, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([7, 0, 0], dtype=torch.int32)
Action: pickup
Reward: -0.10893402248620987
Distance: 25.16187286376953
Next state: tensor([7, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([7, 0, 0], dtype=torch.int32)
Action: left
Reward: -0.124791719019413
Distance: 25.170806884765625
Next state: tensor([6, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([6, 0, 0], dtype=torch.int32)
Action: noop
Reward: -0.0799938216805458
Distance: 25.195598602294922
Next state: tensor([6, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([6, 0, 0], dtype=torch.int32)
Action: right
Reward: -0.05235252529382706
Distance: 25.17559242248535
Next state: tensor([7, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([7, 0, 0], dtype=torch.int32)
Action: right
Reward: -0.08962974697351456
Distance: 25.127944946289062
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 0, 0], dtype=torch.int32)
Action: noop
Reward: -0.1169200912117958
Distance: 25.11757469177246
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

