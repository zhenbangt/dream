Env ID: [154]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.09989509731531143
Distance: 8.328670501708984
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.13006076216697693
Distance: 8.32856559753418
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1730094850063324
Distance: 8.358626365661621
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.14662989974021912
Distance: 8.431635856628418
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.021883390843868256
Distance: 8.478265762329102
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.28083476424217224
Distance: 8.356382369995117
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.10435161739587784
Distance: 8.537217140197754
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.05380592495203018
Distance: 8.541568756103516
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.03733406215906143
Distance: 8.49537467956543
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.05893287807703018
Distance: 8.432708740234375
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.007626153528690338
Distance: 8.391641616821289
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.06665477901697159
Distance: 8.299267768859863
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.08015785366296768
Distance: 8.265922546386719
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.004755593836307526
Distance: 8.24608039855957
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.1719423234462738
Distance: 8.150835990905762
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.04243316501379013
Distance: 8.2227783203125
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.19451770186424255
Distance: 8.080345153808594
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.12077417224645615
Distance: 8.1748628616333
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.1822119653224945
Distance: 7.9540886878967285
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.00181569904088974
Distance: 8.036300659179688
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

