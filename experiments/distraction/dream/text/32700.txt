Env ID: [187]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.2735181748867035
Distance: 8.266422271728516
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.06037197262048721
Distance: 8.439940452575684
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.1673332154750824
Distance: 8.400312423706055
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.07439842075109482
Distance: 8.467645645141602
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.1828370988368988
Distance: 8.29324722290039
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.22529563307762146
Distance: 8.376084327697754
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.21119555830955505
Distance: 8.50137996673584
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.14869269728660583
Distance: 8.61257553100586
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.18718299269676208
Distance: 8.66126823425293
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12797126173973083
Distance: 8.748451232910156
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.09549961239099503
Distance: 8.776422500610352
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.07942066341638565
Distance: 8.77192211151123
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.09573135524988174
Distance: 8.7513427734375
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.09157047420740128
Distance: 8.747074127197266
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.14068087935447693
Distance: 8.73864459991455
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.17286643385887146
Distance: 8.779325485229492
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.16578826308250427
Distance: 8.852191925048828
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.14750441908836365
Distance: 8.917980194091797
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12740573287010193
Distance: 8.965484619140625
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.38956204056739807
Distance: 8.992890357971191
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

