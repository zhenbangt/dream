Env ID: [0]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.1896253526210785
Distance: 9.373077392578125
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.29180774092674255
Distance: 9.462702751159668
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: 0.19372597336769104
Distance: 9.654510498046875
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.002303697168827057
Distance: 9.360784530639648
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.2082897126674652
Distance: 9.26308822631836
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.14132842421531677
Distance: 9.371377944946289
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.40503159165382385
Distance: 9.41270637512207
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.14094695448875427
Distance: 8.907674789428711
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.4243789613246918
Distance: 8.94862174987793
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1017833724617958
Distance: 9.273000717163086
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.8902934789657593
Distance: 9.274784088134766
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 7, 1], dtype=torch.int32)
Action: down
Reward: 0.2661871016025543
Distance: 7.284490585327148
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.1007457748055458
Distance: 6.918303489685059
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 5, 0], dtype=torch.int32)
Action: left
Reward: -0.3818221986293793
Distance: 6.919049263000488
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 5, 0], dtype=torch.int32)
Action: noop
Reward: -0.06637249141931534
Distance: 7.200871467590332
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1823192536830902
Distance: 7.167243957519531
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.06811580806970596
Distance: 7.249563217163086
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.07473383098840714
Distance: 7.217679023742676
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.12241993099451065
Distance: 7.192412853240967
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1406918466091156
Distance: 7.214832782745361
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

