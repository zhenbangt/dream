Env ID: [561]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.2160707414150238
Distance: 9.076029777526855
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08167991787195206
Distance: 9.192100524902344
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.07892856746912003
Distance: 9.17378044128418
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.09509696811437607
Distance: 9.152709007263184
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08222637325525284
Distance: 8.957612037658691
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.14666804671287537
Distance: 8.939838409423828
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.10166225582361221
Distance: 8.986506462097168
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.14022693037986755
Distance: 8.988168716430664
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.06118831783533096
Distance: 9.028395652770996
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.1426587998867035
Distance: 8.989583969116211
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.12045536190271378
Distance: 9.032242774963379
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.11160144954919815
Distance: 9.052698135375977
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.11076030880212784
Distance: 9.064299583435059
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.16671809554100037
Distance: 9.07505989074707
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.0688243880867958
Distance: 9.141777992248535
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.2798047959804535
Distance: 9.110602378845215
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([6, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.049822233617305756
Distance: 9.290407180786133
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.13992270827293396
Distance: 9.140584945678711
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([6, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.109960176050663
Distance: 9.18050765991211
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.1243511214852333
Distance: 9.190467834472656
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

