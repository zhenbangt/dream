Env ID: [473]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.18443164229393005
Distance: 8.115900993347168
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.06373844295740128
Distance: 8.200332641601562
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.33090439438819885
Distance: 8.164071083068848
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.07269344478845596
Distance: 7.733166694641113
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.06392250210046768
Distance: 7.705860137939453
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.1385084092617035
Distance: 7.669782638549805
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.6086200475692749
Distance: 7.708291053771973
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2675953805446625
Distance: 6.99967098236084
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.47834500670433044
Distance: 7.167266368865967
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: 0.06631507724523544
Distance: 7.545611381530762
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.8267277479171753
Distance: 7.37929630279541
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.33050671219825745
Distance: 5.452568531036377
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.152533620595932
Distance: 5.022061824798584
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.066052436828613
Distance: 5.0745954513549805
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 8, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.015629790723323822
Distance: 0.9085431098937988
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

