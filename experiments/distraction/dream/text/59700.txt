Env ID: [506]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.14019545912742615
Distance: 9.03531265258789
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.11643943935632706
Distance: 9.075508117675781
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.0858379378914833
Distance: 9.091947555541992
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.11742649227380753
Distance: 9.07778549194336
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.03421363979578018
Distance: 9.09521198272705
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.22794494032859802
Distance: 9.029425621032715
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([1, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.0578550323843956
Distance: 9.157370567321777
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2659469544887543
Distance: 8.999515533447266
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.0419095978140831
Distance: 9.165462493896484
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.10979900509119034
Distance: 9.023552894592285
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.048009298741817474
Distance: 9.03335189819336
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.024889566004276276
Distance: 8.885342597961426
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.9127098321914673
Distance: 8.810232162475586
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.29066523909568787
Distance: 9.622941970825195
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.1339973509311676
Distance: 9.813607215881348
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: 0.11972179263830185
Distance: 9.579609870910645
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.6816297769546509
Distance: 9.359888076782227
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.4223400056362152
Distance: 9.94151782989502
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 5, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.13336428999900818
Distance: 10.2638578414917
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

