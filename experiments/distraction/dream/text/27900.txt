Env ID: [110]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.06643448024988174
Distance: 9.014055252075195
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.10055866092443466
Distance: 8.980489730834961
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 7, 1], dtype=torch.int32)
Action: right
Reward: 0.029340170323848724
Distance: 8.77993106842041
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([1, 7, 0], dtype=torch.int32)
Action: pickup
Reward: 0.11023654788732529
Distance: 8.650590896606445
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([1, 7, 0], dtype=torch.int32)
Action: noop
Reward: 0.0597166046500206
Distance: 8.440354347229004
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([1, 7, 0], dtype=torch.int32)
Action: right
Reward: -0.4117952287197113
Distance: 8.280637741088867
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([2, 7, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.5971752405166626
Distance: 8.592432975769043
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([2, 7, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.7921844720840454
Distance: 9.089608192443848
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

