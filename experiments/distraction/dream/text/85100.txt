Env ID: [286]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.11615810543298721
Distance: 8.837156295776367
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10174999386072159
Distance: 8.853314399719238
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.6541093587875366
Distance: 8.855064392089844
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.09801159054040909
Distance: 8.10095500946045
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.18458518385887146
Distance: 8.098966598510742
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.11710987240076065
Distance: 8.183551788330078
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.054590798914432526
Distance: 8.200661659240723
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7500251531600952
Distance: 8.155252456665039
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.019420720636844635
Distance: 7.305227279663086
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.22284135222434998
Distance: 7.2246479988098145
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.06870374828577042
Distance: 7.347489356994629
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.2251852750778198
Distance: 7.316193103790283
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.17518702149391174
Distance: 5.9910078048706055
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.6269732713699341
Distance: 5.715820789337158
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.13219889998435974
Distance: 6.242794036865234
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 6.070109844207764
Distance: 6.274992942810059
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.05671565234661102
Distance: 0.10488327592611313
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.17896288633346558
Distance: 0.06159892678260803
Next state: tensor([6, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([6, 5, 0], dtype=torch.int32)
Action: left
Reward: -0.05839674919843674
Distance: 0.14056181907653809
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 5, 1], dtype=torch.int32)
Action: drop
Reward: -0.1395808458328247
Distance: 0.0989585667848587
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

