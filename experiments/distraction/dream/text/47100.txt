Env ID: [528]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.12849482893943787
Distance: 8.860047340393066
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1507316529750824
Distance: 8.888542175292969
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.03897819668054581
Distance: 8.939273834228516
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.2755905091762543
Distance: 8.878252029418945
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.016170881688594818
Distance: 9.053842544555664
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.10134277492761612
Distance: 8.937671661376953
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.10266456753015518
Distance: 8.939014434814453
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.10647354274988174
Distance: 8.941679000854492
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.12430725246667862
Distance: 8.948152542114258
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7068189382553101
Distance: 8.97245979309082
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.08417186886072159
Distance: 8.165640830993652
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 7, 1], dtype=torch.int32)
Action: down
Reward: -0.20767554640769958
Distance: 8.149812698364258
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.3296562135219574
Distance: 8.257488250732422
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 5, 0], dtype=torch.int32)
Action: pickup
Reward: -0.3466106355190277
Distance: 8.487144470214844
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 5, 0], dtype=torch.int32)
Action: up
Reward: 0.1601804792881012
Distance: 8.733755111694336
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 6, 0], dtype=torch.int32)
Action: left
Reward: -0.3940511643886566
Distance: 8.4735746383667
Next state: tensor([7, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([7, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.32591208815574646
Distance: 8.76762580871582
Next state: tensor([7, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([7, 6, 0], dtype=torch.int32)
Action: pickup
Reward: 0.49960365891456604
Distance: 8.993537902832031
Next state: tensor([7, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([7, 6, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.05766334384679794
Distance: 8.39393424987793
Next state: tensor([7, 6, 0], dtype=torch.int32)
================================================================================

