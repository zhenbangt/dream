Env ID: [176]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.07463512569665909
Distance: 8.847997665405273
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1158367171883583
Distance: 8.822632789611816
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.0189942121505737
Distance: 8.838469505310059
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.124944306910038
Distance: 7.719475269317627
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.179114431142807
Distance: 7.744419574737549
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.07678165286779404
Distance: 7.82353401184082
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.20048817992210388
Distance: 7.64675235748291
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.048114873468875885
Distance: 7.7472405433654785
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.8036931753158569
Distance: 7.695355415344238
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.04902944713830948
Distance: 6.791662216186523
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.14622697234153748
Distance: 6.740691661834717
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.012078382074832916
Distance: 6.786918640136719
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.2733707427978516
Distance: 6.6989970207214355
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.15466174483299255
Distance: 4.325626373291016
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2873658239841461
Distance: 4.380288124084473
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.3757762014865875
Distance: 3.992922306060791
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.0504984855651855
Distance: 3.517146110534668
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 0, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.010373972356319427
Distance: 0.36664772033691406
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

