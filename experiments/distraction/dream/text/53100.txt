Env ID: [275]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.12277660518884659
Distance: 7.960162162780762
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.18675193190574646
Distance: 7.982938766479492
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.055613137781620026
Distance: 8.069690704345703
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.32002678513526917
Distance: 8.025303840637207
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.06950006633996964
Distance: 7.605277061462402
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.5795556306838989
Distance: 7.574777126312256
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.07063446193933487
Distance: 8.054332733154297
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: 0.22970905900001526
Distance: 8.024967193603516
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.6360820531845093
Distance: 7.695258140563965
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.03150615841150284
Distance: 6.959176063537598
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9795860052108765
Distance: 6.890682220458984
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.02427969127893448
Distance: 5.81109619140625
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.3232460916042328
Distance: 5.735375881195068
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.4089113175868988
Distance: 5.958621978759766
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.43328723311424255
Distance: 6.267533302307129
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.23363152146339417
Distance: 6.600820541381836
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.057962797582149506
Distance: 6.267189025878906
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -1.4651046991348267
Distance: 6.109226226806641
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 7, 1], dtype=torch.int32)
Action: down
Reward: 0.12865963578224182
Distance: 7.474330902099609
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 6, 0], dtype=torch.int32)
Action: up
Reward: -1.244205117225647
Distance: 7.245671272277832
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

