Env ID: [77]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.046412087976932526
Distance: 9.181685447692871
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10625133663415909
Distance: 9.128097534179688
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.5473283529281616
Distance: 9.13434886932373
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.08234653621912003
Distance: 8.487020492553711
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.06102810055017471
Distance: 8.469367027282715
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.12260589748620987
Distance: 8.430395126342773
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.8520311117172241
Distance: 8.453001022338867
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.05398216098546982
Distance: 6.500969886779785
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.06689129024744034
Distance: 6.346987724304199
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.33733996748924255
Distance: 6.313879013061523
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.4731124639511108
Distance: 6.5512189865112305
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.4237903654575348
Distance: 4.978106498718262
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.36785611510276794
Distance: 4.454316139221191
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.38649845123291
Distance: 4.722172260284424
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.014602981507778168
Distance: 0.23567381501197815
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 3, 0], dtype=torch.int32)
Action: down
Reward: -0.15134260058403015
Distance: 0.12107083201408386
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.25767529010772705
Distance: 0.1724134385585785
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.12671580910682678
Distance: 0.33008873462677
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.14598190784454346
Distance: 0.3568045496940613
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.09636295586824417
Distance: 0.4027864634990692
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

