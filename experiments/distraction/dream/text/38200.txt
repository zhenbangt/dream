Env ID: [363]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.01827392727136612
Distance: 9.14794921875
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.10534248501062393
Distance: 9.06622314453125
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.23146876692771912
Distance: 9.071565628051758
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.3127456605434418
Distance: 9.203034400939941
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.3552003800868988
Distance: 9.415780067443848
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.20042571425437927
Distance: 9.670980453491211
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.2563415467739105
Distance: 9.771406173706055
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.1434837281703949
Distance: 9.92774772644043
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.19074591994285583
Distance: 9.971231460571289
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.028119660913944244
Distance: 10.06197738647461
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.05339203029870987
Distance: 9.990097045898438
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.12558135390281677
Distance: 9.943489074707031
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.12714728713035583
Distance: 9.969070434570312
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.13359031081199646
Distance: 9.996217727661133
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.1479402482509613
Distance: 10.029808044433594
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.13043269515037537
Distance: 10.07774829864502
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.10506973415613174
Distance: 10.10818099975586
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9176667928695679
Distance: 10.113250732421875
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.0008863434195518494
Distance: 9.09558391571045
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

