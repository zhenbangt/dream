Env ID: [187]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.05268993228673935
Distance: 8.854122161865234
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.3227468430995941
Distance: 8.701432228088379
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([2, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.003257371485233307
Distance: 8.924179077148438
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([2, 3, 0], dtype=torch.int32)
Action: down
Reward: -0.2661987245082855
Distance: 8.827436447143555
Next state: tensor([2, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([2, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.5645195245742798
Distance: 8.993635177612305
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([2, 1, 0], dtype=torch.int32)
Action: drop
Reward: -0.4274021089076996
Distance: 9.458154678344727
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([2, 1, 0], dtype=torch.int32)
Action: up
Reward: -0.8358265161514282
Distance: 9.78555679321289
Next state: tensor([2, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([2, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.4036441743373871
Distance: 10.521383285522461
Next state: tensor([2, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([2, 1, 0], dtype=torch.int32)
Action: down
Reward: -0.2850376069545746
Distance: 10.825027465820312
Next state: tensor([2, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([2, 0, 0], dtype=torch.int32)
Action: right
Reward: -0.03279075771570206
Distance: 11.010065078735352
Next state: tensor([3, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 0, 0], dtype=torch.int32)
Action: right
Reward: -0.08426818996667862
Distance: 10.942855834960938
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 0, 1], dtype=torch.int32)
Action: drop
Reward: -0.7197986841201782
Distance: 10.9271240234375
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 0, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.14575424790382385
Distance: 11.54692268371582
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

