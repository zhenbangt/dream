Env ID: [88]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08106289058923721
Distance: 9.352922439575195
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.2891164720058441
Distance: 9.333985328674316
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: drop
Reward: 0.03344383090734482
Distance: 9.523101806640625
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 4, 1], dtype=torch.int32)
Action: drop
Reward: 0.02635803073644638
Distance: 9.389657974243164
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.04713306576013565
Distance: 9.263299942016602
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.005910493433475494
Distance: 9.210433006286621
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.1017519012093544
Distance: 9.11634349822998
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.35091742873191833
Distance: 9.118095397949219
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: 0.011350058019161224
Distance: 9.369012832641602
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.12977084517478943
Distance: 9.257662773132324
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.06726989895105362
Distance: 9.287433624267578
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.10664711147546768
Distance: 9.254703521728516
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.10664520412683487
Distance: 9.261350631713867
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.13645705580711365
Distance: 9.267995834350586
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.1261039674282074
Distance: 9.304452896118164
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.11110744625329971
Distance: 9.330556869506836
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.1109224334359169
Distance: 9.34166431427002
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.11537323147058487
Distance: 9.35258674621582
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.11244640499353409
Distance: 9.367959976196289
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.10832462459802628
Distance: 9.380406379699707
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

