Env ID: [495]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.0538545623421669
Distance: 4.590512275695801
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.4072185456752777
Distance: 4.544366836547852
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 7, 1], dtype=torch.int32)
Action: down
Reward: -0.14028319716453552
Distance: 4.851585388183594
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 6, 0], dtype=torch.int32)
Action: noop
Reward: -0.44633445143699646
Distance: 4.891868591308594
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 6, 0], dtype=torch.int32)
Action: up
Reward: -0.4078141152858734
Distance: 5.238203048706055
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([8, 7, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.23315724730491638
Distance: 5.546017169952393
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

