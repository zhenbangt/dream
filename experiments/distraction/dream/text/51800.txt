Env ID: [462]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.08850441128015518
Distance: 8.999395370483398
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.08075866848230362
Distance: 8.987899780273438
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.14271220564842224
Distance: 8.968658447265625
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.10991726070642471
Distance: 9.011370658874512
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.10691700130701065
Distance: 9.02128791809082
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.09699402004480362
Distance: 9.028204917907715
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1375318467617035
Distance: 9.025198936462402
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.338418573141098
Distance: 9.06273078918457
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.39900073409080505
Distance: 9.301149368286133
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.210606187582016
Distance: 9.600150108337402
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.2979064881801605
Distance: 9.710756301879883
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.20629176497459412
Distance: 9.908662796020508
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.21197471022605896
Distance: 10.014954566955566
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.1844860017299652
Distance: 10.12692928314209
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.307316392660141
Distance: 10.21141529083252
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.28092631697654724
Distance: 10.418731689453125
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.2630487382411957
Distance: 10.599658012390137
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.2808137834072113
Distance: 10.762706756591797
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.578652024269104
Distance: 10.943520545959473
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.09704456478357315
Distance: 11.422172546386719
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

