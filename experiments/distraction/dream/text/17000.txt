Env ID: [341]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.07583007961511612
Distance: 8.802444458007812
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1779588758945465
Distance: 8.778274536132812
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.29697665572166443
Distance: 8.50031566619873
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.4210983216762543
Distance: 8.69729232788086
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.12056408077478409
Distance: 9.018390655517578
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.2852188050746918
Distance: 9.038954734802246
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([1, 1, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.00023021548986434937
Distance: 9.224173545837402
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

