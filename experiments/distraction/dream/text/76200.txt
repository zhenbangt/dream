Env ID: [165]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.11154136806726456
Distance: 9.165803909301758
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10874805599451065
Distance: 9.177345275878906
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.1000779867172241
Distance: 9.1860933303833
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1179109588265419
Distance: 7.986015319824219
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.17507800459861755
Distance: 8.003926277160645
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.02344360202550888
Distance: 8.079004287719727
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.551228404045105
Distance: 7.955560684204102
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.21145257353782654
Distance: 6.304332256317139
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.15410098433494568
Distance: 6.41578483581543
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.22420749068260193
Distance: 6.46988582611084
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.19596251845359802
Distance: 6.594093322753906
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.5301188230514526
Distance: 6.690055847167969
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1180056557059288
Distance: 6.059937000274658
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.12754496932029724
Distance: 5.841931343078613
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.8877482414245605
Distance: 5.869476318359375
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.17424777150154114
Distance: 0.8817281126976013
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: left
Reward: -0.3446442782878876
Distance: 0.6074803471565247
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.3280121386051178
Distance: 0.8521246314048767
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.20038804411888123
Distance: 1.080136775970459
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.16196200251579285
Distance: 1.1805248260498047
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

