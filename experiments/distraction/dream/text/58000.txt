Env ID: [341]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.05775413662195206
Distance: 9.476634979248047
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.14959469437599182
Distance: 9.434389114379883
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.34878596663475037
Distance: 9.184794425964355
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.2656265199184418
Distance: 9.43358039855957
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.13767585158348083
Distance: 9.599206924438477
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.0284351110458374
Distance: 9.636882781982422
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 7, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.14560279250144958
Distance: 8.508447647094727
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

