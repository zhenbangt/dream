Env ID: [330]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.036961935460567474
Distance: 8.967458724975586
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.021599195897579193
Distance: 8.830496788024902
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.052095986902713776
Distance: 8.708897590637207
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([2, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.16991862654685974
Distance: 8.660993576049805
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([2, 3, 0], dtype=torch.int32)
Action: left
Reward: -0.7445989847183228
Distance: 8.730912208557129
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([1, 3, 0], dtype=torch.int32)
Action: up
Reward: 0.048189543187618256
Distance: 9.375511169433594
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([1, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.33378657698631287
Distance: 9.22732162475586
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([1, 5, 0], dtype=torch.int32)
Action: right
Reward: -0.2200838029384613
Distance: 9.461108207702637
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([2, 5, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.008297346532344818
Distance: 9.581192016601562
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

