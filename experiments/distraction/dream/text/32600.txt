Env ID: [110]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.32572707533836365
Distance: 9.484399795532227
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: 0.10331764072179794
Distance: 9.710126876831055
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.029441453516483307
Distance: 9.50680923461914
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.041636087000370026
Distance: 9.436250686645508
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.015846632421016693
Distance: 9.377886772155762
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.16364440321922302
Distance: 9.262040138244629
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.23777064681053162
Distance: 9.325684547424316
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.1257825791835785
Distance: 9.463455200195312
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.1349388062953949
Distance: 9.489237785339355
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.13042792677879333
Distance: 9.524176597595215
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12122020870447159
Distance: 9.554604530334473
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.11627159267663956
Distance: 9.575824737548828
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.10294590145349503
Distance: 9.592096328735352
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.2481294572353363
Distance: 9.59504222869873
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.29843196272850037
Distance: 9.743171691894531
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 5, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.06278476864099503
Distance: 9.941603660583496
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

