Env ID: [550]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.059459783136844635
Distance: 7.532541275024414
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.172766774892807
Distance: 7.492001056671143
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.21590003371238708
Distance: 7.564767837524414
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.38017329573631287
Distance: 7.680667877197266
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.21568450331687927
Distance: 7.960841178894043
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.7500215768814087
Distance: 8.076525688171387
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.22652855515480042
Distance: 8.726547241210938
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.16599807143211365
Distance: 8.400018692016602
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 4, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.2614406645298004
Distance: 8.46601676940918
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

