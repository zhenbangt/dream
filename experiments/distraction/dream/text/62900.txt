Env ID: [44]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.027605630457401276
Distance: 9.625190734863281
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.5608123540878296
Distance: 9.552796363830566
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.12527236342430115
Distance: 8.891983985900879
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.3684450089931488
Distance: 8.917256355285645
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.15596827864646912
Distance: 9.185701370239258
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.529885768890381
Distance: 9.241669654846191
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.12450065463781357
Distance: 5.611783981323242
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.6628128290176392
Distance: 5.3872833251953125
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.6649624109268188
Distance: 5.950096130371094
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.46839579939842224
Distance: 6.515058517456055
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 7, 1], dtype=torch.int32)
Action: up
Reward: -0.3993411958217621
Distance: 6.883454322814941
Next state: tensor([8, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 8, 0], dtype=torch.int32)
Action: down
Reward: -0.24139127135276794
Distance: 7.182795524597168
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.17543324828147888
Distance: 7.3241868019104
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.06159792095422745
Distance: 7.399620056152344
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: 1.0113686323165894
Distance: 7.361217975616455
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: 0.0008110031485557556
Distance: 6.249849319458008
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.08284053951501846
Distance: 6.149038314819336
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.33194342255592346
Distance: 6.131878852844238
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.4492439329624176
Distance: 5.699935436248779
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 3, 0], dtype=torch.int32)
Action: noop
Reward: 0.4490174353122711
Distance: 5.150691509246826
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

