Env ID: [275]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.06128749996423721
Distance: 9.099627494812012
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.06347427517175674
Distance: 9.060914993286133
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.14429053664207458
Distance: 9.024389266967773
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.11607322841882706
Distance: 9.068679809570312
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.16086235642433167
Distance: 9.084753036499023
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.3875642716884613
Distance: 8.823890686035156
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.16070422530174255
Distance: 9.111454963684082
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.061452291905879974
Distance: 9.172159194946289
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.6503676176071167
Distance: 9.010706901550293
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.032776929438114166
Distance: 7.260339260101318
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.9397255182266235
Distance: 7.193116188049316
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.2865472733974457
Distance: 8.032841682434082
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.35112056136131287
Distance: 8.219388961791992
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.23656710982322693
Distance: 8.47050952911377
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.5512841939926147
Distance: 8.607076644897461
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 7, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.9575401544570923
Distance: 7.955792427062988
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

