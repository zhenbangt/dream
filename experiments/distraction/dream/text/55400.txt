Env ID: [55]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08380088955163956
Distance: 9.9461669921875
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.11045894771814346
Distance: 9.929967880249023
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.11621914058923721
Distance: 9.94042682647705
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08558902889490128
Distance: 9.956645965576172
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.03167019039392471
Distance: 9.942234992980957
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.16306552290916443
Distance: 9.873905181884766
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.43315067887306213
Distance: 9.936970710754395
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3694330155849457
Distance: 9.403820037841797
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.04916324466466904
Distance: 9.673253059387207
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.16037216782569885
Distance: 9.524089813232422
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.5003905296325684
Distance: 9.263717651367188
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.2719798982143402
Distance: 6.663327217102051
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.18059787154197693
Distance: 6.8353071212768555
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([1, 1, 0], dtype=torch.int32)
Action: left
Reward: -0.18441924452781677
Distance: 6.915904998779297
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.5752612352371216
Distance: 7.000324249267578
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.13424673676490784
Distance: 7.475585460662842
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

