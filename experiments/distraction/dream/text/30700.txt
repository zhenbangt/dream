Env ID: [473]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.1162935271859169
Distance: 8.492828369140625
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.15406093001365662
Distance: 8.509121894836426
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.3925643861293793
Distance: 8.563182830810547
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 5, 1], dtype=torch.int32)
Action: up
Reward: -0.007698632776737213
Distance: 8.85574722290039
Next state: tensor([4, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 6, 0], dtype=torch.int32)
Action: up
Reward: 0.0012086853384971619
Distance: 8.763445854187012
Next state: tensor([4, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 7, 0], dtype=torch.int32)
Action: left
Reward: -0.06976661831140518
Distance: 8.662237167358398
Next state: tensor([3, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 7, 0], dtype=torch.int32)
Action: right
Reward: -0.1773572862148285
Distance: 8.632003784179688
Next state: tensor([4, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 7, 0], dtype=torch.int32)
Action: pickup
Reward: -0.38070353865623474
Distance: 8.70936107635498
Next state: tensor([4, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 7, 0], dtype=torch.int32)
Action: left
Reward: -0.2131877839565277
Distance: 8.99006462097168
Next state: tensor([3, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 7, 0], dtype=torch.int32)
Action: pickup
Reward: -0.26395949721336365
Distance: 9.103252410888672
Next state: tensor([3, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 7, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.05565681308507919
Distance: 9.2672119140625
Next state: tensor([3, 7, 0], dtype=torch.int32)
================================================================================

