Env ID: [506]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.14189013838768005
Distance: 9.775726318359375
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.12047252804040909
Distance: 9.81761646270752
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.2886835038661957
Distance: 9.838088989257812
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.020143888890743256
Distance: 10.026772499084473
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.08680019527673721
Distance: 9.906628608703613
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.002726934850215912
Distance: 9.893428802490234
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.21241244673728943
Distance: 9.790701866149902
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.06779632717370987
Distance: 9.903114318847656
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.06281623989343643
Distance: 9.87091064453125
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.10153637081384659
Distance: 9.83372688293457
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.17694053053855896
Distance: 9.8352632522583
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.1258348524570465
Distance: 9.912203788757324
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7954839468002319
Distance: 9.686368942260742
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.285457044839859
Distance: 8.790884971618652
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.052159883081912994
Distance: 8.405427932739258
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.16674575209617615
Distance: 8.357587814331055
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.21101608872413635
Distance: 8.424333572387695
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.1782173216342926
Distance: 8.113317489624023
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

