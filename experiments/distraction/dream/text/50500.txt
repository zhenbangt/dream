Env ID: [561]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.09254322201013565
Distance: 9.736432075500488
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.11863765865564346
Distance: 9.728975296020508
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.02387390285730362
Distance: 9.747612953186035
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.04209480434656143
Distance: 9.671486854553223
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.15811023116111755
Distance: 9.613581657409668
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.11187515407800674
Distance: 9.67169189453125
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.25172939896583557
Distance: 9.68356704711914
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.4559102952480316
Distance: 9.33183765411377
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.325479120016098
Distance: 9.687747955322266
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9665683507919312
Distance: 9.913227081298828
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.00025615841150283813
Distance: 8.846658706665039
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.12839087843894958
Distance: 8.746914863586426
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.27813395857810974
Distance: 8.77530574798584
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.8143457174301147
Distance: 8.953439712524414
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.05325803905725479
Distance: 8.039093971252441
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.15786370635032654
Distance: 7.99235200881958
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.2677942216396332
Distance: 8.050215721130371
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.06679954379796982
Distance: 8.218009948730469
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.32099494338035583
Distance: 8.051210403442383
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.19957390427589417
Distance: 8.272205352783203
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

