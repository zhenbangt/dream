Env ID: [396]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.10672435909509659
Distance: 9.373836517333984
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.10706577450037003
Distance: 9.380560874938965
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.20540198683738708
Distance: 9.387626647949219
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.02844085544347763
Distance: 9.49302864074707
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.28127631545066833
Distance: 9.364587783813477
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.1372905671596527
Distance: 9.54586410522461
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: 0.05396118015050888
Distance: 9.583154678344727
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: 0.07698478549718857
Distance: 9.429193496704102
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.17400416731834412
Distance: 9.252208709716797
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.16903838515281677
Distance: 9.326212882995605
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.10780105739831924
Distance: 9.395251274108887
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.8046582937240601
Distance: 9.40305233001709
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.35000285506248474
Distance: 8.498394012451172
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.19375285506248474
Distance: 8.748396873474121
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.32435092329978943
Distance: 8.84214973449707
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.22170791029930115
Distance: 9.066500663757324
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2072681486606598
Distance: 9.18820858001709
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.5276705026626587
Distance: 8.880940437316895
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.17626628279685974
Distance: 9.308610916137695
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([6, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.15794429183006287
Distance: 9.38487720489502
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

