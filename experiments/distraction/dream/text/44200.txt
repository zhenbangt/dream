Env ID: [506]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.16721972823143005
Distance: 9.512898445129395
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.0249238982796669
Distance: 9.580118179321289
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.06524048000574112
Distance: 9.50504207611084
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.06416568905115128
Distance: 9.470282554626465
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.42889460921287537
Distance: 9.4344482421875
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: 0.04515209048986435
Distance: 9.76334285736084
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: up
Reward: -0.20616111159324646
Distance: 9.61819076538086
Next state: tensor([5, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 6, 0], dtype=torch.int32)
Action: up
Reward: 0.35447826981544495
Distance: 9.72435188293457
Next state: tensor([5, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 7, 0], dtype=torch.int32)
Action: noop
Reward: 0.1498827040195465
Distance: 9.26987361907959
Next state: tensor([5, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 7, 0], dtype=torch.int32)
Action: up
Reward: 0.2683185636997223
Distance: 9.019990921020508
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 8, 0], dtype=torch.int32)
Action: noop
Reward: 0.2213367521762848
Distance: 8.65167236328125
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 8, 0], dtype=torch.int32)
Action: noop
Reward: -0.04090461879968643
Distance: 8.33033561706543
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 8, 0], dtype=torch.int32)
Action: noop
Reward: -0.17109450697898865
Distance: 8.271240234375
Next state: tensor([5, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 8, 0], dtype=torch.int32)
Action: down
Reward: 0.08686771243810654
Distance: 8.342334747314453
Next state: tensor([5, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 7, 0], dtype=torch.int32)
Action: down
Reward: -0.18237552046775818
Distance: 8.15546703338623
Next state: tensor([5, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.23566016554832458
Distance: 8.237842559814453
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.46555671095848083
Distance: 8.373502731323242
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.0810626968741417
Distance: 8.739059448242188
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.4438978135585785
Distance: 8.55799674987793
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

