Env ID: [209]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.10528526455163956
Distance: 9.125967025756836
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.08617935329675674
Distance: 9.13125228881836
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.13400688767433167
Distance: 9.117431640625
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.0920940414071083
Distance: 8.883424758911133
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.1250053346157074
Distance: 8.875518798828125
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.14727935194969177
Distance: 8.900524139404297
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.5096153020858765
Distance: 8.947803497314453
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10027370601892471
Distance: 7.338188171386719
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.07541093975305557
Distance: 7.338461875915527
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: 0.28447332978248596
Distance: 7.313872814178467
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.6689391136169434
Distance: 6.929399490356445
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.19862613081932068
Distance: 4.160460472106934
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: 0.4405161440372467
Distance: 4.259086608886719
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: 0.38843169808387756
Distance: 3.7185704708099365
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: 0.1870228350162506
Distance: 3.2301387786865234
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.12198176234960556
Distance: 2.9431159496307373
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.5496535301208496
Distance: 2.7211341857910156
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 0, 1], dtype=torch.int32)
Action: left
Reward: -0.14460858702659607
Distance: 0.07148072868585587
Next state: tensor([3, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 0, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.0961955189704895
Distance: 0.11608930677175522
Next state: tensor([3, 0, 0], dtype=torch.int32)
================================================================================

