Env ID: [253]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.15603408217430115
Distance: 4.719204902648926
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.08909759670495987
Distance: 4.775238990783691
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.13624820113182068
Distance: 4.764336585998535
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.026811696588993073
Distance: 4.80058479309082
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.4610957205295563
Distance: 4.727396488189697
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.05885467678308487
Distance: 4.1663007736206055
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3351074159145355
Distance: 4.125155448913574
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.06166829913854599
Distance: 4.360262870788574
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.19181689620018005
Distance: 4.198594570159912
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10435114055871964
Distance: 4.290411472320557
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10265979915857315
Distance: 4.29476261138916
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: 0.03271331638097763
Distance: 4.297422409057617
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([1, 1, 0], dtype=torch.int32)
Action: up
Reward: -0.33871325850486755
Distance: 4.164709091186523
Next state: tensor([1, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([1, 2, 0], dtype=torch.int32)
Action: pickup
Reward: -0.2594982087612152
Distance: 4.4034223556518555
Next state: tensor([1, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([1, 2, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.20378932356834412
Distance: 4.562920570373535
Next state: tensor([1, 2, 0], dtype=torch.int32)
================================================================================

