Env ID: [484]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.14976462721824646
Distance: 9.228717803955078
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.09462127834558487
Distance: 9.278482437133789
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.11320076137781143
Distance: 9.273103713989258
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.38032475113868713
Distance: 9.286304473876953
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2652517259120941
Distance: 8.80597972869873
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.1597648561000824
Distance: 8.971231460571289
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.12771376967430115
Distance: 9.030996322631836
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.9884666204452515
Distance: 9.058710098266602
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.07174787670373917
Distance: 6.970243453979492
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.16747388243675232
Distance: 6.941991329193115
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.43708619475364685
Distance: 7.009465217590332
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.698491096496582
Distance: 7.346551418304443
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.029442407190799713
Distance: 4.548060417175293
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.047359563410282135
Distance: 4.477502822875977
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.3035591840744019
Distance: 4.424862384796143
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 8, 1], dtype=torch.int32)
Action: noop
Reward: -0.041570283472537994
Distance: 3.021303176879883
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 8, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.15104541182518005
Distance: 2.9628734588623047
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

