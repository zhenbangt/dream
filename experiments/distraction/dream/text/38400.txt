Env ID: [143]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.13125953078269958
Distance: 9.43741226196289
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1183687224984169
Distance: 9.468671798706055
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.08174953609704971
Distance: 9.487040519714355
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.31098899245262146
Distance: 9.468790054321289
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.15309295058250427
Distance: 9.679779052734375
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.4146791398525238
Distance: 9.732872009277344
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.16232451796531677
Distance: 10.047551155090332
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.2711053788661957
Distance: 10.109875679016113
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: 0.13241997361183167
Distance: 10.280981063842773
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: 0.1847309172153473
Distance: 10.048561096191406
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.009902574121952057
Distance: 9.763830184936523
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.1145854964852333
Distance: 9.67373275756836
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.1434074342250824
Distance: 9.688318252563477
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.15521487593650818
Distance: 9.731725692749023
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.18789729475975037
Distance: 9.786940574645996
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.16344603896141052
Distance: 9.874837875366211
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.13294467329978943
Distance: 9.938283920288086
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.08755645900964737
Distance: 9.97122859954834
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.07328663021326065
Distance: 9.958785057067871
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.33315029740333557
Distance: 9.932071685791016
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

