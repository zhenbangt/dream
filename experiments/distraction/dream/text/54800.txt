Env ID: [528]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.1116701140999794
Distance: 9.882229804992676
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.0970369353890419
Distance: 9.893899917602539
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.16616496443748474
Distance: 9.890936851501465
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.14867934584617615
Distance: 9.957101821899414
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7246564626693726
Distance: 10.005781173706055
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.10734310001134872
Distance: 9.181124687194824
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.020011328160762787
Distance: 8.97378158569336
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.10185680538415909
Distance: 8.85377025604248
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.079015351831913
Distance: 8.855627059936523
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 7, 1], dtype=torch.int32)
Action: down
Reward: 0.07235755771398544
Distance: 8.83464241027832
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 6, 0], dtype=torch.int32)
Action: down
Reward: 0.41844692826271057
Distance: 8.662284851074219
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.07924709469079971
Distance: 8.143837928771973
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 5, 0], dtype=torch.int32)
Action: down
Reward: 0.8668407201766968
Distance: 8.123085021972656
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.09657087177038193
Distance: 7.156244277954102
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 3, 0], dtype=torch.int32)
Action: up
Reward: 0.0265439972281456
Distance: 6.9596734046936035
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.13514956831932068
Distance: 6.833129405975342
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([3, 5, 1], dtype=torch.int32)
Action: pickup
Reward: 0.07337655872106552
Distance: 6.868278980255127
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([3, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.43897953629493713
Distance: 6.694902420043945
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.06394825130701065
Distance: 6.155922889709473
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.010013483464717865
Distance: 6.119871139526367
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

