Env ID: [209]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.04429493099451065
Distance: 9.60317325592041
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.09014949947595596
Distance: 9.547468185424805
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.14625510573387146
Distance: 9.537617683410645
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.13956698775291443
Distance: 9.58387279510498
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: 0.0022796615958213806
Distance: 9.62343978881836
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.6320205926895142
Distance: 9.521160125732422
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.16055068373680115
Distance: 10.053180694580078
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.21458300948143005
Distance: 10.113731384277344
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.13801518082618713
Distance: 10.228314399719238
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.2902580201625824
Distance: 9.990299224853516
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.9152778387069702
Distance: 10.180557250976562
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: 0.17810288071632385
Distance: 8.165279388427734
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: 0.17442598938941956
Distance: 7.887176513671875
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.17241200804710388
Distance: 7.61275053024292
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.13615378737449646
Distance: 7.685162544250488
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.027058221399784088
Distance: 7.721316337585449
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.17040500044822693
Distance: 7.648374557495117
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.12271585315465927
Distance: 7.718779563903809
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.11755122989416122
Distance: 7.496063709259033
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.0019999518990516663
Distance: 7.278512477874756
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

