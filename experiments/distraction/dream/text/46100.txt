Env ID: [374]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.14733180403709412
Distance: 8.811030387878418
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.03803596645593643
Distance: 8.858362197875977
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.0977550521492958
Distance: 8.796398162841797
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1310039460659027
Distance: 8.794153213500977
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.1529003083705902
Distance: 8.825157165527344
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.15856704115867615
Distance: 8.878057479858398
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 2, 0], dtype=torch.int32)
Action: noop
Reward: -0.05802116543054581
Distance: 8.936624526977539
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 2, 0], dtype=torch.int32)
Action: left
Reward: -0.22601088881492615
Distance: 8.894645690917969
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 2, 0], dtype=torch.int32)
Action: noop
Reward: 0.0017360672354698181
Distance: 9.02065658569336
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.020198442041873932
Distance: 8.918920516967773
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.13692054152488708
Distance: 8.839118957519531
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.04273948818445206
Distance: 8.876039505004883
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.5309616327285767
Distance: 8.818778991699219
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.5653854608535767
Distance: 9.249740600585938
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.07489357143640518
Distance: 9.715126037597656
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([1, 7, 0], dtype=torch.int32)
Action: right
Reward: -0.2700696885585785
Distance: 9.690019607543945
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([2, 7, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1186443343758583
Distance: 9.860089302062988
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([2, 7, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.019887350499629974
Distance: 9.87873363494873
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([2, 7, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.0840391144156456
Distance: 9.758846282958984
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([2, 7, 0], dtype=torch.int32)
Action: pickup
Reward: -0.16131266951560974
Distance: 9.574807167053223
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

