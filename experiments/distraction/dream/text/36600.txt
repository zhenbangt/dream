Env ID: [374]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.21243152022361755
Distance: 7.628790855407715
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.07978544384241104
Distance: 7.741222381591797
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.11789760738611221
Distance: 7.721007823944092
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.08656320720911026
Distance: 7.738905429840088
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.18016251921653748
Distance: 7.725468635559082
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 3, 0], dtype=torch.int32)
Action: down
Reward: -0.13906916975975037
Distance: 7.805631160736084
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 2, 0], dtype=torch.int32)
Action: noop
Reward: 0.02616109699010849
Distance: 7.844700336456299
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 2, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.35138043761253357
Distance: 7.718539237976074
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

