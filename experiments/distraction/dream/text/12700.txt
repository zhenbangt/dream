Env ID: [495]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.10847625881433487
Distance: 4.932888031005859
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.21066102385520935
Distance: 4.941364288330078
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 7, 1], dtype=torch.int32)
Action: left
Reward: -0.07511339336633682
Distance: 5.052025318145752
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.40103206038475037
Distance: 5.027138710021973
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([1, 7, 0], dtype=torch.int32)
Action: pickup
Reward: -0.21458396315574646
Distance: 5.3281707763671875
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([1, 7, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.0819944366812706
Distance: 5.442754745483398
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([1, 7, 0], dtype=torch.int32)
Action: up
Reward: -0.13230952620506287
Distance: 5.260760307312012
Next state: tensor([1, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([1, 8, 0], dtype=torch.int32)
Action: drop
Reward: -0.2508617341518402
Distance: 5.293069839477539
Next state: tensor([1, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([1, 8, 0], dtype=torch.int32)
Action: pickup
Reward: -0.006974793970584869
Distance: 5.443931579589844
Next state: tensor([1, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([1, 8, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.2544308602809906
Distance: 5.3509063720703125
Next state: tensor([1, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([1, 8, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.05363664776086807
Distance: 5.505337238311768
Next state: tensor([1, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([1, 8, 0], dtype=torch.int32)
Action: right
Reward: 0.1637648642063141
Distance: 5.4589738845825195
Next state: tensor([2, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([2, 8, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.11833276599645615
Distance: 5.19520902633667
Next state: tensor([2, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([2, 8, 0], dtype=torch.int32)
Action: drop
Reward: 0.01088085025548935
Distance: 4.976876258850098
Next state: tensor([2, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([2, 8, 0], dtype=torch.int32)
Action: left
Reward: 0.165706068277359
Distance: 4.865995407104492
Next state: tensor([1, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([1, 8, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.16325339674949646
Distance: 4.600289344787598
Next state: tensor([1, 8, 0], dtype=torch.int32)
================================================================================

