Env ID: [330]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.10801086574792862
Distance: 8.167816162109375
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.1145225539803505
Distance: 8.175827026367188
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09356365352869034
Distance: 8.190349578857422
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.0501340851187706
Distance: 8.183913230895996
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.9384876489639282
Distance: 8.03377914428711
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.004447557032108307
Distance: 8.87226676940918
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.05334606021642685
Distance: 8.776714324951172
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.132686048746109
Distance: 8.623368263244629
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.05832710117101669
Distance: 8.390682220458984
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.01532898098230362
Distance: 8.232355117797852
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.091466523706913
Distance: 8.147684097290039
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.07763443142175674
Distance: 8.139150619506836
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.1610771119594574
Distance: 8.116785049438477
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.18732509016990662
Distance: 8.177862167358398
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.5408731698989868
Distance: 8.26518726348877
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.11323700100183487
Distance: 8.706060409545898
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.4812789857387543
Distance: 8.719297409057617
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.26107558608055115
Distance: 9.100576400756836
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.14093169569969177
Distance: 9.261651992797852
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 1, 1], dtype=torch.int32)
Action: right
Reward: 0.12673130631446838
Distance: 9.302583694458008
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

