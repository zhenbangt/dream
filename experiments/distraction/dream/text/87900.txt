Env ID: [319]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.09937820583581924
Distance: 8.614522933959961
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1019197478890419
Distance: 8.613901138305664
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9759286642074585
Distance: 8.61582088470459
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.11564121395349503
Distance: 7.539892196655273
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.07406721264123917
Distance: 7.555533409118652
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.026514150202274323
Distance: 7.529600620269775
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.13334807753562927
Distance: 7.456114768981934
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.04643974453210831
Distance: 7.489462852478027
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.14991292357444763
Distance: 7.4359025955200195
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.0209745392203331
Distance: 7.485815525054932
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2791198790073395
Distance: 7.364840984344482
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.020000554621219635
Distance: 6.985721111297607
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.055316828191280365
Distance: 6.905721664428711
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.015057660639286041
Distance: 6.7504048347473145
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.5669424533843994
Distance: 6.665462493896484
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.11942610889673233
Distance: 3.9985201358795166
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.06951341778039932
Distance: 4.017946243286133
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.7538325786590576
Distance: 3.987459659576416
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.2202751338481903
Distance: 0.13362722098827362
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([7, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.07724682241678238
Distance: 0.2539023458957672
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

