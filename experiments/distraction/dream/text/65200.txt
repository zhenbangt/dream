Env ID: [539]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.041609860956668854
Distance: 7.938183784484863
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.059972383081912994
Distance: 7.879793643951416
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.08137903362512589
Distance: 7.839766025543213
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.14520224928855896
Distance: 7.821145057678223
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.09477577358484268
Distance: 7.866347312927246
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.3065789937973022
Distance: 7.861123085021973
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.07466020435094833
Distance: 6.4545440673828125
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.30358895659446716
Distance: 6.279883861541748
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.380319207906723
Distance: 6.48347282409668
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.0591357946395874
Distance: 6.763792037963867
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: 0.1314190924167633
Distance: 5.604656219482422
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 7, 1], dtype=torch.int32)
Action: down
Reward: -0.09131822735071182
Distance: 5.373237133026123
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 6, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.22187384963035583
Distance: 5.364555358886719
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

