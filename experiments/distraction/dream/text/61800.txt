Env ID: [242]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08583984524011612
Distance: 9.085912704467773
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.8139485120773315
Distance: 9.071752548217773
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -1.1378499269485474
Distance: 7.157804012298584
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.16515102982521057
Distance: 8.195653915405273
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.7419286966323853
Distance: 7.930502891540527
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.015079878270626068
Distance: 8.572431564331055
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.2846265733242035
Distance: 8.457351684570312
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.8846849203109741
Distance: 8.64197826385498
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.19214877486228943
Distance: 7.657293319702148
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.4351859986782074
Distance: 7.749442100524902
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

