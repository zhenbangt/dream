Env ID: [176]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.087621308863163
Distance: 9.643667221069336
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.2049156129360199
Distance: 9.631288528442383
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.21901854872703552
Distance: 9.736204147338867
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.1114925146102905
Distance: 9.855222702026367
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2636962831020355
Distance: 8.643730163574219
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.23336449265480042
Distance: 8.807426452636719
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.18144646286964417
Distance: 8.474061965942383
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.2487376928329468
Distance: 8.192615509033203
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.08380451053380966
Distance: 6.843877792358398
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

