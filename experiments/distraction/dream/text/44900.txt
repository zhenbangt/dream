Env ID: [462]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.09851036220788956
Distance: 8.558773040771484
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1363445222377777
Distance: 8.557283401489258
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.14458045363426208
Distance: 8.5936279296875
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.2223258912563324
Distance: 8.638208389282227
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.11820755153894424
Distance: 8.760534286499023
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.006460569798946381
Distance: 8.778741836547852
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.08705291897058487
Distance: 8.672281265258789
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.17373332381248474
Distance: 8.659334182739258
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.056383706629276276
Distance: 8.733067512512207
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.262094110250473
Distance: 8.689451217651367
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.27411308884620667
Distance: 8.851545333862305
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3164648115634918
Distance: 8.477432250976562
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.1001354232430458
Distance: 8.060967445373535
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.15778693556785583
Distance: 8.061102867126465
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.7669988870620728
Distance: 8.118889808654785
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.02610168606042862
Distance: 8.785888671875
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.11035976558923721
Distance: 8.711990356445312
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.137547105550766
Distance: 8.722350120544434
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 7, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.08379707485437393
Distance: 8.759897232055664
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

