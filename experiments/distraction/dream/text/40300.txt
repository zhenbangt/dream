Env ID: [385]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.2501278817653656
Distance: 6.2535295486450195
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.2351151406764984
Distance: 6.40365743637085
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1314149796962738
Distance: 6.5387725830078125
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.0007864013314247131
Distance: 6.570187568664551
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.2979613244533539
Distance: 6.470973968505859
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.07522859424352646
Distance: 6.668935298919678
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.03680982440710068
Distance: 6.493706703186035
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1930619180202484
Distance: 6.356896877288818
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.028536222875118256
Distance: 6.449958801269531
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.28668078780174255
Distance: 6.321422576904297
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.014500521123409271
Distance: 6.508103370666504
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.22651156783103943
Distance: 6.3936028480529785
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.009298421442508698
Distance: 6.520114421844482
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1619759500026703
Distance: 6.429412841796875
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.04146871715784073
Distance: 6.49138879776001
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.12025604397058487
Distance: 6.432857513427734
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.06161461025476456
Distance: 6.453113555908203
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.11505470424890518
Distance: 6.414728164672852
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.048203565180301666
Distance: 6.429782867431641
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.13686475157737732
Distance: 6.377986431121826
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

