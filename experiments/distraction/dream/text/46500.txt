Env ID: [462]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.10775051265954971
Distance: 8.626360893249512
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.1003623977303505
Distance: 8.634111404418945
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.08441219478845596
Distance: 8.63447380065918
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.12031231075525284
Distance: 8.61888599395752
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.12639197707176208
Distance: 8.639198303222656
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.11497173458337784
Distance: 8.665590286254883
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.14158686995506287
Distance: 8.680562019348145
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: 0.024138830602169037
Distance: 8.722148895263672
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.1088167205452919
Distance: 8.598010063171387
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.18736419081687927
Distance: 8.606826782226562
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.074376679956913
Distance: 8.694190979003906
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2334846556186676
Distance: 8.668567657470703
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 0, 1], dtype=torch.int32)
Action: pickup
Reward: 0.02567901462316513
Distance: 8.3350830078125
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 0, 1], dtype=torch.int32)
Action: drop
Reward: -0.36862143874168396
Distance: 8.209403991699219
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1538902223110199
Distance: 8.478025436401367
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.4382339417934418
Distance: 8.531915664672852
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.1980176866054535
Distance: 8.870149612426758
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.05079688876867294
Distance: 8.968167304992676
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.24050769209861755
Distance: 8.817370414733887
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.09114418178796768
Distance: 8.957878112792969
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

