Env ID: [132]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.16637954115867615
Distance: 8.46975326538086
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2127613127231598
Distance: 8.5361328125
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: 0.06083526462316513
Distance: 8.223371505737305
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: noop
Reward: 0.17306938767433167
Distance: 8.062536239624023
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 1, 1], dtype=torch.int32)
Action: down
Reward: 0.09767045825719833
Distance: 7.789466857910156
Next state: tensor([0, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 0, 0], dtype=torch.int32)
Action: pickup
Reward: 0.19228258728981018
Distance: 7.591796398162842
Next state: tensor([0, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 0, 0], dtype=torch.int32)
Action: noop
Reward: -0.09454689174890518
Distance: 7.299513816833496
Next state: tensor([0, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 0, 0], dtype=torch.int32)
Action: right
Reward: -0.3509574830532074
Distance: 7.294060707092285
Next state: tensor([1, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([1, 0, 0], dtype=torch.int32)
Action: right
Reward: -0.08531675487756729
Distance: 7.545018196105957
Next state: tensor([2, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([2, 0, 0], dtype=torch.int32)
Action: noop
Reward: 0.08992137759923935
Distance: 7.530334949493408
Next state: tensor([2, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([2, 0, 0], dtype=torch.int32)
Action: drop
Reward: 0.05692615360021591
Distance: 7.340413570404053
Next state: tensor([2, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([2, 0, 0], dtype=torch.int32)
Action: right
Reward: -0.016571618616580963
Distance: 7.183487415313721
Next state: tensor([3, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([3, 0, 0], dtype=torch.int32)
Action: right
Reward: -0.06189260631799698
Distance: 7.1000590324401855
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 0, 1], dtype=torch.int32)
Action: drop
Reward: -0.19157752394676208
Distance: 7.061951637268066
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 0, 1], dtype=torch.int32)
Action: left
Reward: -0.14896830916404724
Distance: 7.153529167175293
Next state: tensor([3, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([3, 0, 0], dtype=torch.int32)
Action: up
Reward: -0.08394155651330948
Distance: 7.202497482299805
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([3, 1, 0], dtype=torch.int32)
Action: pickup
Reward: -0.10575924068689346
Distance: 7.186439037322998
Next state: tensor([3, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([3, 1, 0], dtype=torch.int32)
Action: up
Reward: -0.30960187315940857
Distance: 7.192198276519775
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.0011697784066200256
Distance: 7.401800155639648
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 2, 0], dtype=torch.int32)
Action: drop
Reward: -0.028392888605594635
Distance: 7.302969932556152
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

