Env ID: [385]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.1355501115322113
Distance: 7.429746627807617
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.14670667052268982
Distance: 7.465296745300293
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.023160837590694427
Distance: 7.512003421783447
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.023858167231082916
Distance: 7.388842582702637
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.10775480419397354
Distance: 7.3127007484436035
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.09388599544763565
Distance: 7.320455551147461
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.217716783285141
Distance: 7.3143415451049805
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.07734880596399307
Distance: 7.432058334350586
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.23350581526756287
Distance: 7.409407138824463
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 7, 1], dtype=torch.int32)
Action: drop
Reward: -0.32156381011009216
Distance: 7.54291296005249
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.3443637788295746
Distance: 7.764476776123047
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 7, 1], dtype=torch.int32)
Action: drop
Reward: -0.4110637605190277
Distance: 8.008840560913086
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.2457776963710785
Distance: 8.319904327392578
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.23133811354637146
Distance: 8.465682029724121
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.2719617784023285
Distance: 8.597020149230957
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 7, 1], dtype=torch.int32)
Action: drop
Reward: -0.22794684767723083
Distance: 8.76898193359375
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 7, 1], dtype=torch.int32)
Action: drop
Reward: -0.17650851607322693
Distance: 8.896928787231445
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.14185580611228943
Distance: 8.973437309265137
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.23955878615379333
Distance: 9.01529312133789
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 7, 1], dtype=torch.int32)
Action: drop
Reward: -0.12664946913719177
Distance: 9.154851913452148
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

