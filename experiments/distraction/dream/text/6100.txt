Env ID: [550]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.021641351282596588
Distance: 7.724030494689941
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.0947175994515419
Distance: 7.645671844482422
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.25252875685691833
Distance: 7.640389442443848
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: 0.022157572209835052
Distance: 7.7929182052612305
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([6, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.21048030257225037
Distance: 7.670760631561279
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.09348497539758682
Distance: 7.781240940093994
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.10243568569421768
Distance: 7.774725914001465
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.016028784215450287
Distance: 7.777161598205566
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 3, 0], dtype=torch.int32)
Action: noop
Reward: -0.16810378432273865
Distance: 7.6611328125
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([6, 3, 0], dtype=torch.int32)
Action: noop
Reward: -0.3200546205043793
Distance: 7.729236602783203
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([6, 3, 0], dtype=torch.int32)
Action: left
Reward: -0.02263794094324112
Distance: 7.949291229248047
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.010368727147579193
Distance: 7.871929168701172
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([6, 3, 0], dtype=torch.int32)
Action: down
Reward: -0.2862611711025238
Distance: 7.761560440063477
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([6, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.20167216658592224
Distance: 7.947821617126465
Next state: tensor([6, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([6, 2, 0], dtype=torch.int32)
Action: left
Reward: -0.05275402218103409
Distance: 8.049493789672852
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 2, 0], dtype=torch.int32)
Action: up
Reward: 0.07026710361242294
Distance: 8.00224781036377
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.06713428348302841
Distance: 7.8319807052612305
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.01825561374425888
Distance: 7.664846420288086
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.06031189113855362
Distance: 7.546590805053711
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: 0.10773315280675888
Distance: 7.506902694702148
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

