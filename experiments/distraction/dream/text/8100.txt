Env ID: [44]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.16496190428733826
Distance: 7.7044477462768555
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: 0.016805551946163177
Distance: 7.769409656524658
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.04291496425867081
Distance: 7.652604103088379
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.16109856963157654
Distance: 7.595519065856934
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.21776780486106873
Distance: 7.656617641448975
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1129397377371788
Distance: 7.774385452270508
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.6261988878250122
Distance: 7.561445713043213
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 1, 1], dtype=torch.int32)
Action: down
Reward: 0.5025132894515991
Distance: 8.087644577026367
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 0, 0], dtype=torch.int32)
Action: pickup
Reward: 0.03802909702062607
Distance: 7.48513126373291
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 0, 0], dtype=torch.int32)
Action: down
Reward: 0.02194824069738388
Distance: 7.347102165222168
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 0, 0], dtype=torch.int32)
Action: drop
Reward: -0.1151805892586708
Distance: 7.225153923034668
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 0, 0], dtype=torch.int32)
Action: drop
Reward: -0.10577068477869034
Distance: 7.240334510803223
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 0, 0], dtype=torch.int32)
Action: down
Reward: 0.08492746204137802
Distance: 7.246105194091797
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 0, 0], dtype=torch.int32)
Action: drop
Reward: -0.00200662761926651
Distance: 7.061177730560303
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 0, 0], dtype=torch.int32)
Action: pickup
Reward: 0.007295034825801849
Distance: 6.963184356689453
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 0, 0], dtype=torch.int32)
Action: noop
Reward: 0.038269899785518646
Distance: 6.855889320373535
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 0, 0], dtype=torch.int32)
Action: down
Reward: 0.3552364408969879
Distance: 6.7176194190979
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 0, 0], dtype=torch.int32)
Action: pickup
Reward: -0.047110654413700104
Distance: 6.262382984161377
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 0, 0], dtype=torch.int32)
Action: drop
Reward: -0.0070057883858680725
Distance: 6.209493637084961
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 0, 0], dtype=torch.int32)
Action: down
Reward: 0.10189618915319443
Distance: 6.116499423980713
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

