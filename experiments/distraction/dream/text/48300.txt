Env ID: [550]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.09149131923913956
Distance: 8.910074234008789
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08759651333093643
Distance: 8.901565551757812
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.11879215389490128
Distance: 8.889162063598633
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.061953164637088776
Distance: 8.907954216003418
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.05457267910242081
Distance: 8.86990737915039
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.0789857879281044
Distance: 8.824480056762695
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.28991374373435974
Distance: 8.803465843200684
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.17304953932762146
Distance: 8.993379592895508
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.05412731319665909
Distance: 9.066429138183594
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.0734010711312294
Distance: 9.020556449890137
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: 0.03529777377843857
Distance: 8.99395751953125
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.06969032436609268
Distance: 8.858659744262695
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.13370665907859802
Distance: 8.828350067138672
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.1390148103237152
Distance: 8.862056732177734
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.2419658601284027
Distance: 8.901071548461914
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.3491435945034027
Distance: 9.043037414550781
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.2704692780971527
Distance: 9.292181015014648
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: 0.002052687108516693
Distance: 9.462650299072266
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.12642058730125427
Distance: 9.360597610473633
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.11305675655603409
Distance: 9.387018203735352
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

