Env ID: [286]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.14780959486961365
Distance: 31.952835083007812
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.16818007826805115
Distance: 32.00064468383789
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.10922012478113174
Distance: 32.068824768066406
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.1402641236782074
Distance: 32.07804489135742
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 7, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.09504470974206924
Distance: 32.118309020996094
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

