Env ID: [418]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.19193992018699646
Distance: 9.716070175170898
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.18862971663475037
Distance: 9.80801010131836
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.19459018111228943
Distance: 9.896639823913574
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 5, 1], dtype=torch.int32)
Action: up
Reward: 0.05250205844640732
Distance: 9.991230010986328
Next state: tensor([5, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 6, 0], dtype=torch.int32)
Action: right
Reward: -0.3137737214565277
Distance: 9.838727951049805
Next state: tensor([6, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 6, 0], dtype=torch.int32)
Action: drop
Reward: -0.22028788924217224
Distance: 10.052501678466797
Next state: tensor([6, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 6, 0], dtype=torch.int32)
Action: drop
Reward: -0.17054042220115662
Distance: 10.172789573669434
Next state: tensor([6, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([6, 6, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.12711676955223083
Distance: 10.243330001831055
Next state: tensor([6, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 6, 0], dtype=torch.int32)
Action: right
Reward: -0.2028861939907074
Distance: 10.27044677734375
Next state: tensor([7, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([7, 6, 0], dtype=torch.int32)
Action: right
Reward: -0.052538491785526276
Distance: 10.373332977294922
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 6, 0], dtype=torch.int32)
Action: noop
Reward: -0.07358989864587784
Distance: 10.325871467590332
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 6, 0], dtype=torch.int32)
Action: right
Reward: -0.20932921767234802
Distance: 10.299461364746094
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 6, 0], dtype=torch.int32)
Action: noop
Reward: -0.17248496413230896
Distance: 10.408790588378906
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 6, 0], dtype=torch.int32)
Action: drop
Reward: -0.2507549226284027
Distance: 10.48127555847168
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.13465937972068787
Distance: 10.632030487060547
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 5, 0], dtype=torch.int32)
Action: drop
Reward: -0.060747720301151276
Distance: 10.6666898727417
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 5, 0], dtype=torch.int32)
Action: right
Reward: 0.04269447177648544
Distance: 10.627437591552734
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 5, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.008219145238399506
Distance: 10.484743118286133
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

