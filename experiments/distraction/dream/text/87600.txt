Env ID: [341]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.0756431594491005
Distance: 8.38113021850586
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10117683559656143
Distance: 8.356773376464844
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3449977934360504
Distance: 8.357950210571289
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10166797786951065
Distance: 7.912952423095703
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.0550747886300087
Distance: 7.914620399475098
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.029819585382938385
Distance: 7.86969518661499
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.15138444304466248
Distance: 7.7995147705078125
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.1953941285610199
Distance: 7.8508992195129395
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.01573934406042099
Distance: 7.946293354034424
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 7, 1], dtype=torch.int32)
Action: down
Reward: -0.016007043421268463
Distance: 7.830554008483887
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 6, 0], dtype=torch.int32)
Action: up
Reward: -0.19231805205345154
Distance: 7.746561050415039
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.01556243747472763
Distance: 7.838879108428955
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.17335614562034607
Distance: 7.723316669464111
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.10949907451868057
Distance: 7.796672821044922
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.883418083190918
Distance: 7.806171894073486
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.06843814998865128
Distance: 3.82275390625
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.024831391870975494
Distance: 3.791192054748535
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.54175066947937
Distance: 3.7160234451293945
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.10310535877943039
Distance: 0.07427287101745605
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

