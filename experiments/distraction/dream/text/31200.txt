Env ID: [462]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.12137756496667862
Distance: 7.367561340332031
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: 0.09561290591955185
Distance: 7.388938903808594
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.04504594951868057
Distance: 7.193325996398926
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 5, 1], dtype=torch.int32)
Action: up
Reward: -0.22355565428733826
Distance: 7.13837194442749
Next state: tensor([4, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 6, 0], dtype=torch.int32)
Action: up
Reward: -0.07615146785974503
Distance: 7.261927604675293
Next state: tensor([4, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 7, 0], dtype=torch.int32)
Action: left
Reward: 0.07196082919836044
Distance: 7.238079071044922
Next state: tensor([3, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 7, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.13143643736839294
Distance: 7.066118240356445
Next state: tensor([3, 7, 0], dtype=torch.int32)
================================================================================

