Env ID: [528]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.12484798580408096
Distance: 9.916925430297852
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.07405338436365128
Distance: 9.941773414611816
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.062123872339725494
Distance: 9.915826797485352
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.09405479580163956
Distance: 9.877950668334961
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10897407680749893
Distance: 9.872005462646484
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.02358398586511612
Distance: 9.880979537963867
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.22387561202049255
Distance: 9.804563522338867
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7411264181137085
Distance: 9.928439140319824
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 7, 1], dtype=torch.int32)
Action: up
Reward: -0.4964538514614105
Distance: 9.087312698364258
Next state: tensor([8, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 8, 0], dtype=torch.int32)
Action: down
Reward: -0.06838855892419815
Distance: 9.483766555786133
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.13055744767189026
Distance: 9.452155113220215
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.15001258254051208
Distance: 9.221597671508789
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.07837162166833878
Distance: 9.271610260009766
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3246065080165863
Distance: 9.249981880187988
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 7, 1], dtype=torch.int32)
Action: up
Reward: -0.0634227767586708
Distance: 9.474588394165039
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 8, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.18785056471824646
Distance: 9.438011169433594
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

