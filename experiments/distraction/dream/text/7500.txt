Env ID: [110]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.04043541103601456
Distance: 8.683605194091797
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.16488131880760193
Distance: 8.624040603637695
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.11366520076990128
Distance: 8.688921928405762
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([6, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.1153465285897255
Distance: 8.702587127685547
Next state: tensor([6, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([6, 5, 0], dtype=torch.int32)
Action: right
Reward: -0.16597327589988708
Distance: 8.717933654785156
Next state: tensor([7, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([7, 5, 0], dtype=torch.int32)
Action: up
Reward: -0.16597899794578552
Distance: 8.783906936645508
Next state: tensor([7, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([7, 6, 0], dtype=torch.int32)
Action: right
Reward: 0.030556105077266693
Distance: 8.849885940551758
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 6, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.17260798811912537
Distance: 8.719329833984375
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 6, 0], dtype=torch.int32)
Action: right
Reward: 0.006955526769161224
Distance: 8.791937828063965
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 6, 0], dtype=torch.int32)
Action: right
Reward: -0.08800754696130753
Distance: 8.684982299804688
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 6, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.20662173628807068
Distance: 8.672989845275879
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 6, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.15461120009422302
Distance: 8.779611587524414
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 6, 0], dtype=torch.int32)
Action: right
Reward: 0.005924604833126068
Distance: 8.834222793579102
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 6, 0], dtype=torch.int32)
Action: right
Reward: -0.055242158472537994
Distance: 8.72829818725586
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.173618882894516
Distance: 8.683540344238281
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 6, 0], dtype=torch.int32)
Action: noop
Reward: -0.023575402796268463
Distance: 8.757159233093262
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 6, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1591392457485199
Distance: 8.680734634399414
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 6, 0], dtype=torch.int32)
Action: left
Reward: -0.2813030183315277
Distance: 8.739873886108398
Next state: tensor([7, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([7, 6, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.1524263322353363
Distance: 8.92117691040039
Next state: tensor([7, 6, 0], dtype=torch.int32)
================================================================================

