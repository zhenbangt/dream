Env ID: [363]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.05450878292322159
Distance: 9.572949409484863
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.09558258205652237
Distance: 9.527458190917969
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.13288363814353943
Distance: 9.523040771484375
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.2538934648036957
Distance: 9.555924415588379
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.05377044528722763
Distance: 9.709817886352539
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.3031068742275238
Distance: 9.556047439575195
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.11904392391443253
Distance: 9.759154319763184
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 2, 0], dtype=torch.int32)
Action: pickup
Reward: -0.2472740113735199
Distance: 9.7781982421875
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.42993220686912537
Distance: 9.925472259521484
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: 0.09994067996740341
Distance: 10.255404472351074
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.09117946773767471
Distance: 10.055463790893555
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.3077367842197418
Distance: 10.046643257141113
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: 0.09968795627355576
Distance: 9.638906478881836
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: 0.16491928696632385
Distance: 9.439218521118164
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.26427707076072693
Distance: 9.174299240112305
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.04697094112634659
Distance: 9.338576316833496
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.03466758877038956
Distance: 9.285547256469727
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.2636600434780121
Distance: 9.22021484375
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.2899394929409027
Distance: 9.383874893188477
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.2083049714565277
Distance: 9.573814392089844
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

