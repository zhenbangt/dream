Env ID: [396]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.12810572981834412
Distance: 9.759194374084473
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.042887113988399506
Distance: 9.787300109863281
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([2, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.18229255080223083
Distance: 9.644412994384766
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([1, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.19361838698387146
Distance: 9.726705551147461
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 4, 1], dtype=torch.int32)
Action: right
Reward: 0.10244693607091904
Distance: 9.820323944091797
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([1, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.2406364381313324
Distance: 9.617877006530762
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.17489680647850037
Distance: 9.758513450622559
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.13993415236473083
Distance: 9.833410263061523
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.13665446639060974
Distance: 9.873344421386719
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 5, 0], dtype=torch.int32)
Action: left
Reward: -0.26203784346580505
Distance: 9.909998893737793
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 5, 0], dtype=torch.int32)
Action: left
Reward: -0.2451530396938324
Distance: 10.072036743164062
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 5, 0], dtype=torch.int32)
Action: down
Reward: -0.11268196254968643
Distance: 10.21718978881836
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.13690051436424255
Distance: 10.22987174987793
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.07847271114587784
Distance: 10.266772270202637
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 4, 1], dtype=torch.int32)
Action: right
Reward: 0.0017990097403526306
Distance: 10.245244979858398
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([1, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.2797122895717621
Distance: 10.14344596862793
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([1, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.22523459792137146
Distance: 10.323158264160156
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([1, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.18844470381736755
Distance: 10.448392868041992
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([1, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.25392016768455505
Distance: 10.536837577819824
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([1, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12918433547019958
Distance: 10.690757751464844
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

