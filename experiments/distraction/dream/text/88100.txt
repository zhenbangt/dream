Env ID: [55]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.12085113674402237
Distance: 8.671611785888672
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10029087215662003
Distance: 8.692462921142578
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.12858238816261292
Distance: 8.692753791809082
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.08568058162927628
Distance: 8.464171409606934
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.0845237746834755
Distance: 8.449851989746094
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.19081172347068787
Distance: 8.434375762939453
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.04365120083093643
Distance: 8.525187492370605
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.12628230452537537
Distance: 8.468838691711426
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.11966858059167862
Distance: 8.495121002197266
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: 0.015701673924922943
Distance: 8.514789581298828
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.13015708327293396
Distance: 8.399087905883789
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.1831554174423218
Distance: 8.429244995117188
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.17189350724220276
Distance: 7.146089553833008
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.10531387478113174
Distance: 6.8741960525512695
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: 0.19272032380104065
Distance: 6.879509925842285
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 5, 1], dtype=torch.int32)
Action: up
Reward: -0.6840287446975708
Distance: 6.586789608001709
Next state: tensor([4, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 6, 0], dtype=torch.int32)
Action: down
Reward: 1.5733312368392944
Distance: 7.170818328857422
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.3636096715927124
Distance: 5.4974870681762695
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10399065166711807
Distance: 4.033877372741699
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.25949153304100037
Distance: 4.037868022918701
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

