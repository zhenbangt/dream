Env ID: [363]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.03968944400548935
Distance: 9.939472198486328
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.011237524449825287
Distance: 9.799782752990723
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: 0.021564863622188568
Distance: 9.688545227050781
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.052419282495975494
Distance: 9.566980361938477
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.12094173580408096
Distance: 9.519399642944336
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.2096172273159027
Distance: 9.5403413772583
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([6, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08777771145105362
Distance: 9.649958610534668
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([7, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.024157144129276276
Distance: 9.637736320495605
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([6, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.10461578518152237
Distance: 9.561893463134766
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([6, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.11727295070886612
Distance: 9.566509246826172
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([7, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.043637849390506744
Distance: 9.583782196044922
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([6, 4, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.26102885603904724
Distance: 9.527420043945312
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

