Env ID: [506]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.028302766382694244
Distance: 8.551405906677246
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.11765823513269424
Distance: 8.479708671569824
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.21494349837303162
Distance: 8.497366905212402
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.09809646755456924
Distance: 8.612310409545898
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.13069495558738708
Distance: 8.610406875610352
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.15360793471336365
Distance: 8.641101837158203
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 8, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.438603013753891
Distance: 8.694709777832031
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

