Env ID: [418]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.009743116796016693
Distance: 9.740394592285156
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.22712764143943787
Distance: 9.630651473999023
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.05128536373376846
Distance: 9.757779121398926
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.08320274204015732
Distance: 9.709064483642578
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.14702758193016052
Distance: 9.525861740112305
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: 0.11041488498449326
Distance: 9.57288932800293
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.15325793623924255
Distance: 9.36247444152832
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1778196394443512
Distance: 9.415732383728027
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.28298434615135193
Distance: 9.13791275024414
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([7, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.37626591324806213
Distance: 9.320897102355957
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.24923285841941833
Distance: 8.84463119506836
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([7, 4, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.017901994287967682
Distance: 8.993864059448242
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

