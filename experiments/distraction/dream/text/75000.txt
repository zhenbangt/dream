Env ID: [121]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.05280742794275284
Distance: 9.24787712097168
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10315094143152237
Distance: 9.200684547424316
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.05115547031164169
Distance: 9.203835487365723
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.008797071874141693
Distance: 9.052680015563965
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.005380056798458099
Distance: 8.943882942199707
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.17347201704978943
Distance: 8.838502883911133
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3013109266757965
Distance: 8.911974906921387
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.010692976415157318
Distance: 8.510663986206055
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.2546420991420746
Distance: 8.399971008300781
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.13978347182273865
Distance: 8.55461311340332
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.530464172363281
Distance: 8.594396591186523
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.24629655480384827
Distance: 3.963932514190674
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.10045848041772842
Distance: 3.61763596534729
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.689817190170288
Distance: 3.6180944442749023
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.1376120150089264
Distance: 0.8282773494720459
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 3, 0], dtype=torch.int32)
Action: down
Reward: -0.007434286177158356
Distance: 0.590665340423584
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.18176713585853577
Distance: 0.4980996251106262
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.162471741437912
Distance: 0.5798667669296265
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.2554653584957123
Distance: 0.6423385143280029
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 5, 1], dtype=torch.int32)
Action: up
Reward: -0.3068557679653168
Distance: 0.7978038787841797
Next state: tensor([5, 6, 0], dtype=torch.int32)
================================================================================

