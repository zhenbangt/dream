Env ID: [308]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.22335967421531677
Distance: 8.019386291503906
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.3523155152797699
Distance: 8.142745971679688
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.1728624403476715
Distance: 8.395061492919922
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.25268134474754333
Distance: 8.122199058532715
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.5072246789932251
Distance: 8.274880409240723
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([2, 3, 0], dtype=torch.int32)
Action: up
Reward: 0.1004575714468956
Distance: 8.68210506439209
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([2, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.12029876559972763
Distance: 8.481647491455078
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([2, 4, 0], dtype=torch.int32)
Action: noop
Reward: 0.2937292158603668
Distance: 8.261348724365234
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([2, 4, 0], dtype=torch.int32)
Action: drop
Reward: 0.12724962830543518
Distance: 7.867619514465332
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([2, 4, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.07912168651819229
Distance: 7.640369892120361
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

