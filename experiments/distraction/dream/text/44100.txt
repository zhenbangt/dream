Env ID: [187]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.05495891720056534
Distance: 8.792444229125977
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.07231102138757706
Distance: 8.747403144836426
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.132450670003891
Distance: 8.719714164733887
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.3175121247768402
Distance: 8.752164840698242
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.10588321834802628
Distance: 8.969676971435547
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.47013339400291443
Distance: 8.975560188293457
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: 0.1792530119419098
Distance: 9.345693588256836
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.012064553797245026
Distance: 9.06644058227539
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.0115266814827919
Distance: 8.97850513458252
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.119470976293087
Distance: 8.890031814575195
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.19982394576072693
Distance: 8.670560836791992
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.6033004522323608
Distance: 8.770384788513184
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.004619218409061432
Distance: 8.067084312438965
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.07848844677209854
Distance: 7.97170352935791
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.0848635658621788
Distance: 7.950191974639893
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.5710018873214722
Distance: 7.765328407287598
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.3152538239955902
Distance: 7.094326496124268
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

