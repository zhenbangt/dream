Env ID: [77]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.06472263485193253
Distance: 9.12785816192627
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.090947724878788
Distance: 9.092580795288086
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.5745586156845093
Distance: 9.083528518676758
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10704956203699112
Distance: 8.40896987915039
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.09600792080163956
Distance: 8.416019439697266
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.12051639705896378
Distance: 8.412027359008789
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.2915180921554565
Distance: 8.432543754577637
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.14908131957054138
Distance: 7.041025638580322
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.06513986736536026
Distance: 7.090106964111328
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.19580087065696716
Distance: 7.055246829986572
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.6160038709640503
Distance: 7.151047706604004
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3594736158847809
Distance: 5.435043811798096
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.2244783341884613
Distance: 4.975570201873779
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.822661399841309
Distance: 5.100048542022705
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 4, 1], dtype=torch.int32)
Action: drop
Reward: 0.002470865845680237
Distance: 0.17738720774650574
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.1189315915107727
Distance: 0.07491634041070938
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.16784805059432983
Distance: 0.09384793043136597
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.3697298765182495
Distance: 0.16169598698616028
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.3357657492160797
Distance: 0.43142586946487427
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.30123171210289
Distance: 0.6671916246414185
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

