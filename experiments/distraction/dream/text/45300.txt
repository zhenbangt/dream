Env ID: [352]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.11056003719568253
Distance: 9.644317626953125
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.07016430050134659
Distance: 9.654877662658691
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1163707748055458
Distance: 9.625041961669922
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: 0.023429296910762787
Distance: 9.641412734985352
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.5474640130996704
Distance: 9.517983436584473
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.16470566391944885
Distance: 9.965447425842285
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.24450549483299255
Distance: 9.7007417678833
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.3303627073764801
Distance: 9.845247268676758
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.2065196931362152
Distance: 9.414884567260742
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.24359360337257385
Distance: 9.521404266357422
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.47280463576316833
Distance: 9.177810668945312
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.03741111606359482
Distance: 9.550615310668945
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.4138236939907074
Distance: 9.413204193115234
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.026226423680782318
Distance: 9.727027893066406
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.03297577053308487
Distance: 9.600801467895508
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.1864677369594574
Distance: 9.533777236938477
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.7754989862442017
Distance: 9.620244979858398
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: 0.3490537703037262
Distance: 10.295743942260742
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.48287829756736755
Distance: 9.84669017791748
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.19894179701805115
Distance: 10.229568481445312
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

