Env ID: [341]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.06832466274499893
Distance: 8.350879669189453
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10049400478601456
Distance: 8.319204330444336
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.40913811326026917
Distance: 8.319698333740234
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.11871395260095596
Distance: 7.81056022644043
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.06993303447961807
Distance: 7.8292741775512695
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.01374826580286026
Distance: 7.7992072105407715
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.14858970046043396
Distance: 7.712955474853516
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.20249328017234802
Distance: 7.761545181274414
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1583036482334137
Distance: 7.864038467407227
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.10040607303380966
Distance: 7.605734825134277
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.2529064118862152
Distance: 7.405328750610352
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.12739333510398865
Distance: 7.558235168457031
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.729132890701294
Distance: 7.585628509521484
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.23380693793296814
Distance: 3.756495714187622
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: up
Reward: 0.3949550688266754
Distance: 3.8903026580810547
Next state: tensor([4, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 6, 0], dtype=torch.int32)
Action: down
Reward: -1.7399049997329712
Distance: 3.3953475952148438
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: 0.898922324180603
Distance: 5.035252571105957
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.4532400071620941
Distance: 4.036330223083496
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.254039764404297
Distance: 4.389570236206055
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.10518001019954681
Distance: 0.03553040325641632
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

