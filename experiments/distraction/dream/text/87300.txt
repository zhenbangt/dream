Env ID: [407]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.09637603908777237
Distance: 8.768659591674805
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.109593965113163
Distance: 8.765035629272461
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9627326726913452
Distance: 8.774629592895508
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.11984024196863174
Distance: 7.711896896362305
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.42933711409568787
Distance: 7.73173713684082
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.04397764056921005
Distance: 8.061074256896973
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.019262410700321198
Distance: 7.9170966148376465
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.025444604456424713
Distance: 7.836359024047852
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.17422208189964294
Distance: 7.76180362701416
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([6, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.1502896249294281
Distance: 7.836025714874268
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.1300143003463745
Distance: 7.88631534576416
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1244906410574913
Distance: 6.656301021575928
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.058217622339725494
Distance: 6.43181037902832
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.12412319332361221
Distance: 6.39002799987793
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.588386058807373
Distance: 6.414151191711426
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.17741569876670837
Distance: 3.7257652282714844
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 5, 1], dtype=torch.int32)
Action: drop
Reward: 0.29550328850746155
Distance: 3.8031809329986572
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.4628697335720062
Distance: 3.40767765045166
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.6011111736297607
Distance: 3.770547389984131
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.10820090770721436
Distance: 0.06943625211715698
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

