Env ID: [121]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.07153568416833878
Distance: 8.657539367675781
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.263157457113266
Distance: 8.629075050354004
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.13691672682762146
Distance: 8.792232513427734
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.29205283522605896
Distance: 8.82914924621582
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.21974143385887146
Distance: 9.021202087402344
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.24975070357322693
Distance: 9.14094352722168
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 7, 1], dtype=torch.int32)
Action: down
Reward: -0.5259596109390259
Distance: 9.290694236755371
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 6, 0], dtype=torch.int32)
Action: right
Reward: -0.3088241517543793
Distance: 9.716653823852539
Next state: tensor([1, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([1, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.4762903153896332
Distance: 9.925477981567383
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([1, 5, 0], dtype=torch.int32)
Action: noop
Reward: -0.42929038405418396
Distance: 10.30176830291748
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([1, 5, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.16117724776268005
Distance: 10.631058692932129
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

