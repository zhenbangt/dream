Env ID: [220]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.1764589250087738
Distance: 6.737171173095703
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.15872612595558167
Distance: 6.813630104064941
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.044321440160274506
Distance: 6.554903984069824
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.07930602878332138
Distance: 6.410582542419434
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([2, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.05029716342687607
Distance: 6.231276512145996
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.8836199045181274
Distance: 6.080979347229004
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.26874837279319763
Distance: 6.864599227905273
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 7, 1], dtype=torch.int32)
Action: up
Reward: 0.04522504657506943
Distance: 7.0333476066589355
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 8, 0], dtype=torch.int32)
Action: pickup
Reward: -0.07020197063684464
Distance: 6.88812255859375
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 8, 0], dtype=torch.int32)
Action: pickup
Reward: 0.2606920301914215
Distance: 6.8583245277404785
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 8, 0], dtype=torch.int32)
Action: pickup
Reward: 0.003459356725215912
Distance: 6.4976325035095215
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 8, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.13953152298927307
Distance: 6.3941731452941895
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

