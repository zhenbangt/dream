Env ID: [484]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.19920024275779724
Distance: 9.166728019714355
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1902107298374176
Distance: 9.265928268432617
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.01791248470544815
Distance: 8.975717544555664
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.10115871578454971
Distance: 8.893630027770996
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.11856231838464737
Distance: 8.89478874206543
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.511411190032959
Distance: 8.913351058959961
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.0464872345328331
Distance: 6.301939964294434
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.012587927281856537
Distance: 6.155452728271484
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.161900132894516
Distance: 6.042864799499512
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.171781063079834
Distance: 6.104764938354492
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.13456907868385315
Distance: 3.83298397064209
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.000850580632686615
Distance: 3.598414897918701
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.0789161920547485
Distance: 3.4975643157958984
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 8, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.6109589338302612
Distance: 2.318648099899292
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

