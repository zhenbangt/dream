Env ID: [363]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.03289756923913956
Distance: 9.520744323730469
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.12555846571922302
Distance: 9.453641891479492
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.11230430752038956
Distance: 9.47920036315918
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.22263869643211365
Distance: 9.491504669189453
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.011133573949337006
Distance: 9.614143371582031
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.23916205763816833
Distance: 9.503009796142578
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 2, 0], dtype=torch.int32)
Action: left
Reward: -0.16595134139060974
Distance: 9.642171859741211
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 2, 0], dtype=torch.int32)
Action: noop
Reward: -0.08277187496423721
Distance: 9.708123207092285
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 2, 0], dtype=torch.int32)
Action: drop
Reward: -0.016233064234256744
Distance: 9.690895080566406
Next state: tensor([3, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.1583143174648285
Distance: 9.607128143310547
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.13043269515037537
Distance: 9.66544246673584
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.10505523532629013
Distance: 9.69587516784668
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([3, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.03939209133386612
Distance: 9.490819931030273
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([3, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.1700349748134613
Distance: 9.430212020874023
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.1671958863735199
Distance: 9.50024700164795
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.007893182337284088
Distance: 9.567442893981934
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.08226928859949112
Distance: 9.475336074829102
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.11555633693933487
Distance: 9.457605361938477
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.17957457900047302
Distance: 9.473161697387695
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.07138099521398544
Distance: 9.552736282348633
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

