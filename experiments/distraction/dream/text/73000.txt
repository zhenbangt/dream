Env ID: [143]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.11233101040124893
Distance: 9.333974838256836
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7939780950546265
Distance: 9.346305847167969
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.05249939113855362
Distance: 8.452327728271484
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.07294998317956924
Distance: 8.404827117919922
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.2051607072353363
Distance: 8.377777099609375
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.474626898765564
Distance: 8.482937812805176
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.009199999272823334
Distance: 6.908310890197754
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.017305754125118256
Distance: 6.7991108894348145
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.19331368803977966
Distance: 6.68180513381958
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.861877679824829
Distance: 6.775118827819824
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.02927885204553604
Distance: 3.8132412433624268
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: 0.22023740410804749
Distance: 3.7425200939178467
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 7, 1], dtype=torch.int32)
Action: right
Reward: 0.1063331589102745
Distance: 3.4222826957702637
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1374126374721527
Distance: 3.215949535369873
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.10465989261865616
Distance: 3.2533621788024902
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9584442377090454
Distance: 3.2580220699310303
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.051859281957149506
Distance: 2.199577808380127
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.1242738738656044
Distance: 2.0477185249328613
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

