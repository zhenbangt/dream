Env ID: [572]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.054051972925662994
Distance: 8.91790771484375
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.05709419399499893
Distance: 8.871959686279297
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.504184365272522
Distance: 8.82905387878418
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.07693328708410263
Distance: 9.233238220214844
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.04110374301671982
Distance: 9.056304931640625
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.1616014540195465
Distance: 8.915201187133789
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.12815704941749573
Distance: 8.653599739074707
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.03904666751623154
Distance: 8.425442695617676
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.20051535964012146
Distance: 8.286396026611328
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.022670365869998932
Distance: 8.386911392211914
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.2838931977748871
Distance: 8.309581756591797
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.2864974737167358
Distance: 8.493474960327148
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 7, 1], dtype=torch.int32)
Action: up
Reward: 0.04170551151037216
Distance: 7.106977462768555
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

