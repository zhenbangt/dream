Env ID: [55]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08796653896570206
Distance: 8.95638656616211
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1147075667977333
Distance: 8.944353103637695
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.4764465391635895
Distance: 8.959060668945312
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1241832748055458
Distance: 8.382614135742188
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.036222077906131744
Distance: 8.406797409057617
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1035381332039833
Distance: 8.343019485473633
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.3757948875427246
Distance: 8.3465576171875
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1574607789516449
Distance: 5.870762825012207
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.012189291417598724
Distance: 5.928223609924316
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.025592423975467682
Distance: 5.816034317016602
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.285814046859741
Distance: 5.741626739501953
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.17481550574302673
Distance: 3.3558127880096436
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.15273484587669373
Distance: 3.4306282997131348
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.8319499492645264
Distance: 3.483363151550293
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.12886199355125427
Distance: 0.551413357257843
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 3, 0], dtype=torch.int32)
Action: down
Reward: -0.09140945225954056
Distance: 0.3225513696670532
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.28186914324760437
Distance: 0.31396082043647766
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.0007407739758491516
Distance: 0.4958299696445465
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.2706862986087799
Distance: 0.39657074213027954
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.16414335370063782
Distance: 0.5672570466995239
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

