Env ID: [121]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.20095309615135193
Distance: 9.748916625976562
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.007892034947872162
Distance: 9.849869728088379
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.09411392360925674
Distance: 9.74197769165039
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.1610771119594574
Distance: 9.736091613769531
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.10138092190027237
Distance: 9.797168731689453
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.012122727930545807
Distance: 9.79854965209961
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.41577014327049255
Distance: 9.710672378540039
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.6492449045181274
Distance: 10.026442527770996
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.5089832544326782
Distance: 10.575687408447266
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 0, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.22878685593605042
Distance: 10.984670639038086
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

