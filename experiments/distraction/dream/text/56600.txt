Env ID: [550]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08571109920740128
Distance: 9.9193696975708
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10905227810144424
Distance: 9.905080795288086
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.130149245262146
Distance: 9.914133071899414
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.07461033016443253
Distance: 8.68398380279541
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.026654817163944244
Distance: 8.658594131469727
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.11279716342687607
Distance: 8.585248947143555
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.0951932892203331
Distance: 8.372451782226562
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.008803747594356537
Distance: 8.177258491516113
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.0671764388680458
Distance: 8.06845474243164
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.17112502455711365
Distance: 8.03563117980957
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 3, 1], dtype=torch.int32)
Action: pickup
Reward: 0.022914789617061615
Distance: 8.106756210327148
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.11870298534631729
Distance: 7.983841419219971
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.15380915999412537
Distance: 8.002544403076172
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.006639100611209869
Distance: 8.056353569030762
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.18153342604637146
Distance: 7.9629926681518555
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.19229945540428162
Distance: 8.044526100158691
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10521850734949112
Distance: 8.136825561523438
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.1410423219203949
Distance: 8.142044067382812
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

