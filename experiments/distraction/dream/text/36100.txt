Env ID: [297]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.06989441066980362
Distance: 8.741125106811523
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.13175925612449646
Distance: 8.711019515991211
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.02709827572107315
Distance: 8.742778778076172
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: 0.09205856174230576
Distance: 8.669877052307129
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.06832943111658096
Distance: 8.477818489074707
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 2, 0], dtype=torch.int32)
Action: up
Reward: 0.018259428441524506
Distance: 8.446147918701172
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.16859111189842224
Distance: 8.327888488769531
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.011469461023807526
Distance: 8.396479606628418
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.20337924361228943
Distance: 8.30794906616211
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([6, 3, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.9597097635269165
Distance: 8.411328315734863
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([6, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.327000230550766
Distance: 9.271038055419922
Next state: tensor([6, 3, 0], dtype=torch.int32)
================================================================================

