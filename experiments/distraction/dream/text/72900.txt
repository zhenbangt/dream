Env ID: [484]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.1303306519985199
Distance: 8.797674179077148
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.1014019027352333
Distance: 8.828004837036133
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.10714969784021378
Distance: 8.82940673828125
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.08927593380212784
Distance: 8.836556434631348
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.16123905777931213
Distance: 8.82583236694336
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.07795391231775284
Distance: 8.564593315124512
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.18454989790916443
Distance: 8.542547225952148
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.15341433882713318
Distance: 8.627097129821777
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.9739054441452026
Distance: 8.680511474609375
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.46940508484840393
Distance: 6.6066060066223145
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.1286202371120453
Distance: 6.037200927734375
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.3521362245082855
Distance: 6.065821170806885
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.137700080871582
Distance: 6.317957401275635
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.5323306322097778
Distance: 4.080257415771484
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.0606137290596962
Distance: 4.512588024139404
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.6825056076049805
Distance: 4.473201751708984
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 8, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.0951007530093193
Distance: 0.6906963586807251
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.03649280220270157
Distance: 0.4955956041812897
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.14644494652748108
Distance: 0.4320884048938751
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.20319944620132446
Distance: 0.4785333573818207
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

