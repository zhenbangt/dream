Env ID: [0]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.12932738661766052
Distance: 7.4897027015686035
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.34664955735206604
Distance: 7.5190300941467285
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.09157047420740128
Distance: 7.072380542755127
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.016765020787715912
Distance: 7.063951015472412
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 5, 1], dtype=torch.int32)
Action: drop
Reward: 0.09526481479406357
Distance: 6.94718599319458
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2968460023403168
Distance: 6.7519211769104
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 1, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.35603436827659607
Distance: 6.948767185211182
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

