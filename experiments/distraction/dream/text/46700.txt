Env ID: [462]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1413017213344574
Distance: 8.71702766418457
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.09945831447839737
Distance: 8.758329391479492
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08241424709558487
Distance: 8.757787704467773
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.0822063460946083
Distance: 8.740201950073242
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.168156236410141
Distance: 8.722408294677734
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.06116428226232529
Distance: 8.79056453704834
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.1275525987148285
Distance: 8.629400253295898
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.08710155636072159
Distance: 8.656952857971191
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.17952784895896912
Distance: 8.644054412841797
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.07370319217443466
Distance: 8.72358226776123
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 0, 1], dtype=torch.int32)
Action: pickup
Reward: -0.036313630640506744
Distance: 8.54987907409668
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 0, 1], dtype=torch.int32)
Action: drop
Reward: -0.39263495802879333
Distance: 8.48619270324707
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.16632899641990662
Distance: 8.778827667236328
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.24221286177635193
Distance: 8.8451566696167
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.1523643434047699
Distance: 8.987369537353516
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.11427078396081924
Distance: 9.03973388671875
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.09664478152990341
Distance: 9.054004669189453
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: 0.0728849396109581
Distance: 8.857359886169434
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.6518532037734985
Distance: 8.68447494506836
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.11237678676843643
Distance: 9.236328125
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

