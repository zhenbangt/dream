Env ID: [451]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.05878410488367081
Distance: 8.743824005126953
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.15688666701316833
Distance: 8.702608108520508
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.0003169998526573181
Distance: 8.75949478149414
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.18646582961082458
Distance: 8.659177780151367
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.3971181809902191
Distance: 8.745643615722656
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08413753658533096
Distance: 9.04276180267334
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.20722541213035583
Distance: 9.026899337768555
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

