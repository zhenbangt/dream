Env ID: [330]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.10865554958581924
Distance: 9.42831039428711
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.06653652340173721
Distance: 9.436965942382812
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.21364650130271912
Distance: 9.403502464294434
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.06778392940759659
Distance: 9.517148971557617
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09054432064294815
Distance: 9.484932899475098
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.10723838955163956
Distance: 9.47547721862793
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.07946643978357315
Distance: 9.482715606689453
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.10521183162927628
Distance: 9.46218204498291
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.569257378578186
Distance: 9.46739387512207
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.17499390244483948
Distance: 9.936651229858398
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.08251533657312393
Distance: 9.661657333374023
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: 0.012410543859004974
Distance: 9.644172668457031
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10436878353357315
Distance: 9.53176212310791
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2923313081264496
Distance: 9.536130905151367
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: 0.0771411880850792
Distance: 9.728462219238281
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -1.2629753351211548
Distance: 9.551321029663086
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: 0.0036758407950401306
Distance: 10.714296340942383
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([1, 1, 0], dtype=torch.int32)
Action: left
Reward: -1.379754662513733
Distance: 10.610620498657227
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -1.0776010751724243
Distance: 11.890375137329102
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.2684701979160309
Distance: 12.867976188659668
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

