Env ID: [154]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.06619606167078018
Distance: 8.857872009277344
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.03400840610265732
Distance: 8.824068069458008
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.17241248488426208
Distance: 8.690059661865234
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.1354881227016449
Distance: 8.762472152709961
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.14393481612205505
Distance: 8.79796028137207
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.7368615865707397
Distance: 8.84189510345459
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1431041657924652
Distance: 7.005033493041992
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.20795974135398865
Distance: 7.048137664794922
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: 0.5325897932052612
Distance: 7.156097412109375
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.2123241424560547
Distance: 6.523507595062256
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.05208311229944229
Distance: 4.211183547973633
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.22011622786521912
Distance: 4.163266658782959
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.3681482076644897
Distance: 4.283382892608643
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 0, 1], dtype=torch.int32)
Action: right
Reward: 0.18128743767738342
Distance: 2.815234661102295
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 0, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.49226507544517517
Distance: 2.533947229385376
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 0, 0], dtype=torch.int32)
Action: drop
Reward: -0.19204768538475037
Distance: 2.9262123107910156
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 0, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.5855926275253296
Distance: 3.0182600021362305
Next state: tensor([5, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 0, 0], dtype=torch.int32)
Action: left
Reward: -0.005913831293582916
Distance: 3.503852605819702
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.7456330060958862
Distance: 3.409766435623169
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.5534578561782837
Distance: 4.055399417877197
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

