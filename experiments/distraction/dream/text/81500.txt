Env ID: [363]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.12460193783044815
Distance: 8.916315078735352
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.11446819454431534
Distance: 8.940917015075684
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.0829978957772255
Distance: 8.955385208129883
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1617484986782074
Distance: 8.938383102416992
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.05387458950281143
Distance: 9.000131607055664
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.08816490322351456
Distance: 8.95400619506836
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.13859900832176208
Distance: 8.942171096801758
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2501491606235504
Distance: 8.980770111083984
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.06606731563806534
Distance: 8.630620956420898
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.03431663662195206
Distance: 8.596688270568848
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.3741022050380707
Distance: 8.531004905700684
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 3.6733040809631348
Distance: 8.805107116699219
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.39158114790916443
Distance: 5.031803131103516
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.16799584031105042
Distance: 5.3233842849731445
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.771823406219482
Distance: 5.055388450622559
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.09651712328195572
Distance: 0.1835651695728302
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.4957873821258545
Distance: 0.1800822913646698
Next state: tensor([8, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 5, 0], dtype=torch.int32)
Action: up
Reward: -0.2729831635951996
Distance: 0.5758696794509888
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 6, 0], dtype=torch.int32)
Action: up
Reward: -0.2811839282512665
Distance: 0.7488528490066528
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 7, 1], dtype=torch.int32)
Action: down
Reward: -0.32610663771629333
Distance: 0.9300367832183838
Next state: tensor([8, 6, 0], dtype=torch.int32)
================================================================================

