Env ID: [572]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.08816681057214737
Distance: 9.155800819396973
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.1908031404018402
Distance: 9.143967628479004
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.07992897182703018
Distance: 9.234770774841309
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.031071282923221588
Distance: 9.214699745178223
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.2622905671596527
Distance: 9.145771026611328
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: down
Reward: 0.5201772451400757
Distance: 9.308061599731445
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.42414340376853943
Distance: 8.687884330749512
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: 0.10885848850011826
Distance: 9.012027740478516
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.34350165724754333
Distance: 8.803169250488281
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.3511682450771332
Distance: 9.046670913696289
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.6735063791275024
Distance: 9.297839164733887
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.11068878322839737
Distance: 9.871345520019531
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 4, 1], dtype=torch.int32)
Action: right
Reward: 0.1935657560825348
Distance: 9.882034301757812
Next state: tensor([1, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([1, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.6383785009384155
Distance: 9.588468551635742
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([1, 3, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.3501029908657074
Distance: 8.850090026855469
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

