Env ID: [231]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08516273647546768
Distance: 8.909940719604492
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.10622081905603409
Distance: 8.895103454589844
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.7110780477523804
Distance: 8.901324272155762
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: 0.5087627172470093
Distance: 8.090246200561523
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: 0.05780353397130966
Distance: 7.481483459472656
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.18901261687278748
Distance: 7.3236799240112305
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.16210374236106873
Distance: 7.412692546844482
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.17171868681907654
Distance: 7.474796295166016
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.15388068556785583
Distance: 7.546514987945557
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3497561514377594
Distance: 7.600395679473877
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.3171028196811676
Distance: 7.150639533996582
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: -1.0132538080215454
Distance: 6.733536720275879
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.008847616612911224
Distance: 7.646790504455566
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.10131321102380753
Distance: 7.537942886352539
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.4335999488830566
Distance: 7.5392560958862305
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.6726747751235962
Distance: 5.0056562423706055
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.05244579166173935
Distance: 5.578330993652344
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.46909627318382263
Distance: 5.425885200500488
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.270687460899353
Distance: 5.794981479644775
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([8, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.48772773146629333
Distance: 4.4242939949035645
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

