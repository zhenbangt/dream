Env ID: [495]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.09073028713464737
Distance: 5.90452766418457
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08788929134607315
Distance: 5.895257949829102
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.06793604046106339
Distance: 5.883147239685059
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.10150585323572159
Distance: 5.851083278656006
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.1281176507472992
Distance: 5.852589130401611
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.20678099989891052
Distance: 5.880706787109375
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.049854181706905365
Distance: 5.98748779296875
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.23182687163352966
Distance: 5.8376336097717285
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.017513848841190338
Distance: 5.969460487365723
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.20820435881614685
Distance: 5.886974334716797
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.042138196527957916
Distance: 5.995178699493408
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.21251162886619568
Distance: 5.93731689453125
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.05273256450891495
Distance: 6.04982852935791
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.20731458067893982
Distance: 6.002561092376709
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 3, 1], dtype=torch.int32)
Action: down
Reward: 0.0427798256278038
Distance: 6.109875679016113
Next state: tensor([5, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 2, 0], dtype=torch.int32)
Action: left
Reward: -0.12011728435754776
Distance: 5.967095851898193
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.12712249159812927
Distance: 5.987213134765625
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.03877124935388565
Distance: 6.014335632324219
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1702190339565277
Distance: 5.953106880187988
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.37140360474586487
Distance: 6.0233259201049805
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

