Env ID: [99]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.13461264967918396
Distance: 8.649901390075684
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.11245594173669815
Distance: 8.684514045715332
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.08958110958337784
Distance: 8.696969985961914
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.0695253387093544
Distance: 8.686551094055176
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.0873512253165245
Distance: 8.656076431274414
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.23457488417625427
Distance: 8.468725204467773
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.2729392945766449
Distance: 8.603300094604492
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.21798571944236755
Distance: 8.776239395141602
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.09341468662023544
Distance: 8.894225120544434
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.012229539453983307
Distance: 8.700810432434082
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.01007232815027237
Distance: 8.61303997039795
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.17126426100730896
Distance: 8.523112297058105
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.02131805568933487
Distance: 8.594376564025879
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.214268296957016
Distance: 8.515694618225098
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.06150493770837784
Distance: 8.629962921142578
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.1214815154671669
Distance: 8.59146785736084
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.13790282607078552
Distance: 8.61294937133789
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.15159758925437927
Distance: 8.65085220336914
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.1277843415737152
Distance: 8.702449798583984
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: 0.13029137253761292
Distance: 8.730234146118164
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

