Env ID: [66]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.15493544936180115
Distance: 9.27581787109375
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.27921828627586365
Distance: 9.330753326416016
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.15590152144432068
Distance: 9.509971618652344
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.14837512373924255
Distance: 9.565873146057129
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.060941316187381744
Distance: 9.614248275756836
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.0927024856209755
Distance: 9.575189590454102
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1499544084072113
Distance: 9.567892074584961
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.13856753706932068
Distance: 9.617846488952637
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.015843011438846588
Distance: 9.656414031982422
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 2, 0], dtype=torch.int32)
Action: noop
Reward: -0.14497718214988708
Distance: 9.572257041931152
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 2, 0], dtype=torch.int32)
Action: up
Reward: -0.09338436275720596
Distance: 9.617234230041504
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.17412909865379333
Distance: 9.610618591308594
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.1370445191860199
Distance: 9.684747695922852
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.08104953914880753
Distance: 9.721792221069336
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.10910282284021378
Distance: 9.702841758728027
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.033876799046993256
Distance: 9.711944580078125
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([3, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.007521055638790131
Distance: 9.578067779541016
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([2, 3, 0], dtype=torch.int32)
Action: left
Reward: -0.1728712022304535
Distance: 9.47054672241211
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([1, 3, 0], dtype=torch.int32)
Action: left
Reward: -0.18113383650779724
Distance: 9.543417930603027
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 3, 0], dtype=torch.int32)
Action: noop
Reward: -0.34622249007225037
Distance: 9.624551773071289
Next state: tensor([0, 3, 0], dtype=torch.int32)
================================================================================

