Env ID: [451]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.10286865383386612
Distance: 7.950465202331543
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.15325412154197693
Distance: 7.953333854675293
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.0971742644906044
Distance: 8.006587982177734
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.08029041439294815
Distance: 8.003762245178223
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.05057106167078018
Distance: 7.984052658081055
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.24067649245262146
Distance: 7.934623718261719
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.16140803694725037
Distance: 8.075300216674805
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.35932406783103943
Distance: 8.13670825958252
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.32612666487693787
Distance: 8.396032333374023
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.17958775162696838
Distance: 8.622159004211426
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.8024164438247681
Distance: 8.342571258544922
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 1, 1], dtype=torch.int32)
Action: down
Reward: 0.25131550431251526
Distance: 9.044987678527832
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

