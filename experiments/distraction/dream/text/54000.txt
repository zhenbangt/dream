Env ID: [352]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.05019340664148331
Distance: 9.455987930297852
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.07269439846277237
Distance: 9.406181335449219
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.18872490525245667
Distance: 9.378875732421875
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2214454710483551
Distance: 9.090150833129883
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.11448822170495987
Distance: 8.768705368041992
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.17790564894676208
Distance: 8.783193588256836
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.17313727736473083
Distance: 8.861099243164062
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.3308025300502777
Distance: 8.934236526489258
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.08743057399988174
Distance: 9.1650390625
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.3410373628139496
Distance: 9.152469635009766
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 4, 1], dtype=torch.int32)
Action: right
Reward: 0.1046384796500206
Distance: 9.39350700378418
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.37570056319236755
Distance: 9.188868522644043
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: 0.4326919615268707
Distance: 9.464569091796875
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([3, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.4200683534145355
Distance: 8.931877136230469
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.35461100935935974
Distance: 9.251945495605469
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9853223562240601
Distance: 9.506556510925293
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.33106574416160583
Distance: 8.421234130859375
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.10133133083581924
Distance: 8.652299880981445
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.08208618313074112
Distance: 8.653631210327148
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1086658462882042
Distance: 8.635717391967773
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

