Env ID: [561]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.08441600948572159
Distance: 9.427107810974121
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.012919999659061432
Distance: 9.411523818969727
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.13998755812644958
Distance: 9.324443817138672
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.02406463772058487
Distance: 9.364431381225586
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.10331878811120987
Distance: 9.288496017456055
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.120732881128788
Distance: 9.291814804077148
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.28007277846336365
Distance: 9.31254768371582
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.1504102647304535
Distance: 9.492620468139648
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.1494966447353363
Distance: 9.543030738830566
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.13075867295265198
Distance: 9.592527389526367
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.24299660325050354
Distance: 9.36176872253418
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.019882582128047943
Distance: 9.01877212524414
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.22321662306785583
Distance: 8.898889541625977
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1441875398159027
Distance: 9.022106170654297
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1948467195034027
Distance: 9.066293716430664
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 1, 1], dtype=torch.int32)
Action: noop
Reward: -0.2606063783168793
Distance: 9.161140441894531
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 1, 1], dtype=torch.int32)
Action: up
Reward: 0.14206066727638245
Distance: 9.321746826171875
Next state: tensor([8, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.16787394881248474
Distance: 9.079686164855957
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.023876570165157318
Distance: 9.147560119628906
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: 0.1765216886997223
Distance: 9.023683547973633
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

