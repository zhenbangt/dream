Env ID: [88]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.12730559706687927
Distance: 9.075980186462402
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.131443589925766
Distance: 9.103285789489746
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.10769996792078018
Distance: 9.134729385375977
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.07091102749109268
Distance: 9.14242935180664
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.1948305070400238
Distance: 9.113340377807617
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.0791105031967163
Distance: 9.208170890808105
Next state: tensor([4, 0, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 0, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3505016267299652
Distance: 8.029060363769531
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.1661609709262848
Distance: 8.279561996459961
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.15842971205711365
Distance: 8.01340103149414
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.10800132900476456
Distance: 8.071830749511719
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.24533042311668396
Distance: 8.079832077026367
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.03308715671300888
Distance: 8.225162506103516
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3908296525478363
Distance: 8.09207534790039
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.3537355363368988
Distance: 8.382905006408691
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.3794885575771332
Distance: 8.636640548706055
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.477692127227783
Distance: 8.916129112243652
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 7, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.46610507369041443
Distance: 6.338437080383301
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

