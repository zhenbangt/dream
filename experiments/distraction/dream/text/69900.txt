Env ID: [506]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.09820327907800674
Distance: 9.268588066101074
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.0669606924057007
Distance: 9.266791343688965
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.03496494144201279
Distance: 8.099830627441406
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.12754115462303162
Distance: 7.964865684509277
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.15191611647605896
Distance: 7.992406845092773
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.6257604360580444
Distance: 8.044322967529297
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.27574023604393005
Distance: 6.3185625076293945
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.05217180401086807
Distance: 6.494302749633789
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.25936898589134216
Distance: 6.446474552154541
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 5, 1], dtype=torch.int32)
Action: left
Reward: -0.17751464247703552
Distance: 6.605843544006348
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.8999611139297485
Distance: 6.683358192443848
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.9155994653701782
Distance: 7.483319282531738
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 4.156836032867432
Distance: 8.298918724060059
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 8, 1], dtype=torch.int32)
Action: ride_bus
Reward: -1.8929554224014282
Distance: 4.042082786560059
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.8703910112380981
Distance: 5.835038185119629
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.5207520723342896
Distance: 6.605429172515869
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 3, 1], dtype=torch.int32)
Action: end_episode
Reward: 0.02996959537267685
Distance: 7.026181221008301
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

