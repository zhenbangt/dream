Env ID: [506]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.10173378139734268
Distance: 8.396886825561523
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.05449829250574112
Distance: 8.39862060546875
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([6, 4, 0], dtype=torch.int32)
Action: drop
Reward: 0.03395786136388779
Distance: 8.353118896484375
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([6, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.16610106825828552
Distance: 8.219161033630371
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([6, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.17404994368553162
Distance: 8.285262107849121
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([6, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.120397187769413
Distance: 8.359312057495117
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([7, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.13806113600730896
Distance: 8.379709243774414
Next state: tensor([7, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([7, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.03768787533044815
Distance: 8.417770385742188
Next state: tensor([8, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.08739528805017471
Distance: 8.35545825958252
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.18673667311668396
Distance: 8.342853546142578
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 5, 1], dtype=torch.int32)
Action: drop
Reward: -0.12847766280174255
Distance: 8.429590225219727
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.11523685604333878
Distance: 8.458067893981934
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

