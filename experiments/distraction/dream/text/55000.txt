Env ID: [44]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08633098751306534
Distance: 9.289704322814941
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.12343845516443253
Distance: 9.27603530883789
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.13565310835838318
Distance: 9.299473762512207
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1025615707039833
Distance: 9.335126876831055
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.6931909322738647
Distance: 9.337688446044922
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.028698541224002838
Distance: 8.5444974899292
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.398751825094223
Distance: 8.473196029663086
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.29064711928367615
Distance: 8.771947860717773
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.9302176237106323
Distance: 8.962594985961914
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 7, 1], dtype=torch.int32)
Action: left
Reward: 0.35992422699928284
Distance: 7.932377338409424
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 7, 1], dtype=torch.int32)
Action: right
Reward: 0.05202188342809677
Distance: 7.4724531173706055
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([1, 7, 0], dtype=torch.int32)
Action: left
Reward: -0.036611177027225494
Distance: 7.320431232452393
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 7, 1], dtype=torch.int32)
Action: up
Reward: 0.5045942068099976
Distance: 7.257042407989502
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

