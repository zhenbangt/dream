Env ID: [198]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.14117392897605896
Distance: 8.168769836425781
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.2792125642299652
Distance: 8.209943771362305
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.11836566776037216
Distance: 8.389156341552734
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.4841028153896332
Distance: 8.170790672302246
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 7, 1], dtype=torch.int32)
Action: down
Reward: -0.3328300416469574
Distance: 8.554893493652344
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 6, 0], dtype=torch.int32)
Action: drop
Reward: -0.23814162611961365
Distance: 8.787723541259766
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 6, 0], dtype=torch.int32)
Action: left
Reward: -0.3626781404018402
Distance: 8.925865173339844
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 6, 0], dtype=torch.int32)
Action: drop
Reward: -0.2580171525478363
Distance: 9.188543319702148
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.12684497237205505
Distance: 9.34656047821045
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.06036510318517685
Distance: 9.373405456542969
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 5, 0], dtype=torch.int32)
Action: up
Reward: -0.4710603654384613
Distance: 9.213040351867676
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 6, 0], dtype=torch.int32)
Action: noop
Reward: -0.4428344666957855
Distance: 9.584100723266602
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 6, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.1466279923915863
Distance: 9.926935195922852
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 6, 0], dtype=torch.int32)
Action: drop
Reward: -0.37458762526512146
Distance: 9.973563194274902
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.10005436092615128
Distance: 10.248150825500488
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 6, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.149440199136734
Distance: 10.248205184936523
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

