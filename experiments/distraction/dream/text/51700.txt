Env ID: [517]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.08810005336999893
Distance: 9.350866317749023
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.05538902431726456
Distance: 9.338966369628906
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.18246611952781677
Distance: 9.294355392456055
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.1811012327671051
Distance: 9.376821517944336
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 7, 1], dtype=torch.int32)
Action: left
Reward: -0.13247832655906677
Distance: 9.095720291137695
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.003387071192264557
Distance: 9.128198623657227
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.19111976027488708
Distance: 9.031585693359375
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.07378540188074112
Distance: 9.122705459594727
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 1.075914740562439
Distance: 9.096490859985352
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: 0.9511455297470093
Distance: 7.920576095581055
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: 0.4147895872592926
Distance: 6.8694305419921875
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 1, 1], dtype=torch.int32)
Action: right
Reward: 0.20649948716163635
Distance: 6.354640960693359
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([1, 1, 0], dtype=torch.int32)
Action: left
Reward: -0.010161973536014557
Distance: 6.0481414794921875
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.49283257126808167
Distance: 5.958303451538086
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: pickup
Reward: 0.15077534317970276
Distance: 5.365470886230469
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.13105353713035583
Distance: 5.1146955490112305
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.2587924897670746
Distance: 5.145749092102051
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.32269486784935
Distance: 5.30454158782959
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.4924592077732086
Distance: 5.527236461639404
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.2121509611606598
Distance: 4.93477725982666
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

