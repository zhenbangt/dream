Env ID: [11]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.039986707270145416
Distance: 7.298274517059326
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.06744156032800674
Distance: 7.2382612228393555
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.13670310378074646
Distance: 7.205702781677246
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.3365615904331207
Distance: 7.242405891418457
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.15792235732078552
Distance: 6.805844306945801
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.07863817363977432
Distance: 6.863766670227051
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08510790020227432
Distance: 6.842404842376709
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.5855573415756226
Distance: 6.827512741088867
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.06479177623987198
Distance: 6.141955375671387
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.044130899012088776
Distance: 6.106747150421143
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.09905491024255753
Distance: 6.050878047943115
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.4829367697238922
Distance: 6.049932956695557
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 7, 1], dtype=torch.int32)
Action: left
Reward: 0.18301859498023987
Distance: 5.466996192932129
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.5045567750930786
Distance: 5.1839776039123535
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.5270000696182251
Distance: 5.588534355163574
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.6231580972671509
Distance: 6.015534400939941
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.5700665712356567
Distance: 6.538692474365234
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.3358708322048187
Distance: 7.008759021759033
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.12010059505701065
Distance: 7.244629859924316
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.150089830160141
Distance: 7.264730453491211
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

