Env ID: [110]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.13369140028953552
Distance: 9.579334259033203
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.084294892847538
Distance: 9.613025665283203
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.13524875044822693
Distance: 9.597320556640625
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.14414843916893005
Distance: 9.632569313049316
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.23775729537010193
Distance: 9.676717758178711
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([2, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.31525859236717224
Distance: 9.814475059509277
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([2, 5, 0], dtype=torch.int32)
Action: noop
Reward: 0.8017820119857788
Distance: 10.029733657836914
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([2, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.12922534346580505
Distance: 9.127951622009277
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([2, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.01078186184167862
Distance: 9.157176971435547
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([2, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.552054762840271
Distance: 9.06795883178711
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([2, 5, 0], dtype=torch.int32)
Action: down
Reward: -0.5711256265640259
Distance: 8.41590404510498
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([2, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.08515319973230362
Distance: 8.887029647827148
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([2, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.0027517303824424744
Distance: 8.872182846069336
Next state: tensor([2, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([2, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.012637712061405182
Distance: 8.769431114196777
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([2, 3, 0], dtype=torch.int32)
Action: left
Reward: -0.5633102655410767
Distance: 8.682068824768066
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([1, 3, 0], dtype=torch.int32)
Action: noop
Reward: -0.23740634322166443
Distance: 9.145379066467285
Next state: tensor([1, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([1, 3, 0], dtype=torch.int32)
Action: right
Reward: 0.1217731460928917
Distance: 9.282785415649414
Next state: tensor([2, 3, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([2, 3, 0], dtype=torch.int32)
Action: right
Reward: -0.1494060456752777
Distance: 9.061012268066406
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([3, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.619235634803772
Distance: 9.110418319702148
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 8, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.11625442653894424
Distance: 9.629653930664062
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

