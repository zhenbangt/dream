Env ID: [374]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.09601936489343643
Distance: 8.83746337890625
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.07843074947595596
Distance: 8.83348274230957
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1618410050868988
Distance: 8.81191349029541
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.2021251618862152
Distance: 8.873754501342773
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.07800140231847763
Distance: 8.975879669189453
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3660503327846527
Distance: 8.79787826538086
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: 0.022323988378047943
Distance: 9.063928604125977
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.11356296390295029
Distance: 8.941604614257812
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.21190223097801208
Distance: 8.728041648864746
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: 0.23475494980812073
Distance: 8.839943885803223
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.0997520461678505
Distance: 8.505188941955566
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.03640422970056534
Distance: 8.5049409866333
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: down
Reward: -0.49041327834129333
Distance: 8.44134521484375
Next state: tensor([4, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 2, 0], dtype=torch.int32)
Action: up
Reward: 0.11344947665929794
Distance: 8.831758499145508
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.04885444790124893
Distance: 8.618309020996094
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.027259446680545807
Distance: 8.567163467407227
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.23156681656837463
Distance: 8.494422912597656
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 7, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.31355342268943787
Distance: 8.162856101989746
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

