Env ID: [484]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.07820949703454971
Distance: 8.501462936401367
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.16935023665428162
Distance: 8.4796724319458
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.01389942318201065
Distance: 8.549022674560547
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.36926040053367615
Distance: 8.462922096252441
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.7687708139419556
Distance: 8.732182502746582
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.6790205240249634
Distance: 9.40095329284668
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 5, 1], dtype=torch.int32)
Action: up
Reward: -0.184055894613266
Distance: 9.979973793029785
Next state: tensor([3, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 6, 0], dtype=torch.int32)
Action: drop
Reward: -0.3967624604701996
Distance: 10.064029693603516
Next state: tensor([3, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 6, 0], dtype=torch.int32)
Action: drop
Reward: -0.35522422194480896
Distance: 10.36079216003418
Next state: tensor([3, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([3, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.5690075159072876
Distance: 10.616016387939453
Next state: tensor([3, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([3, 5, 1], dtype=torch.int32)
Action: left
Reward: 0.18268433213233948
Distance: 11.085023880004883
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([2, 5, 0], dtype=torch.int32)
Action: ride_bus
Reward: 0.05463256686925888
Distance: 10.802339553833008
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([2, 5, 0], dtype=torch.int32)
Action: pickup
Reward: -0.2712741792201996
Distance: 10.647706985473633
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([2, 5, 0], dtype=torch.int32)
Action: pickup
Reward: -0.006766892969608307
Distance: 10.818981170654297
Next state: tensor([2, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([2, 5, 0], dtype=torch.int32)
Action: left
Reward: -0.341348260641098
Distance: 10.725748062133789
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([1, 5, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.5947555303573608
Distance: 10.967096328735352
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

