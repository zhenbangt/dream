Env ID: [297]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.13061866164207458
Distance: 8.565027236938477
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.6715949773788452
Distance: 8.595645904541016
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.270850270986557
Distance: 7.8240509033203125
Next state: tensor([7, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([7, 1, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.42359742522239685
Distance: 7.994901180267334
Next state: tensor([7, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([7, 1, 0], dtype=torch.int32)
Action: pickup
Reward: -0.7410398721694946
Distance: 8.318498611450195
Next state: tensor([7, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([7, 1, 0], dtype=torch.int32)
Action: pickup
Reward: -0.7998710870742798
Distance: 8.959538459777832
Next state: tensor([7, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([7, 1, 0], dtype=torch.int32)
Action: right
Reward: -0.8551441431045532
Distance: 9.659409523010254
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.8116849660873413
Distance: 10.41455364227295
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.16070231795310974
Distance: 9.50286865234375
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.2958246171474457
Distance: 9.563570976257324
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([6, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.06621970981359482
Distance: 9.759395599365234
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 4, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.016563989222049713
Distance: 9.593175888061523
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

