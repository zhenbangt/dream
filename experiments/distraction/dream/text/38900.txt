Env ID: [330]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.017731092870235443
Distance: 9.555952072143555
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1293693482875824
Distance: 9.438220977783203
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.15486487746238708
Distance: 9.46759033203125
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.09406433254480362
Distance: 9.522455215454102
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.3055187165737152
Distance: 9.516519546508789
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.03706226497888565
Distance: 9.722038269042969
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.25785884261131287
Distance: 9.659100532531738
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: 0.06643714755773544
Distance: 9.816959381103516
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.18158206343650818
Distance: 9.650522232055664
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: 0.05589427798986435
Distance: 9.732104301452637
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: 0.024401091039180756
Distance: 9.576210021972656
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.058748818933963776
Distance: 9.45180892944336
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.10701236873865128
Distance: 9.410557746887207
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.11274585872888565
Distance: 9.417570114135742
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.09616909176111221
Distance: 9.430315971374512
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.08842048794031143
Distance: 9.426485061645508
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.0832887664437294
Distance: 9.414905548095703
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.0803743377327919
Distance: 9.398194313049316
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.1952110230922699
Distance: 9.378568649291992
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([5, 3, 1], dtype=torch.int32)
Action: up
Reward: 0.10553397983312607
Distance: 9.473779678344727
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

