Env ID: [484]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.1957298219203949
Distance: 8.92819595336914
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.015674211084842682
Distance: 9.02392578125
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([8, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.14695987105369568
Distance: 8.939599990844727
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.060549356043338776
Distance: 8.986559867858887
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.09635696560144424
Distance: 8.94710922241211
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.4718613624572754
Distance: 8.943466186523438
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.16224423050880432
Distance: 6.371604919433594
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: 0.007630251348018646
Distance: 6.109360694885254
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.09650430828332901
Distance: 6.001730442047119
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.8045411109924316
Distance: 5.998234748840332
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 1, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.33758744597435
Distance: 3.093693733215332
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: 0.029034040868282318
Distance: 3.3312811851501465
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 2.1772289276123047
Distance: 3.202247142791748
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 8, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.049577973783016205
Distance: 0.925018310546875
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([5, 5, 1], dtype=torch.int32)
Action: down
Reward: -0.05407986789941788
Distance: 0.7754403352737427
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([5, 4, 1], dtype=torch.int32)
Action: left
Reward: -0.18380823731422424
Distance: 0.7295202016830444
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.2398177683353424
Distance: 0.8133284449577332
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 5, 1], dtype=torch.int32)
Action: right
Reward: -0.13397958874702454
Distance: 0.95314621925354
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([5, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.4724564850330353
Distance: 0.987125813961029
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 8, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.06550834327936172
Distance: 1.3595823049545288
Next state: tensor([4, 8, 1], dtype=torch.int32)
================================================================================

