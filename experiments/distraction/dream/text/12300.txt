Env ID: [572]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.13484343886375427
Distance: 7.056234359741211
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.04794750362634659
Distance: 7.09107780456543
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: 0.09349336475133896
Distance: 7.03902530670166
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.11872587352991104
Distance: 6.845531940460205
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: 0.10943164676427841
Distance: 6.8642578125
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.1902237832546234
Distance: 6.6548261642456055
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.35913267731666565
Distance: 6.745049953460693
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.15074977278709412
Distance: 6.285917282104492
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: 0.2385634481906891
Distance: 6.336667060852051
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: -0.0017796531319618225
Distance: 5.998103618621826
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 7, 1], dtype=torch.int32)
Action: up
Reward: -0.06789932399988174
Distance: 5.899883270263672
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

