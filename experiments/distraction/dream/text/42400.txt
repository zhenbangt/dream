Env ID: [440]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.054918862879276276
Distance: 7.548732280731201
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.06082639843225479
Distance: 7.503651142120361
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.023183442652225494
Distance: 7.4644775390625
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.26068058609962463
Distance: 7.387660980224609
Next state: tensor([8, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([8, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.04279889911413193
Distance: 7.026980400085449
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.27044573426246643
Distance: 6.884181499481201
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.3636459410190582
Distance: 6.513735771179199
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.0037931427359580994
Distance: 6.0500898361206055
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.04278097301721573
Distance: 5.946296691894531
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.1114874854683876
Distance: 5.889077663421631
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.008368872106075287
Distance: 5.900565147399902
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.08104143291711807
Distance: 5.792196273803711
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.1317053735256195
Distance: 5.773237705230713
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.1476942002773285
Distance: 5.804943084716797
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.1403256356716156
Distance: 5.85263729095459
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.1386795938014984
Distance: 5.89296293258667
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.13599643111228943
Distance: 5.931642532348633
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.11974916607141495
Distance: 5.967638969421387
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.16336163878440857
Distance: 5.9873881340026855
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: -0.20013627409934998
Distance: 6.050749778747559
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

