Env ID: [429]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.10063419491052628
Distance: 9.028288841247559
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12863311171531677
Distance: 9.028923034667969
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.08514461666345596
Distance: 9.05755615234375
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.04237232357263565
Distance: 9.04270076751709
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.06779060512781143
Distance: 8.98507308959961
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.3191562592983246
Distance: 8.952863693237305
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.3782220780849457
Distance: 9.172019958496094
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 7, 1], dtype=torch.int32)
Action: left
Reward: 0.28624287247657776
Distance: 9.450242042541504
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.03405036777257919
Distance: 9.06399917602539
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.005145646631717682
Distance: 8.929948806762695
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 7, 1], dtype=torch.int32)
Action: left
Reward: -0.1604991853237152
Distance: 8.835094451904297
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 7, 1], dtype=torch.int32)
Action: down
Reward: 0.10398616641759872
Distance: 8.895593643188477
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.3947349488735199
Distance: 8.691607475280762
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.01075897365808487
Distance: 8.986342430114746
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 6, 0], dtype=torch.int32)
Action: pickup
Reward: 0.07917728275060654
Distance: 8.897101402282715
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.13652858138084412
Distance: 8.717924118041992
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.344822496175766
Distance: 8.7544527053833
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.343937486410141
Distance: 8.999275207519531
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.3921590745449066
Distance: 9.243212699890137
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.3161254823207855
Distance: 9.535371780395508
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

