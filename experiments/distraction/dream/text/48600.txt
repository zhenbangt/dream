Env ID: [319]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.10836944729089737
Distance: 8.938820838928223
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.06023464351892471
Distance: 8.947190284729004
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.1317811906337738
Distance: 8.907424926757812
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 5, 1], dtype=torch.int32)
Action: left
Reward: -0.3579908311367035
Distance: 8.93920612335205
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.6599124670028687
Distance: 9.197196960449219
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 7, 1], dtype=torch.int32)
Action: noop
Reward: 0.8250359296798706
Distance: 8.437284469604492
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.10609159618616104
Distance: 7.512248516082764
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.2721725404262543
Distance: 7.518340110778809
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.3027673661708832
Distance: 7.690512657165527
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.278634637594223
Distance: 7.893280029296875
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 7, 1], dtype=torch.int32)
Action: noop
Reward: -0.2658573091030121
Distance: 8.071914672851562
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 7, 1], dtype=torch.int32)
Action: right
Reward: -0.3691326081752777
Distance: 8.237771987915039
Next state: tensor([1, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([1, 7, 0], dtype=torch.int32)
Action: right
Reward: -0.24711188673973083
Distance: 8.506904602050781
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([2, 7, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.12446174770593643
Distance: 8.654016494750977
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([2, 7, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.16857776045799255
Distance: 8.678478240966797
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([2, 7, 0], dtype=torch.int32)
Action: drop
Reward: -0.24399718642234802
Distance: 8.747056007385254
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([2, 7, 0], dtype=torch.int32)
Action: drop
Reward: -0.159672349691391
Distance: 8.891053199768066
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([2, 7, 0], dtype=torch.int32)
Action: drop
Reward: -0.16959723830223083
Distance: 8.950725555419922
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([2, 7, 0], dtype=torch.int32)
Action: drop
Reward: -0.17759761214256287
Distance: 9.020322799682617
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([2, 7, 0], dtype=torch.int32)
Action: drop
Reward: -0.2252698838710785
Distance: 9.097920417785645
Next state: tensor([2, 7, 0], dtype=torch.int32)
================================================================================

