Env ID: [176]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: right
Reward: -0.13120707869529724
Distance: 8.87692642211914
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1], dtype=torch.int32)
Action: right
Reward: -0.024227716028690338
Distance: 8.908133506774902
Next state: tensor([6, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([6, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.14944496750831604
Distance: 8.832361221313477
Next state: tensor([5, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([5, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.09684715420007706
Distance: 8.582916259765625
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([5, 5, 1], dtype=torch.int32)
Action: drop
Reward: 0.23098793625831604
Distance: 8.579763412475586
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 5, 1], dtype=torch.int32)
Action: up
Reward: -0.023853875696659088
Distance: 8.248775482177734
Next state: tensor([5, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([5, 6, 0], dtype=torch.int32)
Action: noop
Reward: 0.014324568212032318
Distance: 8.172629356384277
Next state: tensor([5, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.027787305414676666
Distance: 8.058304786682129
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([5, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.0842810645699501
Distance: 7.9860920906066895
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 5, 1], dtype=torch.int32)
Action: pickup
Reward: 0.054491423070430756
Distance: 7.970373153686523
Next state: tensor([5, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([5, 5, 1], dtype=torch.int32)
Action: left
Reward: -0.25512275099754333
Distance: 7.815881729125977
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([4, 5, 1], dtype=torch.int32)
Action: drop
Reward: -0.3539815843105316
Distance: 7.971004486083984
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([4, 5, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.2724805772304535
Distance: 8.22498607635498
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 7, 1], dtype=torch.int32)
Action: end_episode
Reward: -0.23480281233787537
Distance: 8.397466659545898
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

