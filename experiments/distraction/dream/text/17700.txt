Env ID: [99]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: drop
Reward: -0.18469485640525818
Distance: 8.765227317810059
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.04607544094324112
Distance: 8.849922180175781
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.08796463161706924
Distance: 8.795997619628906
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.13273468613624573
Distance: 8.78396224975586
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 7, 1], dtype=torch.int32)
Action: ride_bus
Reward: 0.16284409165382385
Distance: 8.551227569580078
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1], dtype=torch.int32)
Action: drop
Reward: 0.42668190598487854
Distance: 8.288383483886719
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.26870211958885193
Distance: 7.761701583862305
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 7, 1], dtype=torch.int32)
Action: pickup
Reward: 0.24742546677589417
Distance: 7.930403709411621
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([0, 7, 1], dtype=torch.int32)
Action: left
Reward: -0.06646690517663956
Distance: 7.582978248596191
Next state: tensor([0, 7, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([0, 7, 1], dtype=torch.int32)
Action: up
Reward: -0.3441949784755707
Distance: 7.549445152282715
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 8, 0], dtype=torch.int32)
Action: drop
Reward: -0.009198285639286041
Distance: 7.79364013671875
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 8, 0], dtype=torch.int32)
Action: right
Reward: -0.324190229177475
Distance: 7.70283842086792
Next state: tensor([1, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([1, 8, 0], dtype=torch.int32)
Action: left
Reward: -0.33064040541648865
Distance: 7.927028656005859
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 8, 0], dtype=torch.int32)
Action: end_episode
Reward: 0.04823341220617294
Distance: 8.157669067382812
Next state: tensor([0, 8, 0], dtype=torch.int32)
================================================================================

