Env ID: [154]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: up
Reward: -0.10996542125940323
Distance: 7.984949588775635
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 5, 1], dtype=torch.int32)
Action: noop
Reward: -0.19265899062156677
Distance: 7.994915008544922
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 5, 1], dtype=torch.int32)
Action: pickup
Reward: -0.06594906002283096
Distance: 8.087574005126953
Next state: tensor([4, 5, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 5, 1], dtype=torch.int32)
Action: up
Reward: 0.2291201651096344
Distance: 8.053523063659668
Next state: tensor([4, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 6, 0], dtype=torch.int32)
Action: up
Reward: -0.04172859340906143
Distance: 7.724402904510498
Next state: tensor([4, 7, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 7, 0], dtype=torch.int32)
Action: down
Reward: -0.09786806255578995
Distance: 7.666131496429443
Next state: tensor([4, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 6, 0], dtype=torch.int32)
Action: end_episode
Reward: -0.35924920439720154
Distance: 7.663999557495117
Next state: tensor([4, 6, 0], dtype=torch.int32)
================================================================================

