Env ID: [11]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.21711596846580505
Distance: 7.996548652648926
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: 0.05686893314123154
Distance: 8.113664627075195
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([3, 4, 1], dtype=torch.int32)
Action: down
Reward: -0.06645307689905167
Distance: 7.956795692443848
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([3, 3, 1], dtype=torch.int32)
Action: pickup
Reward: -0.1312905251979828
Distance: 7.923248767852783
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([3, 3, 1], dtype=torch.int32)
Action: noop
Reward: -0.14567908644676208
Distance: 7.9545392990112305
Next state: tensor([3, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([3, 3, 1], dtype=torch.int32)
Action: up
Reward: -0.05110225826501846
Distance: 8.000218391418457
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([3, 4, 1], dtype=torch.int32)
Action: noop
Reward: -0.13152369856834412
Distance: 7.951320648193359
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([3, 4, 1], dtype=torch.int32)
Action: drop
Reward: -0.14801844954490662
Distance: 7.982844352722168
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.10415992885828018
Distance: 8.030862808227539
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.18642672896385193
Distance: 8.035022735595703
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.1627260148525238
Distance: 8.12144947052002
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([8, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.09099922329187393
Distance: 8.184175491333008
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([8, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.10952911525964737
Distance: 8.175174713134766
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([8, 1, 1], dtype=torch.int32)
Action: drop
Reward: -0.1850658357143402
Distance: 8.184703826904297
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([8, 1, 1], dtype=torch.int32)
Action: right
Reward: -0.1101442351937294
Distance: 8.269769668579102
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([8, 1, 1], dtype=torch.int32)
Action: down
Reward: 0.054056547582149506
Distance: 8.279913902282715
Next state: tensor([8, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 0, 0], dtype=torch.int32)
Action: up
Reward: -0.05491695553064346
Distance: 8.12585735321045
Next state: tensor([8, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 1, 1], dtype=torch.int32)
Action: left
Reward: -0.18771132826805115
Distance: 8.080774307250977
Next state: tensor([7, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([7, 1, 0], dtype=torch.int32)
Action: up
Reward: -0.1996055543422699
Distance: 8.168485641479492
Next state: tensor([7, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([7, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.002918817102909088
Distance: 8.268091201782227
Next state: tensor([7, 2, 0], dtype=torch.int32)
================================================================================

