Env ID: [308]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: left
Reward: -0.1233893409371376
Distance: 6.715172290802002
Next state: tensor([3, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([3, 4, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.24692019820213318
Distance: 6.738561630249023
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([0, 1, 1], dtype=torch.int32)
Action: down
Reward: -0.4002610146999359
Distance: 6.885481834411621
Next state: tensor([0, 0, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([0, 0, 0], dtype=torch.int32)
Action: up
Reward: -0.09202108532190323
Distance: 7.1857428550720215
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([0, 1, 1], dtype=torch.int32)
Action: up
Reward: 0.16406860947608948
Distance: 7.177763938903809
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([0, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.3604942262172699
Distance: 6.913695335388184
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([0, 2, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.36883601546287537
Distance: 7.174189567565918
Next state: tensor([0, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([0, 2, 0], dtype=torch.int32)
Action: right
Reward: -0.17378005385398865
Distance: 7.443025588989258
Next state: tensor([1, 2, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([1, 2, 0], dtype=torch.int32)
Action: down
Reward: -0.11770305782556534
Distance: 7.516805648803711
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([1, 1, 0], dtype=torch.int32)
Action: pickup
Reward: 0.6840442419052124
Distance: 7.53450870513916
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([1, 1, 0], dtype=torch.int32)
Action: pickup
Reward: 0.13145104050636292
Distance: 6.75046443939209
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([1, 1, 0], dtype=torch.int32)
Action: pickup
Reward: -0.2719598710536957
Distance: 6.519013404846191
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([1, 1, 0], dtype=torch.int32)
Action: pickup
Reward: -0.38784465193748474
Distance: 6.690973281860352
Next state: tensor([1, 1, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([1, 1, 0], dtype=torch.int32)
Action: left
Reward: -0.14091166853904724
Distance: 6.978817939758301
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: 0.2358345091342926
Distance: 7.0197296142578125
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([0, 1, 1], dtype=torch.int32)
Action: drop
Reward: 0.005431555211544037
Distance: 6.683895111083984
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.04351339489221573
Distance: 6.578463554382324
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.1295090615749359
Distance: 6.521976947784424
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.17338714003562927
Distance: 6.551486015319824
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([0, 1, 1], dtype=torch.int32)
Action: pickup
Reward: -0.16772231459617615
Distance: 6.624873161315918
Next state: tensor([0, 1, 1], dtype=torch.int32)
================================================================================

