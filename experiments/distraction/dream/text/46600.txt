Env ID: [220]
================================================================================
Timestep: 0
State: tensor([4, 4, 0], dtype=torch.int32)
Action: ride_bus
Reward: -0.08807144314050674
Distance: 7.987001895904541
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 1
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.10301695019006729
Distance: 7.975073337554932
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 2
State: tensor([4, 4, 0], dtype=torch.int32)
Action: noop
Reward: -0.132786363363266
Distance: 7.978090286254883
Next state: tensor([4, 4, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0], dtype=torch.int32)
Action: down
Reward: -0.0720592513680458
Distance: 8.010876655578613
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.09957943111658096
Distance: 7.982935905456543
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 5
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.09429798275232315
Distance: 7.982515335083008
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.09157953411340714
Distance: 7.976813316345215
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 7
State: tensor([5, 3, 1], dtype=torch.int32)
Action: left
Reward: -0.18888291716575623
Distance: 7.968392848968506
Next state: tensor([4, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1], dtype=torch.int32)
Action: right
Reward: -0.10577163845300674
Distance: 8.057275772094727
Next state: tensor([5, 3, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 9
State: tensor([5, 3, 1], dtype=torch.int32)
Action: ride_bus
Reward: -0.1391979157924652
Distance: 8.063047409057617
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 10
State: tensor([0, 4, 1], dtype=torch.int32)
Action: pickup
Reward: -0.43865737318992615
Distance: 8.102245330810547
Next state: tensor([0, 4, 1], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 11
State: tensor([0, 4, 1], dtype=torch.int32)
Action: up
Reward: -0.29948386549949646
Distance: 8.440902709960938
Next state: tensor([0, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 12
State: tensor([0, 5, 0], dtype=torch.int32)
Action: up
Reward: -0.12289581447839737
Distance: 8.640386581420898
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 13
State: tensor([0, 6, 0], dtype=torch.int32)
Action: pickup
Reward: -0.12313327938318253
Distance: 8.66328239440918
Next state: tensor([0, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 14
State: tensor([0, 6, 0], dtype=torch.int32)
Action: right
Reward: -0.009186364710330963
Distance: 8.686415672302246
Next state: tensor([1, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 15
State: tensor([1, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.46287211775779724
Distance: 8.595602035522461
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 16
State: tensor([1, 5, 0], dtype=torch.int32)
Action: up
Reward: 0.16844406723976135
Distance: 8.958474159240723
Next state: tensor([1, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 17
State: tensor([1, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.1506410539150238
Distance: 8.690030097961426
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 18
State: tensor([1, 5, 0], dtype=torch.int32)
Action: up
Reward: 0.09686126559972763
Distance: 8.740671157836914
Next state: tensor([1, 6, 0], dtype=torch.int32)
================================================================================

================================================================================
Timestep: 19
State: tensor([1, 6, 0], dtype=torch.int32)
Action: down
Reward: -0.24414214491844177
Distance: 8.54380989074707
Next state: tensor([1, 5, 0], dtype=torch.int32)
================================================================================

