Env ID: [8]
================================================================================
Timestep: 0
State: tensor([4, 4, 0, 0])
Action: right
Reward: 0.2595790922641754
Distance: 9.32209587097168
Next state: tensor([5, 4, 1, 0])
================================================================================

================================================================================
Timestep: 1
State: tensor([5, 4, 1, 0])
Action: noop
Reward: -0.08297119289636612
Distance: 8.962516784667969
Next state: tensor([5, 4, 1, 0])
================================================================================

================================================================================
Timestep: 2
State: tensor([5, 4, 1, 0])
Action: left
Reward: -0.097478486597538
Distance: 8.945487976074219
Next state: tensor([4, 4, 0, 0])
================================================================================

================================================================================
Timestep: 3
State: tensor([4, 4, 0, 0])
Action: drop
Reward: -0.20858725905418396
Distance: 8.94296646118164
Next state: tensor([4, 4, 0, 0])
================================================================================

================================================================================
Timestep: 4
State: tensor([4, 4, 0, 0])
Action: down
Reward: -0.17783603072166443
Distance: 9.051553726196289
Next state: tensor([4, 3, 1, 0])
================================================================================

================================================================================
Timestep: 5
State: tensor([4, 3, 1, 0])
Action: noop
Reward: -0.08587894588708878
Distance: 9.129389762878418
Next state: tensor([4, 3, 1, 0])
================================================================================

================================================================================
Timestep: 6
State: tensor([4, 3, 1, 0])
Action: down
Reward: -0.12178955227136612
Distance: 9.11526870727539
Next state: tensor([4, 2, 0, 0])
================================================================================

================================================================================
Timestep: 7
State: tensor([4, 2, 0, 0])
Action: up
Reward: -0.03799781948328018
Distance: 9.13705825805664
Next state: tensor([4, 3, 1, 0])
================================================================================

================================================================================
Timestep: 8
State: tensor([4, 3, 1, 0])
Action: ride_bus
Reward: -0.1197715774178505
Distance: 9.075056076049805
Next state: tensor([8, 7, 1, 0])
================================================================================

================================================================================
Timestep: 9
State: tensor([8, 7, 1, 0])
Action: right
Reward: -0.02517566829919815
Distance: 9.094827651977539
Next state: tensor([8, 7, 1, 0])
================================================================================

================================================================================
Timestep: 10
State: tensor([8, 7, 1, 0])
Action: left
Reward: -0.019045449793338776
Distance: 9.020003318786621
Next state: tensor([7, 7, 0, 0])
================================================================================

================================================================================
Timestep: 11
State: tensor([7, 7, 0, 0])
Action: ride_bus
Reward: -0.17968043684959412
Distance: 8.939048767089844
Next state: tensor([7, 7, 0, 0])
================================================================================

================================================================================
Timestep: 12
State: tensor([7, 7, 0, 0])
Action: down
Reward: -0.09989605098962784
Distance: 9.018729209899902
Next state: tensor([7, 6, 0, 0])
================================================================================

================================================================================
Timestep: 13
State: tensor([7, 6, 0, 0])
Action: ride_bus
Reward: -0.11255226284265518
Distance: 9.018625259399414
Next state: tensor([7, 6, 0, 0])
================================================================================

================================================================================
Timestep: 14
State: tensor([7, 6, 0, 0])
Action: pickup
Reward: -0.07144413143396378
Distance: 9.031177520751953
Next state: tensor([7, 6, 0, 0])
================================================================================

================================================================================
Timestep: 15
State: tensor([7, 6, 0, 0])
Action: right
Reward: -0.10589084774255753
Distance: 9.0026216506958
Next state: tensor([8, 6, 0, 0])
================================================================================

================================================================================
Timestep: 16
State: tensor([8, 6, 0, 0])
Action: down
Reward: -0.10745010524988174
Distance: 9.008512496948242
Next state: tensor([8, 5, 0, 0])
================================================================================

================================================================================
Timestep: 17
State: tensor([8, 5, 0, 0])
Action: end_episode
Reward: -0.06109008938074112
Distance: 9.015962600708008
Next state: tensor([8, 5, 0, 0])
================================================================================

